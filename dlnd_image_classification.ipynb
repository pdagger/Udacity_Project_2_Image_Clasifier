{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 2:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 984, 1: 1007, 2: 1010, 3: 995, 4: 1010, 5: 988, 6: 1008, 7: 1026, 8: 987, 9: 985}\n",
      "First 20 Labels: [1, 6, 6, 8, 8, 3, 4, 6, 0, 6, 0, 3, 6, 6, 5, 4, 8, 3, 2, 6]\n",
      "\n",
      "Example of Image 14:\n",
      "Image - Min Value: 6 Max Value: 202\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 5 Name: dog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe425661a90>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 2\n",
    "sample_id = 14\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    X = np.zeros(x.shape)\n",
    "        \n",
    "    for i in range (0, x.shape[0]):\n",
    "        for j in range (0, x.shape[3]):\n",
    "            max_value = np.amax(x[i, :, :, j])\n",
    "            min_value = np.amin(x[i, :, :, j])\n",
    "            X[i, :, :, j] = np.divide(x[i, :, :, j] - min_value, max_value - min_value)\n",
    "            \n",
    "    return X\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    X = np.zeros(len(x))\n",
    "    X = x\n",
    "    \n",
    "    #one-hot encoding with determined classes from 0 to 9\n",
    "    lb = preprocessing.label_binarize(X, classes=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "    \n",
    "    return lb\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    Shape = []                 #Initialize list containing image batch tensor shape\n",
    "    Shape.append(None)         #First tensor element is None\n",
    "    #Append to shape the dimensions from image_shape\n",
    "    for i in range(0, len(image_shape)):  \n",
    "        Shape.append(image_shape[i])\n",
    "        \n",
    "    return tf.placeholder(tf.float32, shape = Shape, name = \"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    Shape = []                 #Initialize list containing tensor batch label shape\n",
    "    Shape.append(None)         #First tensor element is None\n",
    "    Shape.append(n_classes)\n",
    "    \n",
    "    return tf.placeholder(tf.float32, shape = Shape, name = \"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    return tf.placeholder(tf.float32, name = \"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    #print(pool_strides)\n",
    "    # TODO: Implement Function\n",
    "    #Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`\n",
    "    image_height = int(x_tensor.get_shape()[1])\n",
    "    image_width = int(x_tensor.get_shape()[2])\n",
    "    image_depth = int(x_tensor.get_shape()[3])\n",
    "    weight = tf.Variable(tf.truncated_normal([conv_ksize[0], conv_ksize[1], image_depth, conv_num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.random_normal([conv_num_outputs]))\n",
    "    \n",
    "    #Apply a convolution to `x_tensor` using weight and `conv_strides`, padding \"SAME\"\n",
    "    stride_height = conv_strides[0]\n",
    "    stride_width = conv_strides[1]\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, strides = [1, stride_height, stride_width, 1], padding = 'SAME')\n",
    "    #Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    #Add a nonlinear activation to the convolution\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    #Apply Max Pooling using `pool_ksize` and `pool_strides`, padding \"SAME\"\n",
    "    pool_height = pool_ksize[0]\n",
    "    pool_width = pool_ksize[1]\n",
    "    pstride_height = pool_strides[0]\n",
    "    pstride_width = pool_strides[1]\n",
    "    conv_layer = tf.nn.max_pool(conv_layer, ksize = [1, pool_height, pool_width, 1], \n",
    "                                strides = [1, pstride_height, pstride_width, 1], padding = 'SAME')\n",
    "        \n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    #print(x_tensor.get_shape())\n",
    "    #Get last 3D shape from 4D tensor\n",
    "    x_height = int(x_tensor.get_shape()[1])\n",
    "    x_width = int(x_tensor.get_shape()[2])\n",
    "    x_depth = int(x_tensor.get_shape()[3])\n",
    "    #Dimension flatten layer\n",
    "    flat_tensor_dim = x_height * x_width * x_depth\n",
    "    #Create flat tensor\n",
    "    flat_tensor = tf.reshape(x_tensor, [-1, flat_tensor_dim])\n",
    "        \n",
    "    return flat_tensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    number_weights = int(x_tensor.get_shape()[1])\n",
    "    weights = tf.Variable(tf.truncated_normal([number_weights, num_outputs], stddev=0.1))\n",
    "    biases = tf.Variable(tf.random_normal([num_outputs]))\n",
    "  \n",
    "    fc_layer = tf.add(tf.matmul(x_tensor, weights), biases)\n",
    "    fc_layer = tf.nn.relu(fc_layer)\n",
    "    \n",
    "    return fc_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    number_weights = int(x_tensor.get_shape()[1])\n",
    "    weights_out = tf.Variable(tf.truncated_normal([number_weights, num_outputs], stddev=0.1))\n",
    "    biases_out = tf.Variable(tf.random_normal([num_outputs]))\n",
    " \n",
    "    output_layer = tf.add(tf.matmul(x_tensor, weights_out), biases_out)\n",
    "        \n",
    "    return output_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    conv = conv2d_maxpool(x, 64, (4, 4), (1, 1), (4, 4), (4, 4))\n",
    "    conv = conv2d_maxpool(conv, 32, (2, 2), (1, 1), (4, 4), (4, 4))\n",
    "    #conv = conv2d_maxpool(conv, 10, (1, 1), (1, 1), (1, 1), (1, 1))\n",
    "       \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flat = flatten(conv)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    #num_outputs = int(flat.get_shape()[1])\n",
    "    full = fully_conn(flat, 16)\n",
    "    #full = fully_conn(full, 64)\n",
    "    #full = fully_conn(full, 32)\n",
    "    \n",
    "    \n",
    "    #Dropout\n",
    "    full_drop = tf.nn.dropout(full, keep_prob)\n",
    "    \n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(full_drop, 10)\n",
    "    \n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.})\n",
    "    valid_accuracy = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.})\n",
    "   \n",
    "    print(\"Loss: {:>10.4f} Validation Accuracy: {:.10f}\".format(loss, valid_accuracy))\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 124\n",
    "batch_size = 64\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2947 Validation Accuracy: 0.1247999817\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.2309 Validation Accuracy: 0.2328000069\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     2.1160 Validation Accuracy: 0.2771999836\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.9995 Validation Accuracy: 0.3005999923\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.9273 Validation Accuracy: 0.3125999570\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.9336 Validation Accuracy: 0.3391999900\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.9122 Validation Accuracy: 0.3346000314\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.8605 Validation Accuracy: 0.3515999913\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.8771 Validation Accuracy: 0.3607999384\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.7906 Validation Accuracy: 0.3669999838\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.7743 Validation Accuracy: 0.3783999979\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.7696 Validation Accuracy: 0.3759999871\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.7483 Validation Accuracy: 0.3905999959\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.7331 Validation Accuracy: 0.4019999504\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.7133 Validation Accuracy: 0.3999999762\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.6631 Validation Accuracy: 0.3983999789\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.6963 Validation Accuracy: 0.4061999619\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.6525 Validation Accuracy: 0.4117999673\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.6505 Validation Accuracy: 0.4187999666\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.6337 Validation Accuracy: 0.4083999395\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.5874 Validation Accuracy: 0.4165999591\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.5735 Validation Accuracy: 0.4225999415\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.5885 Validation Accuracy: 0.4193999469\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.5360 Validation Accuracy: 0.4289999902\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.5450 Validation Accuracy: 0.4319999516\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.5629 Validation Accuracy: 0.4269999862\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.5091 Validation Accuracy: 0.4329999983\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.4703 Validation Accuracy: 0.4373999536\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.4754 Validation Accuracy: 0.4415999651\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.5031 Validation Accuracy: 0.4445999563\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.4574 Validation Accuracy: 0.4451999664\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.4238 Validation Accuracy: 0.4477999508\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.4405 Validation Accuracy: 0.4471999407\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.4825 Validation Accuracy: 0.4441999793\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.4066 Validation Accuracy: 0.4515999556\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.4374 Validation Accuracy: 0.4603999853\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.4165 Validation Accuracy: 0.4525999427\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.3925 Validation Accuracy: 0.4601999819\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.4047 Validation Accuracy: 0.4689999819\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.4039 Validation Accuracy: 0.4709999859\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.3882 Validation Accuracy: 0.4725999534\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.3768 Validation Accuracy: 0.4707999825\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.3864 Validation Accuracy: 0.4717999697\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.3529 Validation Accuracy: 0.4747999907\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.3500 Validation Accuracy: 0.4739999771\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.3408 Validation Accuracy: 0.4825999141\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     1.3474 Validation Accuracy: 0.4779999852\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     1.3335 Validation Accuracy: 0.4825999737\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     1.3334 Validation Accuracy: 0.4795999527\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.3182 Validation Accuracy: 0.4881999493\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.3144 Validation Accuracy: 0.4825999737\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     1.2846 Validation Accuracy: 0.4857999682\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.3355 Validation Accuracy: 0.4811999798\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     1.2767 Validation Accuracy: 0.4885999560\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     1.2492 Validation Accuracy: 0.4885999560\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     1.2679 Validation Accuracy: 0.4915999174\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     1.2688 Validation Accuracy: 0.4881999493\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     1.2737 Validation Accuracy: 0.4949999750\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     1.2705 Validation Accuracy: 0.4873999357\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     1.2620 Validation Accuracy: 0.4989999533\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     1.2823 Validation Accuracy: 0.4925999343\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     1.2286 Validation Accuracy: 0.4959999621\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     1.2683 Validation Accuracy: 0.4959999323\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     1.2356 Validation Accuracy: 0.4935999513\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     1.1970 Validation Accuracy: 0.4977999628\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     1.2111 Validation Accuracy: 0.4983999431\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     1.1901 Validation Accuracy: 0.5031999350\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     1.2148 Validation Accuracy: 0.5011999607\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     1.1832 Validation Accuracy: 0.5021999478\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     1.1717 Validation Accuracy: 0.5027999878\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     1.1771 Validation Accuracy: 0.4995999336\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     1.1829 Validation Accuracy: 0.5099999309\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     1.1756 Validation Accuracy: 0.5072000027\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     1.1958 Validation Accuracy: 0.5058000088\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     1.1454 Validation Accuracy: 0.5067999363\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     1.1315 Validation Accuracy: 0.5131999254\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     1.1474 Validation Accuracy: 0.5079998970\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     1.1274 Validation Accuracy: 0.5073999166\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     1.1366 Validation Accuracy: 0.5071999431\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     1.1408 Validation Accuracy: 0.5143999457\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     1.1651 Validation Accuracy: 0.5071999431\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     1.1216 Validation Accuracy: 0.5097999573\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     1.1151 Validation Accuracy: 0.5063999295\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     1.1174 Validation Accuracy: 0.5059999228\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     1.1062 Validation Accuracy: 0.5141999125\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     1.0880 Validation Accuracy: 0.5119999647\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     1.1219 Validation Accuracy: 0.5109999776\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     1.0952 Validation Accuracy: 0.5157999396\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     1.1030 Validation Accuracy: 0.5105999708\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     1.1106 Validation Accuracy: 0.5113999248\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     1.0825 Validation Accuracy: 0.5103999972\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     1.0900 Validation Accuracy: 0.5195999742\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     1.0899 Validation Accuracy: 0.5123999119\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     1.1415 Validation Accuracy: 0.5109999180\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     1.0782 Validation Accuracy: 0.5121999383\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     1.0597 Validation Accuracy: 0.5199999213\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     1.0926 Validation Accuracy: 0.5191999078\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     1.0609 Validation Accuracy: 0.5183998942\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     1.0842 Validation Accuracy: 0.5093999505\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     1.0863 Validation Accuracy: 0.5127999783\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     1.0558 Validation Accuracy: 0.5165999532\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     1.0550 Validation Accuracy: 0.5235999823\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     1.0664 Validation Accuracy: 0.5209999084\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     1.0398 Validation Accuracy: 0.5151999593\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     1.0720 Validation Accuracy: 0.5243999362\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     1.0388 Validation Accuracy: 0.5209999681\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     1.0486 Validation Accuracy: 0.5188000202\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     1.0176 Validation Accuracy: 0.5145999193\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     1.0043 Validation Accuracy: 0.5139999390\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     1.0400 Validation Accuracy: 0.5165998936\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     1.0363 Validation Accuracy: 0.5163999796\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     1.0334 Validation Accuracy: 0.5233999491\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     1.0334 Validation Accuracy: 0.5175999403\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     1.0322 Validation Accuracy: 0.5180000067\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     1.0257 Validation Accuracy: 0.5155999064\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     1.0278 Validation Accuracy: 0.5191999078\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     1.0009 Validation Accuracy: 0.5205999613\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     1.0089 Validation Accuracy: 0.5217999220\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     0.9998 Validation Accuracy: 0.5167999268\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     1.0107 Validation Accuracy: 0.5193999410\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     0.9938 Validation Accuracy: 0.5083999634\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     0.9930 Validation Accuracy: 0.5195999146\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     0.9901 Validation Accuracy: 0.5135999918\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     0.9577 Validation Accuracy: 0.5205999613\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:     0.9619 Validation Accuracy: 0.5123999715\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:     0.9738 Validation Accuracy: 0.5215998888\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:     0.9703 Validation Accuracy: 0.5197999477\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:     0.9775 Validation Accuracy: 0.5151998997\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:     0.9938 Validation Accuracy: 0.5095999837\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:     0.9790 Validation Accuracy: 0.5115999579\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:     0.9678 Validation Accuracy: 0.5151999593\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:     1.0069 Validation Accuracy: 0.5187999606\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:     1.0031 Validation Accuracy: 0.5139999390\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:     0.9951 Validation Accuracy: 0.5197999477\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:     0.9907 Validation Accuracy: 0.5205999613\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:     0.9816 Validation Accuracy: 0.5187999010\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:     0.9513 Validation Accuracy: 0.5137999654\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:     0.9603 Validation Accuracy: 0.5199999809\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:     0.9586 Validation Accuracy: 0.5219999552\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:     0.9346 Validation Accuracy: 0.5173999071\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:     0.9404 Validation Accuracy: 0.5041999221\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:     0.9699 Validation Accuracy: 0.5077999234\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:     0.9278 Validation Accuracy: 0.5165999532\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:     0.9864 Validation Accuracy: 0.5069999695\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:     0.9671 Validation Accuracy: 0.5081999898\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:     0.9750 Validation Accuracy: 0.5047999620\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:     0.9875 Validation Accuracy: 0.5017999411\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:     0.9484 Validation Accuracy: 0.5111999512\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:     0.8964 Validation Accuracy: 0.5185999870\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:     0.9175 Validation Accuracy: 0.5141999125\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:     0.9481 Validation Accuracy: 0.5025999546\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:     0.9114 Validation Accuracy: 0.5125999451\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:     0.9763 Validation Accuracy: 0.4875999689\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:     0.9568 Validation Accuracy: 0.4940000176\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:     0.9415 Validation Accuracy: 0.5093999505\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:     0.9489 Validation Accuracy: 0.5103999376\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:     0.9655 Validation Accuracy: 0.5031999350\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:     0.9214 Validation Accuracy: 0.5133998990\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:     0.9265 Validation Accuracy: 0.4993999600\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:     0.9646 Validation Accuracy: 0.5069999695\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:     0.8933 Validation Accuracy: 0.4971999824\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:     0.8979 Validation Accuracy: 0.5119999647\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:     0.9116 Validation Accuracy: 0.5079999566\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:     0.8843 Validation Accuracy: 0.5067999363\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:     0.8965 Validation Accuracy: 0.5173999667\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:     0.8824 Validation Accuracy: 0.5081999302\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:     0.9067 Validation Accuracy: 0.5195999742\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:     0.8956 Validation Accuracy: 0.5153999329\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:     0.8800 Validation Accuracy: 0.5181999207\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:     0.9015 Validation Accuracy: 0.5109999776\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:     0.9178 Validation Accuracy: 0.5057999492\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:     0.8953 Validation Accuracy: 0.5149999857\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:     0.8813 Validation Accuracy: 0.5089999437\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:     0.9010 Validation Accuracy: 0.5091999769\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:     0.8972 Validation Accuracy: 0.5199999809\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:     0.9064 Validation Accuracy: 0.5033999681\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:     0.8707 Validation Accuracy: 0.5069999695\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:     0.9019 Validation Accuracy: 0.4997999668\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:     0.9083 Validation Accuracy: 0.5099999309\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:     0.8661 Validation Accuracy: 0.4999999404\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:     0.8727 Validation Accuracy: 0.5023999214\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:     0.8837 Validation Accuracy: 0.5139999390\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:     0.8687 Validation Accuracy: 0.5049999356\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:     0.8771 Validation Accuracy: 0.5195999742\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:     0.8689 Validation Accuracy: 0.5123999119\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:     0.8633 Validation Accuracy: 0.5157999396\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:     0.8811 Validation Accuracy: 0.5153999329\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:     0.8722 Validation Accuracy: 0.5131999254\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:     0.8988 Validation Accuracy: 0.5085999370\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:     0.8722 Validation Accuracy: 0.5123999119\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:     0.8413 Validation Accuracy: 0.5125999451\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:     0.9002 Validation Accuracy: 0.5139999390\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:     0.8777 Validation Accuracy: 0.5085999370\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:     0.9038 Validation Accuracy: 0.5037999749\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:     0.8530 Validation Accuracy: 0.5029999614\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:     0.8812 Validation Accuracy: 0.5095999241\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:     0.8751 Validation Accuracy: 0.5130000114\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:     0.9026 Validation Accuracy: 0.5125999451\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:     0.8513 Validation Accuracy: 0.5119999647\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:     0.8791 Validation Accuracy: 0.5057999492\n",
      "Epoch 201, CIFAR-10 Batch 1:  Loss:     0.8387 Validation Accuracy: 0.5045999289\n",
      "Epoch 202, CIFAR-10 Batch 1:  Loss:     0.8867 Validation Accuracy: 0.5143999457\n",
      "Epoch 203, CIFAR-10 Batch 1:  Loss:     0.8447 Validation Accuracy: 0.5215999484\n",
      "Epoch 204, CIFAR-10 Batch 1:  Loss:     0.8808 Validation Accuracy: 0.5119999051\n",
      "Epoch 205, CIFAR-10 Batch 1:  Loss:     0.9007 Validation Accuracy: 0.5061999559\n",
      "Epoch 206, CIFAR-10 Batch 1:  Loss:     0.8979 Validation Accuracy: 0.5035999417\n",
      "Epoch 207, CIFAR-10 Batch 1:  Loss:     0.9042 Validation Accuracy: 0.5067999363\n",
      "Epoch 208, CIFAR-10 Batch 1:  Loss:     0.8708 Validation Accuracy: 0.5125999451\n",
      "Epoch 209, CIFAR-10 Batch 1:  Loss:     0.8636 Validation Accuracy: 0.5139999390\n",
      "Epoch 210, CIFAR-10 Batch 1:  Loss:     0.9011 Validation Accuracy: 0.5162000060\n",
      "Epoch 211, CIFAR-10 Batch 1:  Loss:     0.8619 Validation Accuracy: 0.5143999457\n",
      "Epoch 212, CIFAR-10 Batch 1:  Loss:     0.8769 Validation Accuracy: 0.5159999728\n",
      "Epoch 213, CIFAR-10 Batch 1:  Loss:     0.8302 Validation Accuracy: 0.5137999654\n",
      "Epoch 214, CIFAR-10 Batch 1:  Loss:     0.8705 Validation Accuracy: 0.5137999654\n",
      "Epoch 215, CIFAR-10 Batch 1:  Loss:     0.8336 Validation Accuracy: 0.5167999864\n",
      "Epoch 216, CIFAR-10 Batch 1:  Loss:     0.8271 Validation Accuracy: 0.5157999992\n",
      "Epoch 217, CIFAR-10 Batch 1:  Loss:     0.8421 Validation Accuracy: 0.5119999051\n",
      "Epoch 218, CIFAR-10 Batch 1:  Loss:     0.8212 Validation Accuracy: 0.5141999722\n",
      "Epoch 219, CIFAR-10 Batch 1:  Loss:     0.8391 Validation Accuracy: 0.5131999254\n",
      "Epoch 220, CIFAR-10 Batch 1:  Loss:     0.8607 Validation Accuracy: 0.5125999451\n",
      "Epoch 221, CIFAR-10 Batch 1:  Loss:     0.8235 Validation Accuracy: 0.5107999444\n",
      "Epoch 222, CIFAR-10 Batch 1:  Loss:     0.8438 Validation Accuracy: 0.5179999471\n",
      "Epoch 223, CIFAR-10 Batch 1:  Loss:     0.8696 Validation Accuracy: 0.5167999864\n",
      "Epoch 224, CIFAR-10 Batch 1:  Loss:     0.8532 Validation Accuracy: 0.5135999322\n",
      "Epoch 225, CIFAR-10 Batch 1:  Loss:     0.8419 Validation Accuracy: 0.5157998800\n",
      "Epoch 226, CIFAR-10 Batch 1:  Loss:     0.8070 Validation Accuracy: 0.5144000053\n",
      "Epoch 227, CIFAR-10 Batch 1:  Loss:     0.8247 Validation Accuracy: 0.5127999187\n",
      "Epoch 228, CIFAR-10 Batch 1:  Loss:     0.8125 Validation Accuracy: 0.5147999525\n",
      "Epoch 229, CIFAR-10 Batch 1:  Loss:     0.8173 Validation Accuracy: 0.5155999660\n",
      "Epoch 230, CIFAR-10 Batch 1:  Loss:     0.8662 Validation Accuracy: 0.5113999844\n",
      "Epoch 231, CIFAR-10 Batch 1:  Loss:     0.8267 Validation Accuracy: 0.5175999403\n",
      "Epoch 232, CIFAR-10 Batch 1:  Loss:     0.7949 Validation Accuracy: 0.5153999329\n",
      "Epoch 233, CIFAR-10 Batch 1:  Loss:     0.8248 Validation Accuracy: 0.5149998665\n",
      "Epoch 234, CIFAR-10 Batch 1:  Loss:     0.8046 Validation Accuracy: 0.5099999309\n",
      "Epoch 235, CIFAR-10 Batch 1:  Loss:     0.8852 Validation Accuracy: 0.5073999763\n",
      "Epoch 236, CIFAR-10 Batch 1:  Loss:     0.8526 Validation Accuracy: 0.5111999512\n",
      "Epoch 237, CIFAR-10 Batch 1:  Loss:     0.8712 Validation Accuracy: 0.5071999431\n",
      "Epoch 238, CIFAR-10 Batch 1:  Loss:     0.7763 Validation Accuracy: 0.5155999660\n",
      "Epoch 239, CIFAR-10 Batch 1:  Loss:     0.8320 Validation Accuracy: 0.5209999084\n",
      "Epoch 240, CIFAR-10 Batch 1:  Loss:     0.8240 Validation Accuracy: 0.5125999451\n",
      "Epoch 241, CIFAR-10 Batch 1:  Loss:     0.8724 Validation Accuracy: 0.5039999485\n",
      "Epoch 242, CIFAR-10 Batch 1:  Loss:     0.8170 Validation Accuracy: 0.5131999254\n",
      "Epoch 243, CIFAR-10 Batch 1:  Loss:     0.8045 Validation Accuracy: 0.5177999735\n",
      "Epoch 244, CIFAR-10 Batch 1:  Loss:     0.8049 Validation Accuracy: 0.5051999092\n",
      "Epoch 245, CIFAR-10 Batch 1:  Loss:     0.8006 Validation Accuracy: 0.5027999282\n",
      "Epoch 246, CIFAR-10 Batch 1:  Loss:     0.7709 Validation Accuracy: 0.5121999383\n",
      "Epoch 247, CIFAR-10 Batch 1:  Loss:     0.8217 Validation Accuracy: 0.5109999776\n",
      "Epoch 248, CIFAR-10 Batch 1:  Loss:     0.8285 Validation Accuracy: 0.5079999566\n",
      "Epoch 249, CIFAR-10 Batch 1:  Loss:     0.8107 Validation Accuracy: 0.5077999234\n",
      "Epoch 250, CIFAR-10 Batch 1:  Loss:     0.8786 Validation Accuracy: 0.5135999322\n",
      "Epoch 251, CIFAR-10 Batch 1:  Loss:     0.8292 Validation Accuracy: 0.5057999492\n",
      "Epoch 252, CIFAR-10 Batch 1:  Loss:     0.8531 Validation Accuracy: 0.5007999539\n",
      "Epoch 253, CIFAR-10 Batch 1:  Loss:     0.7748 Validation Accuracy: 0.5103999376\n",
      "Epoch 254, CIFAR-10 Batch 1:  Loss:     0.8193 Validation Accuracy: 0.5057999492\n",
      "Epoch 255, CIFAR-10 Batch 1:  Loss:     0.9206 Validation Accuracy: 0.4935999215\n",
      "Epoch 256, CIFAR-10 Batch 1:  Loss:     0.7923 Validation Accuracy: 0.5097999573\n",
      "Epoch 257, CIFAR-10 Batch 1:  Loss:     0.8408 Validation Accuracy: 0.5105999708\n",
      "Epoch 258, CIFAR-10 Batch 1:  Loss:     0.8173 Validation Accuracy: 0.5098000169\n",
      "Epoch 259, CIFAR-10 Batch 1:  Loss:     0.7725 Validation Accuracy: 0.5153999329\n",
      "Epoch 260, CIFAR-10 Batch 1:  Loss:     0.7966 Validation Accuracy: 0.5019999146\n",
      "Epoch 261, CIFAR-10 Batch 1:  Loss:     0.7982 Validation Accuracy: 0.5109999180\n",
      "Epoch 262, CIFAR-10 Batch 1:  Loss:     0.8035 Validation Accuracy: 0.5119999647\n",
      "Epoch 263, CIFAR-10 Batch 1:  Loss:     0.8516 Validation Accuracy: 0.5031999946\n",
      "Epoch 264, CIFAR-10 Batch 1:  Loss:     0.8322 Validation Accuracy: 0.5019999743\n",
      "Epoch 265, CIFAR-10 Batch 1:  Loss:     0.8608 Validation Accuracy: 0.5001999736\n",
      "Epoch 266, CIFAR-10 Batch 1:  Loss:     0.8314 Validation Accuracy: 0.5083999634\n",
      "Epoch 267, CIFAR-10 Batch 1:  Loss:     0.8218 Validation Accuracy: 0.5095999837\n",
      "Epoch 268, CIFAR-10 Batch 1:  Loss:     0.8473 Validation Accuracy: 0.5059999228\n",
      "Epoch 269, CIFAR-10 Batch 1:  Loss:     0.8573 Validation Accuracy: 0.5039999485\n",
      "Epoch 270, CIFAR-10 Batch 1:  Loss:     0.7980 Validation Accuracy: 0.5149999857\n",
      "Epoch 271, CIFAR-10 Batch 1:  Loss:     0.8811 Validation Accuracy: 0.4991999567\n",
      "Epoch 272, CIFAR-10 Batch 1:  Loss:     0.8746 Validation Accuracy: 0.4939999580\n",
      "Epoch 273, CIFAR-10 Batch 1:  Loss:     0.8660 Validation Accuracy: 0.5005999804\n",
      "Epoch 274, CIFAR-10 Batch 1:  Loss:     0.7928 Validation Accuracy: 0.5117999315\n",
      "Epoch 275, CIFAR-10 Batch 1:  Loss:     0.8639 Validation Accuracy: 0.4965999126\n",
      "Epoch 276, CIFAR-10 Batch 1:  Loss:     0.8177 Validation Accuracy: 0.5069999695\n",
      "Epoch 277, CIFAR-10 Batch 1:  Loss:     0.8526 Validation Accuracy: 0.5029999614\n",
      "Epoch 278, CIFAR-10 Batch 1:  Loss:     0.8835 Validation Accuracy: 0.4991999567\n",
      "Epoch 279, CIFAR-10 Batch 1:  Loss:     0.8443 Validation Accuracy: 0.5039999485\n",
      "Epoch 280, CIFAR-10 Batch 1:  Loss:     0.9202 Validation Accuracy: 0.4981999695\n",
      "Epoch 281, CIFAR-10 Batch 1:  Loss:     0.8814 Validation Accuracy: 0.5095999837\n",
      "Epoch 282, CIFAR-10 Batch 1:  Loss:     0.8315 Validation Accuracy: 0.5089999437\n",
      "Epoch 283, CIFAR-10 Batch 1:  Loss:     0.7658 Validation Accuracy: 0.5089999437\n",
      "Epoch 284, CIFAR-10 Batch 1:  Loss:     0.8206 Validation Accuracy: 0.5054000020\n",
      "Epoch 285, CIFAR-10 Batch 1:  Loss:     0.8395 Validation Accuracy: 0.5097999573\n",
      "Epoch 286, CIFAR-10 Batch 1:  Loss:     0.8189 Validation Accuracy: 0.5105999708\n",
      "Epoch 287, CIFAR-10 Batch 1:  Loss:     0.8116 Validation Accuracy: 0.5065999627\n",
      "Epoch 288, CIFAR-10 Batch 1:  Loss:     0.9111 Validation Accuracy: 0.5023999214\n",
      "Epoch 289, CIFAR-10 Batch 1:  Loss:     0.8337 Validation Accuracy: 0.5047999620\n",
      "Epoch 290, CIFAR-10 Batch 1:  Loss:     0.7955 Validation Accuracy: 0.5131999254\n",
      "Epoch 291, CIFAR-10 Batch 1:  Loss:     0.7568 Validation Accuracy: 0.5121999979\n",
      "Epoch 292, CIFAR-10 Batch 1:  Loss:     0.8473 Validation Accuracy: 0.5083999634\n",
      "Epoch 293, CIFAR-10 Batch 1:  Loss:     0.8514 Validation Accuracy: 0.5059999228\n",
      "Epoch 294, CIFAR-10 Batch 1:  Loss:     0.7986 Validation Accuracy: 0.5073999763\n",
      "Epoch 295, CIFAR-10 Batch 1:  Loss:     0.8097 Validation Accuracy: 0.5093998909\n",
      "Epoch 296, CIFAR-10 Batch 1:  Loss:     0.8156 Validation Accuracy: 0.5135999918\n",
      "Epoch 297, CIFAR-10 Batch 1:  Loss:     0.8740 Validation Accuracy: 0.5081999302\n",
      "Epoch 298, CIFAR-10 Batch 1:  Loss:     0.8140 Validation Accuracy: 0.5101999044\n",
      "Epoch 299, CIFAR-10 Batch 1:  Loss:     0.8277 Validation Accuracy: 0.5167999864\n",
      "Epoch 300, CIFAR-10 Batch 1:  Loss:     0.7993 Validation Accuracy: 0.5170000196\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:     2.2760 Validation Accuracy: 0.1440000087\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:     2.3122 Validation Accuracy: 0.1597999930\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:     2.1826 Validation Accuracy: 0.2081999779\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:     2.0480 Validation Accuracy: 0.2605999708\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:     2.0663 Validation Accuracy: 0.2779999971\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:     2.0496 Validation Accuracy: 0.2925999761\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:     2.0146 Validation Accuracy: 0.3241999745\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:     1.7859 Validation Accuracy: 0.3243999779\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:     1.8056 Validation Accuracy: 0.3579999804\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:     1.7788 Validation Accuracy: 0.3622000217\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:     1.8104 Validation Accuracy: 0.3741999865\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:     1.8849 Validation Accuracy: 0.3773999810\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:     1.6000 Validation Accuracy: 0.3805999756\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:     1.6695 Validation Accuracy: 0.4025999904\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:     1.7492 Validation Accuracy: 0.3939999938\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:     1.7266 Validation Accuracy: 0.4029999673\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:     1.8192 Validation Accuracy: 0.4047999680\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:     1.5213 Validation Accuracy: 0.4205999672\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:     1.6567 Validation Accuracy: 0.4264000058\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:     1.6741 Validation Accuracy: 0.4289999604\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:     1.6796 Validation Accuracy: 0.4296000004\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:     1.7373 Validation Accuracy: 0.4257999659\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:     1.4703 Validation Accuracy: 0.4319999516\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:     1.6247 Validation Accuracy: 0.4271999896\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:     1.6410 Validation Accuracy: 0.4327999651\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:     1.6497 Validation Accuracy: 0.4349999726\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:     1.7033 Validation Accuracy: 0.4307999611\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:     1.4184 Validation Accuracy: 0.4333999753\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:     1.5538 Validation Accuracy: 0.4411999583\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:     1.6180 Validation Accuracy: 0.4391999543\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:     1.6726 Validation Accuracy: 0.4377999902\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:     1.6646 Validation Accuracy: 0.4325999618\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:     1.3922 Validation Accuracy: 0.4377999604\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:     1.5805 Validation Accuracy: 0.4357999563\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:     1.5467 Validation Accuracy: 0.4539999664\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:     1.5981 Validation Accuracy: 0.4517999887\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:     1.6442 Validation Accuracy: 0.4457999468\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:     1.3443 Validation Accuracy: 0.4545999765\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:     1.5353 Validation Accuracy: 0.4477999806\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:     1.5681 Validation Accuracy: 0.4563999772\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:     1.6191 Validation Accuracy: 0.4517999291\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:     1.5788 Validation Accuracy: 0.4617999792\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:     1.3066 Validation Accuracy: 0.4541999996\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:     1.4991 Validation Accuracy: 0.4397999644\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:     1.5370 Validation Accuracy: 0.4567999840\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:     1.5756 Validation Accuracy: 0.4623999596\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:     1.5269 Validation Accuracy: 0.4725999832\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:     1.2852 Validation Accuracy: 0.4655999541\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:     1.4914 Validation Accuracy: 0.4513999522\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:     1.4943 Validation Accuracy: 0.4753999710\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:     1.6092 Validation Accuracy: 0.4633999765\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:     1.5143 Validation Accuracy: 0.4767999649\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:     1.2902 Validation Accuracy: 0.4631999731\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:     1.5072 Validation Accuracy: 0.4631999731\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:     1.4591 Validation Accuracy: 0.4807999730\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:     1.5584 Validation Accuracy: 0.4819999635\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:     1.4712 Validation Accuracy: 0.4857999384\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:     1.2955 Validation Accuracy: 0.4607999623\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:     1.4676 Validation Accuracy: 0.4731999338\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:     1.4619 Validation Accuracy: 0.4791999459\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:     1.5788 Validation Accuracy: 0.4797999859\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:     1.5353 Validation Accuracy: 0.4831999540\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:     1.2596 Validation Accuracy: 0.4709999859\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:     1.4467 Validation Accuracy: 0.4683999717\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:     1.3934 Validation Accuracy: 0.4929999709\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:     1.5349 Validation Accuracy: 0.4821999967\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:     1.4739 Validation Accuracy: 0.4933999479\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:     1.2518 Validation Accuracy: 0.4549999535\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:     1.4595 Validation Accuracy: 0.4863999486\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:     1.3822 Validation Accuracy: 0.4893999994\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:     1.5533 Validation Accuracy: 0.4889999330\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:     1.4795 Validation Accuracy: 0.4955999255\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:     1.2754 Validation Accuracy: 0.4683999419\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:     1.4445 Validation Accuracy: 0.4823999703\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:     1.3984 Validation Accuracy: 0.4939999878\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:     1.5029 Validation Accuracy: 0.4955999255\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:     1.5068 Validation Accuracy: 0.4921999574\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:     1.2250 Validation Accuracy: 0.4769999981\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:     1.4426 Validation Accuracy: 0.4765999913\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:     1.3439 Validation Accuracy: 0.4927999675\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:     1.5058 Validation Accuracy: 0.4789999723\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:     1.4420 Validation Accuracy: 0.5059999228\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:     1.1991 Validation Accuracy: 0.4831999242\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:     1.4248 Validation Accuracy: 0.4907999337\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:     1.3604 Validation Accuracy: 0.4953999817\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:     1.4895 Validation Accuracy: 0.4939999580\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:     1.4174 Validation Accuracy: 0.4967999756\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:     1.2393 Validation Accuracy: 0.4747999609\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:     1.4529 Validation Accuracy: 0.4929999709\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:     1.3366 Validation Accuracy: 0.5019999743\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:     1.5358 Validation Accuracy: 0.4941999316\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:     1.4526 Validation Accuracy: 0.5076000094\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:     1.2170 Validation Accuracy: 0.4723999500\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:     1.4595 Validation Accuracy: 0.4851999581\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:     1.3363 Validation Accuracy: 0.5005999804\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:     1.5043 Validation Accuracy: 0.4973999560\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:     1.4254 Validation Accuracy: 0.5045999289\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:     1.1797 Validation Accuracy: 0.4853999317\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:     1.4181 Validation Accuracy: 0.4959999621\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:     1.3011 Validation Accuracy: 0.5063999295\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:     1.4828 Validation Accuracy: 0.4981999695\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:     1.4523 Validation Accuracy: 0.5067999363\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:     1.1605 Validation Accuracy: 0.4869999588\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:     1.4186 Validation Accuracy: 0.5021998882\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:     1.3122 Validation Accuracy: 0.5049999356\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:     1.4858 Validation Accuracy: 0.5105999112\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:     1.4608 Validation Accuracy: 0.5131999254\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:     1.2006 Validation Accuracy: 0.4867999554\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:     1.4333 Validation Accuracy: 0.4995999932\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:     1.3128 Validation Accuracy: 0.4997999072\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:     1.4520 Validation Accuracy: 0.5127999187\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:     1.4527 Validation Accuracy: 0.5107999444\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:     1.2400 Validation Accuracy: 0.4771999717\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:     1.4099 Validation Accuracy: 0.4939999580\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:     1.2667 Validation Accuracy: 0.5126000047\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:     1.4515 Validation Accuracy: 0.5049999952\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:     1.4067 Validation Accuracy: 0.5027999282\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:     1.2264 Validation Accuracy: 0.4659999609\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:     1.3985 Validation Accuracy: 0.5075999498\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:     1.2787 Validation Accuracy: 0.5079999566\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:     1.4628 Validation Accuracy: 0.5080000162\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:     1.3867 Validation Accuracy: 0.5141999125\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:     1.1551 Validation Accuracy: 0.4819999635\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:     1.3993 Validation Accuracy: 0.4979999661\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:     1.2710 Validation Accuracy: 0.5211999416\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:     1.4933 Validation Accuracy: 0.5131999850\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:     1.3598 Validation Accuracy: 0.5199999213\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:     1.1677 Validation Accuracy: 0.4997999370\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:     1.3669 Validation Accuracy: 0.5119999647\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:     1.2568 Validation Accuracy: 0.5183999538\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:     1.4444 Validation Accuracy: 0.5145999193\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:     1.3969 Validation Accuracy: 0.5165999532\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:     1.1484 Validation Accuracy: 0.5011999607\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:     1.3313 Validation Accuracy: 0.5227999687\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:     1.2412 Validation Accuracy: 0.5211999416\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:     1.4554 Validation Accuracy: 0.5093999505\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:     1.4019 Validation Accuracy: 0.5147999525\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:     1.1575 Validation Accuracy: 0.5125999451\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:     1.4059 Validation Accuracy: 0.5195999742\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:     1.2574 Validation Accuracy: 0.5189999342\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:     1.4023 Validation Accuracy: 0.5249999166\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:     1.3244 Validation Accuracy: 0.5271999240\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:     1.1891 Validation Accuracy: 0.4887999594\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:     1.3705 Validation Accuracy: 0.5099999309\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:     1.2377 Validation Accuracy: 0.5287999511\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:     1.4037 Validation Accuracy: 0.5199999809\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:     1.3632 Validation Accuracy: 0.5325999260\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:     1.1680 Validation Accuracy: 0.4891999960\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:     1.3465 Validation Accuracy: 0.5249999762\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:     1.2322 Validation Accuracy: 0.5299999714\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:     1.4348 Validation Accuracy: 0.5039999485\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:     1.3330 Validation Accuracy: 0.5237999558\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:     1.1999 Validation Accuracy: 0.4873999655\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:     1.3545 Validation Accuracy: 0.5163999796\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:     1.2129 Validation Accuracy: 0.5329999328\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:     1.4008 Validation Accuracy: 0.5161999464\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:     1.2792 Validation Accuracy: 0.5311999321\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:     1.1400 Validation Accuracy: 0.5139999390\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:     1.3385 Validation Accuracy: 0.5295999646\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:     1.2154 Validation Accuracy: 0.5331999660\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:     1.3888 Validation Accuracy: 0.5194000006\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:     1.3552 Validation Accuracy: 0.5333999395\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:     1.1425 Validation Accuracy: 0.5029999018\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:     1.3052 Validation Accuracy: 0.5375999212\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:     1.2275 Validation Accuracy: 0.5307999849\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:     1.3620 Validation Accuracy: 0.5317999125\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:     1.3436 Validation Accuracy: 0.5379999280\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:     1.1835 Validation Accuracy: 0.4963999987\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:     1.3091 Validation Accuracy: 0.5161999464\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:     1.2040 Validation Accuracy: 0.5375999212\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:     1.3487 Validation Accuracy: 0.5273998976\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:     1.2786 Validation Accuracy: 0.5388000011\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:     1.1582 Validation Accuracy: 0.5177999735\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:     1.3230 Validation Accuracy: 0.5245999098\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:     1.2002 Validation Accuracy: 0.5425999165\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:     1.2933 Validation Accuracy: 0.5345999002\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:     1.2288 Validation Accuracy: 0.5407999754\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:     1.1482 Validation Accuracy: 0.5167999268\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:     1.2905 Validation Accuracy: 0.5233999491\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:     1.1984 Validation Accuracy: 0.5361999273\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:     1.3240 Validation Accuracy: 0.5365999937\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:     1.2464 Validation Accuracy: 0.5399999022\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:     1.1090 Validation Accuracy: 0.5253999233\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:     1.3021 Validation Accuracy: 0.5397999287\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:     1.2021 Validation Accuracy: 0.5487999320\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:     1.3326 Validation Accuracy: 0.5429999232\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:     1.2513 Validation Accuracy: 0.5393999219\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:     1.1151 Validation Accuracy: 0.5191999078\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:     1.3068 Validation Accuracy: 0.5337999463\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:     1.1805 Validation Accuracy: 0.5383999348\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:     1.3491 Validation Accuracy: 0.5421999097\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:     1.2084 Validation Accuracy: 0.5493999720\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:     1.0984 Validation Accuracy: 0.5187999606\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:     1.3110 Validation Accuracy: 0.5463999510\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:     1.2009 Validation Accuracy: 0.5517998934\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:     1.3176 Validation Accuracy: 0.5355999470\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:     1.2557 Validation Accuracy: 0.5451999903\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:     1.0814 Validation Accuracy: 0.5299999714\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:     1.2542 Validation Accuracy: 0.5377999544\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:     1.1699 Validation Accuracy: 0.5491999388\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:     1.3211 Validation Accuracy: 0.5387999415\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:     1.2630 Validation Accuracy: 0.5499999523\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:     1.1245 Validation Accuracy: 0.5205999613\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:     1.2890 Validation Accuracy: 0.5407999158\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:     1.1882 Validation Accuracy: 0.5551999211\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:     1.3388 Validation Accuracy: 0.5363999009\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:     1.2640 Validation Accuracy: 0.5489999056\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:     1.1119 Validation Accuracy: 0.5241999626\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:     1.2581 Validation Accuracy: 0.5541999340\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:     1.2082 Validation Accuracy: 0.5451999307\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:     1.3127 Validation Accuracy: 0.5441999435\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:     1.1919 Validation Accuracy: 0.5569999814\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:     1.0421 Validation Accuracy: 0.5405999422\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:     1.2759 Validation Accuracy: 0.5369999409\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:     1.1543 Validation Accuracy: 0.5589999557\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:     1.3028 Validation Accuracy: 0.5433999300\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:     1.2009 Validation Accuracy: 0.5535999537\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:     1.1174 Validation Accuracy: 0.5293999314\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:     1.2813 Validation Accuracy: 0.5313999653\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:     1.1959 Validation Accuracy: 0.5579999089\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:     1.3212 Validation Accuracy: 0.5595999360\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:     1.1574 Validation Accuracy: 0.5501999259\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:     1.1163 Validation Accuracy: 0.5319999456\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:     1.2307 Validation Accuracy: 0.5367999673\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:     1.1667 Validation Accuracy: 0.5565999150\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:     1.2971 Validation Accuracy: 0.5431998968\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:     1.2138 Validation Accuracy: 0.5533999205\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:     1.1133 Validation Accuracy: 0.5279998779\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:     1.2434 Validation Accuracy: 0.5321999192\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:     1.1556 Validation Accuracy: 0.5619999170\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:     1.3108 Validation Accuracy: 0.5481999516\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:     1.1869 Validation Accuracy: 0.5471999049\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:     1.1034 Validation Accuracy: 0.5451999307\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:     1.2363 Validation Accuracy: 0.5579998493\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:     1.1468 Validation Accuracy: 0.5497999191\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:     1.3155 Validation Accuracy: 0.5505999327\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:     1.2276 Validation Accuracy: 0.5603999496\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:     1.1005 Validation Accuracy: 0.5375999212\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:     1.2138 Validation Accuracy: 0.5455999374\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:     1.1509 Validation Accuracy: 0.5635999441\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:     1.2780 Validation Accuracy: 0.5501999259\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:     1.1897 Validation Accuracy: 0.5615999103\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:     1.0213 Validation Accuracy: 0.5559999347\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:     1.2306 Validation Accuracy: 0.5475999117\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:     1.1677 Validation Accuracy: 0.5663999319\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:     1.3489 Validation Accuracy: 0.5455999374\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:     1.2623 Validation Accuracy: 0.5569999814\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:     1.0429 Validation Accuracy: 0.5460000038\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:     1.2392 Validation Accuracy: 0.5537999272\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:     1.1345 Validation Accuracy: 0.5621999502\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:     1.2749 Validation Accuracy: 0.5535998940\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:     1.2220 Validation Accuracy: 0.5603998899\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:     1.0602 Validation Accuracy: 0.5439999104\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:     1.2149 Validation Accuracy: 0.5515999198\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:     1.1112 Validation Accuracy: 0.5689999461\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:     1.2925 Validation Accuracy: 0.5527998805\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:     1.2103 Validation Accuracy: 0.5629999638\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:     1.0922 Validation Accuracy: 0.5437999368\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:     1.2368 Validation Accuracy: 0.5587999225\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:     1.1386 Validation Accuracy: 0.5643998981\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:     1.3126 Validation Accuracy: 0.5535998940\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:     1.1706 Validation Accuracy: 0.5653999448\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:     1.0658 Validation Accuracy: 0.5491999388\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:     1.1711 Validation Accuracy: 0.5567999482\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:     1.1205 Validation Accuracy: 0.5689999461\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:     1.2799 Validation Accuracy: 0.5525999069\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:     1.2252 Validation Accuracy: 0.5559998751\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:     1.0351 Validation Accuracy: 0.5521999002\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:     1.2429 Validation Accuracy: 0.5519999266\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:     1.1195 Validation Accuracy: 0.5659999251\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:     1.3089 Validation Accuracy: 0.5611999035\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:     1.1445 Validation Accuracy: 0.5639999509\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:     1.1045 Validation Accuracy: 0.5455999374\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:     1.2217 Validation Accuracy: 0.5579999685\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:     1.0782 Validation Accuracy: 0.5639998913\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:     1.2749 Validation Accuracy: 0.5565999746\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:     1.1642 Validation Accuracy: 0.5701999664\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:     0.9846 Validation Accuracy: 0.5517998934\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:     1.2407 Validation Accuracy: 0.5643999577\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:     1.0608 Validation Accuracy: 0.5677999258\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:     1.2485 Validation Accuracy: 0.5555999279\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:     1.1375 Validation Accuracy: 0.5695998669\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:     1.0417 Validation Accuracy: 0.5579999089\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:     1.2033 Validation Accuracy: 0.5685999393\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:     1.1142 Validation Accuracy: 0.5653999448\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:     1.2163 Validation Accuracy: 0.5573999882\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:     1.1850 Validation Accuracy: 0.5669999123\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:     1.0948 Validation Accuracy: 0.5460000038\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:     1.2205 Validation Accuracy: 0.5699999332\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:     1.0743 Validation Accuracy: 0.5727999806\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:     1.2278 Validation Accuracy: 0.5553998947\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:     1.1723 Validation Accuracy: 0.5687999129\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:     1.1508 Validation Accuracy: 0.5185999274\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:     1.2055 Validation Accuracy: 0.5633999109\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:     1.1133 Validation Accuracy: 0.5743999481\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:     1.2566 Validation Accuracy: 0.5653999448\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:     1.1244 Validation Accuracy: 0.5669999123\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:     0.9882 Validation Accuracy: 0.5679999590\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:     1.1845 Validation Accuracy: 0.5661999583\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:     1.0939 Validation Accuracy: 0.5713999271\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:     1.2615 Validation Accuracy: 0.5585999489\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:     1.1337 Validation Accuracy: 0.5699998736\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:     1.0231 Validation Accuracy: 0.5559999347\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:     1.2141 Validation Accuracy: 0.5687998533\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:     1.0971 Validation Accuracy: 0.5685999393\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:     1.2084 Validation Accuracy: 0.5647999048\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:     1.1495 Validation Accuracy: 0.5717999339\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:     1.0021 Validation Accuracy: 0.5625999570\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:     1.1931 Validation Accuracy: 0.5623999238\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:     1.0952 Validation Accuracy: 0.5773999691\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:     1.1861 Validation Accuracy: 0.5641999245\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:     1.1669 Validation Accuracy: 0.5607998967\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:     0.9861 Validation Accuracy: 0.5603998899\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:     1.1909 Validation Accuracy: 0.5661998987\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:     1.1016 Validation Accuracy: 0.5715999007\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:     1.2469 Validation Accuracy: 0.5661999583\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:     1.1050 Validation Accuracy: 0.5725998878\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:     1.0055 Validation Accuracy: 0.5607999563\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:     1.1944 Validation Accuracy: 0.5629999042\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:     1.0884 Validation Accuracy: 0.5701999068\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:     1.1978 Validation Accuracy: 0.5727999210\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:     1.1353 Validation Accuracy: 0.5715999007\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:     1.0091 Validation Accuracy: 0.5617999434\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:     1.1693 Validation Accuracy: 0.5665998459\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:     1.0559 Validation Accuracy: 0.5733999014\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:     1.1518 Validation Accuracy: 0.5689999461\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:     1.1467 Validation Accuracy: 0.5665999651\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:     0.9437 Validation Accuracy: 0.5643998981\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:     1.1762 Validation Accuracy: 0.5711998940\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:     1.0552 Validation Accuracy: 0.5725998878\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:     1.2040 Validation Accuracy: 0.5695999265\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:     1.1897 Validation Accuracy: 0.5673998594\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:     1.0136 Validation Accuracy: 0.5647999048\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:     1.1765 Validation Accuracy: 0.5695999861\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:     1.0784 Validation Accuracy: 0.5773999691\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:     1.1786 Validation Accuracy: 0.5659999251\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:     1.1685 Validation Accuracy: 0.5713999271\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:     1.0077 Validation Accuracy: 0.5663999319\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:     1.1764 Validation Accuracy: 0.5733999014\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:     1.0682 Validation Accuracy: 0.5819998980\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:     1.1491 Validation Accuracy: 0.5691999197\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:     1.1784 Validation Accuracy: 0.5687999129\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:     1.0820 Validation Accuracy: 0.5495999455\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:     1.1528 Validation Accuracy: 0.5787999630\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:     1.0825 Validation Accuracy: 0.5741999745\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:     1.1716 Validation Accuracy: 0.5705999732\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:     1.1253 Validation Accuracy: 0.5669999123\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:     1.0106 Validation Accuracy: 0.5567999482\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:     1.1228 Validation Accuracy: 0.5703999400\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:     1.0489 Validation Accuracy: 0.5809999704\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:     1.1844 Validation Accuracy: 0.5701999068\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:     1.1345 Validation Accuracy: 0.5795999169\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:     1.0251 Validation Accuracy: 0.5575999618\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:     1.1723 Validation Accuracy: 0.5737999678\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:     1.0616 Validation Accuracy: 0.5783998966\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:     1.1965 Validation Accuracy: 0.5695999265\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:     1.1314 Validation Accuracy: 0.5773999691\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:     1.0122 Validation Accuracy: 0.5689999461\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:     1.1453 Validation Accuracy: 0.5671999454\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:     1.0859 Validation Accuracy: 0.5773999095\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:     1.2046 Validation Accuracy: 0.5813999176\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:     1.1348 Validation Accuracy: 0.5729998946\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:     1.0011 Validation Accuracy: 0.5647999644\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:     1.1571 Validation Accuracy: 0.5753999352\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:     1.0350 Validation Accuracy: 0.5775998831\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:     1.1596 Validation Accuracy: 0.5775998831\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:     1.1508 Validation Accuracy: 0.5667999387\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:     1.0307 Validation Accuracy: 0.5643999577\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:     1.1362 Validation Accuracy: 0.5755999088\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:     1.0297 Validation Accuracy: 0.5863999128\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:     1.1683 Validation Accuracy: 0.5701999664\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:     1.1486 Validation Accuracy: 0.5745999217\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:     1.0297 Validation Accuracy: 0.5673999190\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:     1.0873 Validation Accuracy: 0.5751999021\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:     1.1027 Validation Accuracy: 0.5741999149\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:     1.2016 Validation Accuracy: 0.5713999271\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:     1.1445 Validation Accuracy: 0.5821999311\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:     1.0033 Validation Accuracy: 0.5679999590\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:     1.1609 Validation Accuracy: 0.5771998763\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:     1.0507 Validation Accuracy: 0.5809999704\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:     1.1553 Validation Accuracy: 0.5737999082\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:     1.1247 Validation Accuracy: 0.5777999163\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:     1.0008 Validation Accuracy: 0.5691999197\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:     1.1070 Validation Accuracy: 0.5789998770\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:     1.0710 Validation Accuracy: 0.5789998770\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:     1.2154 Validation Accuracy: 0.5709999204\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:     1.1641 Validation Accuracy: 0.5713999271\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:     1.0117 Validation Accuracy: 0.5699999332\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:     1.1407 Validation Accuracy: 0.5779999495\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:     1.0731 Validation Accuracy: 0.5703999400\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:     1.2121 Validation Accuracy: 0.5749999285\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:     1.1367 Validation Accuracy: 0.5731999278\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:     1.0508 Validation Accuracy: 0.5487999320\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:     1.1698 Validation Accuracy: 0.5667999387\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:     1.0719 Validation Accuracy: 0.5771999359\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:     1.1540 Validation Accuracy: 0.5799999237\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:     1.1033 Validation Accuracy: 0.5865998864\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:     1.0877 Validation Accuracy: 0.5481999516\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:     1.1621 Validation Accuracy: 0.5837999582\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:     1.0635 Validation Accuracy: 0.5795999169\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:     1.1922 Validation Accuracy: 0.5805999041\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:     1.1170 Validation Accuracy: 0.5737999678\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:     1.0136 Validation Accuracy: 0.5639998913\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:     1.1527 Validation Accuracy: 0.5789999366\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:     1.0677 Validation Accuracy: 0.5777999163\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:     1.2129 Validation Accuracy: 0.5639999509\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:     1.1176 Validation Accuracy: 0.5823999643\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:     1.0075 Validation Accuracy: 0.5651999712\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:     1.1589 Validation Accuracy: 0.5817999244\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:     1.0212 Validation Accuracy: 0.5763999820\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:     1.1489 Validation Accuracy: 0.5807999372\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:     1.1076 Validation Accuracy: 0.5853999257\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:     1.0132 Validation Accuracy: 0.5689999461\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:     1.1479 Validation Accuracy: 0.5747999549\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:     1.0346 Validation Accuracy: 0.5763999224\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:     1.1594 Validation Accuracy: 0.5825999379\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:     1.0964 Validation Accuracy: 0.5783998966\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:     1.0078 Validation Accuracy: 0.5713999271\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:     1.1248 Validation Accuracy: 0.5789998770\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:     1.0333 Validation Accuracy: 0.5751999617\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:     1.1659 Validation Accuracy: 0.5773999691\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:     1.1010 Validation Accuracy: 0.5835999250\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:     1.0256 Validation Accuracy: 0.5625999570\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:     1.2087 Validation Accuracy: 0.5737999082\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:     1.0223 Validation Accuracy: 0.5831999183\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:     1.0907 Validation Accuracy: 0.5799999237\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:     1.0872 Validation Accuracy: 0.5829998851\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:     1.0805 Validation Accuracy: 0.5565999746\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:     1.1373 Validation Accuracy: 0.5771999359\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:     1.0389 Validation Accuracy: 0.5807999372\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:     1.1508 Validation Accuracy: 0.5801999569\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:     1.1223 Validation Accuracy: 0.5847999454\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:     1.0298 Validation Accuracy: 0.5601999760\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:     1.1832 Validation Accuracy: 0.5667998791\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:     1.0244 Validation Accuracy: 0.5827999115\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:     1.1256 Validation Accuracy: 0.5843999386\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:     1.0882 Validation Accuracy: 0.5809999704\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:     1.0026 Validation Accuracy: 0.5663999319\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:     1.1125 Validation Accuracy: 0.5723999143\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:     1.0656 Validation Accuracy: 0.5797999501\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:     1.1400 Validation Accuracy: 0.5631999373\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:     1.1217 Validation Accuracy: 0.5769999027\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:     1.0670 Validation Accuracy: 0.5561999679\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:     1.1427 Validation Accuracy: 0.5769999027\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:     1.0347 Validation Accuracy: 0.5807999372\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:     1.1468 Validation Accuracy: 0.5805999041\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:     1.1558 Validation Accuracy: 0.5805999041\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:     1.0187 Validation Accuracy: 0.5733999610\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:     1.0867 Validation Accuracy: 0.5815999508\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:     1.0544 Validation Accuracy: 0.5793999434\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:     1.1876 Validation Accuracy: 0.5803999305\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:     1.1441 Validation Accuracy: 0.5825999379\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:     1.0533 Validation Accuracy: 0.5681999922\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:     1.0763 Validation Accuracy: 0.5787999034\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:     1.0196 Validation Accuracy: 0.5819998980\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:     1.1187 Validation Accuracy: 0.5849999785\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:     1.1330 Validation Accuracy: 0.5873999000\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:     0.9983 Validation Accuracy: 0.5811998844\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:     1.0364 Validation Accuracy: 0.5811998844\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:     1.0753 Validation Accuracy: 0.5811998844\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:     1.1288 Validation Accuracy: 0.5817998648\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:     1.1134 Validation Accuracy: 0.5893999338\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:     0.9984 Validation Accuracy: 0.5753999352\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:     1.0957 Validation Accuracy: 0.5837998986\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:     1.1015 Validation Accuracy: 0.5847999454\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:     1.0963 Validation Accuracy: 0.5805999637\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:     1.0660 Validation Accuracy: 0.5919998884\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:     1.0419 Validation Accuracy: 0.5677999258\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:     1.1166 Validation Accuracy: 0.5831999183\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:     1.0834 Validation Accuracy: 0.5723999143\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:     1.0916 Validation Accuracy: 0.5883998871\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:     1.0921 Validation Accuracy: 0.5898000002\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:     1.0610 Validation Accuracy: 0.5573999286\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:     1.1077 Validation Accuracy: 0.5839998722\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:     1.0423 Validation Accuracy: 0.5837999582\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:     1.0881 Validation Accuracy: 0.5839998126\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:     1.1178 Validation Accuracy: 0.5879999399\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:     1.0385 Validation Accuracy: 0.5715999603\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:     1.0539 Validation Accuracy: 0.5727999210\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:     1.0625 Validation Accuracy: 0.5747999549\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:     1.1422 Validation Accuracy: 0.5763999224\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:     1.1314 Validation Accuracy: 0.5835999250\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:     0.9771 Validation Accuracy: 0.5781999230\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:     1.0412 Validation Accuracy: 0.5823998451\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:     1.0312 Validation Accuracy: 0.5789999962\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:     1.1342 Validation Accuracy: 0.5807999372\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:     1.0705 Validation Accuracy: 0.5815998912\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:     1.0393 Validation Accuracy: 0.5645999908\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:     1.1019 Validation Accuracy: 0.5775998831\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:     1.0454 Validation Accuracy: 0.5841999650\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:     1.1192 Validation Accuracy: 0.5859999061\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:     1.0722 Validation Accuracy: 0.5897998810\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:     0.9989 Validation Accuracy: 0.5777999163\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:     1.0608 Validation Accuracy: 0.5737999678\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:     1.0457 Validation Accuracy: 0.5917998552\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:     1.1322 Validation Accuracy: 0.5699998736\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:     1.0757 Validation Accuracy: 0.5799999833\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:     1.0000 Validation Accuracy: 0.5777999163\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:     1.1153 Validation Accuracy: 0.5743999481\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:     1.0759 Validation Accuracy: 0.5821999311\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:     1.1565 Validation Accuracy: 0.5841999054\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:     1.0910 Validation Accuracy: 0.5819998980\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:     0.9936 Validation Accuracy: 0.5797998905\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:     1.0946 Validation Accuracy: 0.5773999095\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:     1.0920 Validation Accuracy: 0.5825999379\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:     1.1180 Validation Accuracy: 0.5853999257\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:     1.1465 Validation Accuracy: 0.5821999311\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:     0.9575 Validation Accuracy: 0.5769999027\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:     1.0953 Validation Accuracy: 0.5815998912\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:     1.0612 Validation Accuracy: 0.5795999169\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:     1.1328 Validation Accuracy: 0.5693999529\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:     1.1477 Validation Accuracy: 0.5851999521\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:     1.0571 Validation Accuracy: 0.5725998878\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:     1.0753 Validation Accuracy: 0.5841999054\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:     1.0542 Validation Accuracy: 0.5811999440\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:     1.1363 Validation Accuracy: 0.5899999142\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:     1.0786 Validation Accuracy: 0.5857999325\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:     1.0324 Validation Accuracy: 0.5755999088\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss:     1.0823 Validation Accuracy: 0.5759998560\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:     1.0557 Validation Accuracy: 0.5771999359\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:     1.1392 Validation Accuracy: 0.5891999006\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:     1.1038 Validation Accuracy: 0.5819998980\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:     0.9773 Validation Accuracy: 0.5765999556\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:     1.1037 Validation Accuracy: 0.5921999216\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:     1.0482 Validation Accuracy: 0.5807998776\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:     1.1298 Validation Accuracy: 0.5781999230\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:     1.1052 Validation Accuracy: 0.5877999067\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:     0.9655 Validation Accuracy: 0.5849999189\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss:     1.0652 Validation Accuracy: 0.5849999189\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:     1.0264 Validation Accuracy: 0.5815999508\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:     1.1100 Validation Accuracy: 0.5891999006\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:     1.1189 Validation Accuracy: 0.5823999643\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:     0.9904 Validation Accuracy: 0.5745999217\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:     1.1424 Validation Accuracy: 0.5817999244\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss:     1.0113 Validation Accuracy: 0.5805999637\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:     1.1037 Validation Accuracy: 0.5783999562\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:     1.0990 Validation Accuracy: 0.5867999196\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:     0.9565 Validation Accuracy: 0.5769999027\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:     1.0835 Validation Accuracy: 0.5771999359\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss:     1.0342 Validation Accuracy: 0.5777998567\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:     1.0780 Validation Accuracy: 0.5791999102\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:     1.1194 Validation Accuracy: 0.5825998187\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:     0.9860 Validation Accuracy: 0.5733999014\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:     1.1279 Validation Accuracy: 0.5831999183\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:     1.0756 Validation Accuracy: 0.5857999325\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:     1.0735 Validation Accuracy: 0.5855998993\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:     1.0400 Validation Accuracy: 0.5833999515\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:     0.9757 Validation Accuracy: 0.5727999210\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:     1.0850 Validation Accuracy: 0.5859999657\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:     1.0741 Validation Accuracy: 0.5823999047\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:     1.1244 Validation Accuracy: 0.5863999724\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:     1.0820 Validation Accuracy: 0.5909998417\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:     1.0502 Validation Accuracy: 0.5701999068\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:     1.1081 Validation Accuracy: 0.5829999447\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:     1.0135 Validation Accuracy: 0.5893999338\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:     1.0898 Validation Accuracy: 0.5843999386\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:     1.0793 Validation Accuracy: 0.5851999521\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:     0.9861 Validation Accuracy: 0.5825999379\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:     1.0739 Validation Accuracy: 0.5799999237\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:     1.0197 Validation Accuracy: 0.5861999393\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:     1.0479 Validation Accuracy: 0.5881999731\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:     1.1096 Validation Accuracy: 0.5869999528\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:     0.9846 Validation Accuracy: 0.5837999582\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:     1.0953 Validation Accuracy: 0.5785999894\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:     1.0107 Validation Accuracy: 0.5837999582\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:     1.0575 Validation Accuracy: 0.5839998722\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:     1.0612 Validation Accuracy: 0.5807999372\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:     0.9869 Validation Accuracy: 0.5805999637\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:     1.0814 Validation Accuracy: 0.5779999495\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:     1.0033 Validation Accuracy: 0.5859999061\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:     1.1207 Validation Accuracy: 0.5843998790\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:     1.0792 Validation Accuracy: 0.5831999183\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:     1.0135 Validation Accuracy: 0.5731999278\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:     1.0747 Validation Accuracy: 0.5801998973\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:     1.0859 Validation Accuracy: 0.5835999250\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:     1.1197 Validation Accuracy: 0.5857999325\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:     1.0673 Validation Accuracy: 0.5899999142\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:     1.0077 Validation Accuracy: 0.5807999372\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:     1.0982 Validation Accuracy: 0.5817999244\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:     1.0026 Validation Accuracy: 0.5829998255\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:     1.1025 Validation Accuracy: 0.5863999128\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:     1.0910 Validation Accuracy: 0.5867999792\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:     0.9845 Validation Accuracy: 0.5861999393\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:     1.0943 Validation Accuracy: 0.5833998919\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:     1.0290 Validation Accuracy: 0.5837998986\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:     1.0839 Validation Accuracy: 0.5853999257\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:     1.0823 Validation Accuracy: 0.5923999548\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:     1.0021 Validation Accuracy: 0.5763998628\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:     1.0665 Validation Accuracy: 0.5935999751\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:     1.0120 Validation Accuracy: 0.5889999866\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:     1.1747 Validation Accuracy: 0.5815998912\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:     1.0947 Validation Accuracy: 0.5877999067\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:     0.9881 Validation Accuracy: 0.5895999670\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:     1.0730 Validation Accuracy: 0.5855999589\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:     1.0661 Validation Accuracy: 0.5855999589\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:     1.1486 Validation Accuracy: 0.5889998674\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:     1.0639 Validation Accuracy: 0.5877999067\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:     0.9902 Validation Accuracy: 0.5869998932\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:     1.0971 Validation Accuracy: 0.5763998628\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:     0.9871 Validation Accuracy: 0.5833999515\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:     1.1238 Validation Accuracy: 0.5855998993\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:     1.1078 Validation Accuracy: 0.5927999020\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:     1.0207 Validation Accuracy: 0.5673999190\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:     1.0573 Validation Accuracy: 0.5873999596\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:     1.0494 Validation Accuracy: 0.5873999596\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:     1.1494 Validation Accuracy: 0.5887999535\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:     1.0648 Validation Accuracy: 0.5881999135\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:     1.0099 Validation Accuracy: 0.5789999366\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:     1.0441 Validation Accuracy: 0.5847998857\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:     1.0254 Validation Accuracy: 0.5795999765\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:     1.1128 Validation Accuracy: 0.5817999244\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:     1.1114 Validation Accuracy: 0.5895999670\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:     0.9902 Validation Accuracy: 0.5807999372\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:     1.0662 Validation Accuracy: 0.5879999399\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:     1.0220 Validation Accuracy: 0.5847999454\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:     1.0931 Validation Accuracy: 0.5891999602\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:     1.0182 Validation Accuracy: 0.5913999677\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:     0.9368 Validation Accuracy: 0.5789999962\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:     1.0809 Validation Accuracy: 0.5847999454\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:     1.0133 Validation Accuracy: 0.5893999338\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.5915605095541401\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAWJQAAFiUBSVIk8AAAIABJREFUeJzs3Xl8XFd5//HPo12yLO+7YzvO6pAAISQhpGRhh7CkQNhK\nS6ClZS+BUiiUNpTSUtpC2CmlEKDQhEKhv7KVNSELaSAJCdnI6sR2HO+2LFuytuf3x3Nm7tX1SBpZ\nu/R9v17zGs095957ZjTLmWeec465OyIiIiIiAjWT3QARERERkalCnWMRERERkUSdYxERERGRRJ1j\nEREREZFEnWMRERERkUSdYxERERGRRJ1jEREREZFEnWMRERERkUSdYxERERGRRJ1jEREREZFEnWMR\nERERkUSdYxERERGRRJ1jEREREZFEnWMRERERkUSd40lmZmvN7EVm9gYz+wsze7eZvcXMLjKzJ5pZ\n62S3cTBmVmNmLzSzK8zsPjNrNzPPXb492W0UmWrMbF3hdXLpWNSdqszsvMJ9uHiy2yQiMpS6yW7A\nbGRmC4E3AK8D1g5Tvd/M7gSuAb4L/MTdu8a5icNK9+EbwPmT3RaZeGZ2OfDqYar1AnuBncDNxHP4\nP9x93/i2TkRE5MgpcjzBzOx5wJ3A3zJ8xxjif3Qy0Zn+DvCS8WvdiHyZEXSMFT2aleqAxcCJwCuB\nzwBbzOxSM9MX82mk8Nq9fLLbIyIynvQBNYHM7KXA14DaQlE78BvgUeAQsABYA2xgCn6BMbMnARfk\nNj0EvB/4FbA/t/3gRLZLpoU5wF8D55jZc9z90GQ3SEREJE+d4wliZscQ0dZ8x/h24L3A99y9t8I+\nrcC5wEXA7wJtE9DUaryocPuF7n7rpLREpop3Emk2eXXAMuB3gDcSX/hKziciya+dkNaJiIhUSZ3j\nifNBoDF3+8fAC9y9c7Ad3L2DyDP+rpm9BfgjIro82U7L/b1RHWMBdrr7xgrb7wOuM7OPA18lvuSV\nXGxmH3f3X09EA6ej9JjaZLdjNNz9Kqb5fRCR2WXK/WQ/E5lZM/CC3KYe4NVDdYyL3H2/u3/U3X88\n5g0cuaW5vx+ZtFbItJGe678H3JPbbMDrJ6dFIiIilalzPDGeADTnbl/v7tO5U5mfXq5n0loh00rq\nIH+0sPlpk9EWERGRwSitYmIsL9zeMpEnN7M24CnAKmARMWhuG/B/7v7wkRxyDJs3JsxsPZHusRpo\nADYCP3P37cPst5rIiT2KuF9b036bR9GWVcBjgPXA/LR5N/Aw8ItZPpXZTwq3jzGzWnfvG8lBzOxk\n4CRgBTHIb6O7f62K/RqBJxMzxSwF+ojXwm3ufttI2jDI8Y8DzgBWAl3AZuBGd5/Q13yFdh0PPB5Y\nQjwnDxLP9duBO929fxKbNywzOwp4EpHDPpd4PT0CXOPue8f4XOuJgMZRxBiRbcB17v7AKI55AvH4\nLyeCC71AB7AJuBe42919lE0XkbHi7rqM8wV4OeC5y/cn6LxPBL4PdBfOn7/cRkyzZUMc57wh9h/s\nclXad+OR7ltow+X5Ornt5wI/A/orHKcb+DTQWuF4JwHfG2S/fuCbwKoqH+ea1I7PAPcPc9/6iHzz\n86s89pcK+39uBP//vy/s+52h/s8jfG5dXjj2xVXu11zhMVlaoV7+eXNVbvtriA5d8Rh7hznvycB/\nAgeG+N9sAt4G1B/B43E28H+DHLeXGDtwWqq7rlB+6RDHrbpuhX3nA39DfCkb6jm5A/gCcPow/+Oq\nLlW8f1T1XEn7vhT49RDn6wF+BDxpBMe8Krf/xtz2M4kvb5XeExy4AThrBOepB95B5N0P97jtJd5z\nnjEWr09ddNFldJdJb8BsuABPLbwR7gfmj+P5DPjwEG/ylS5XAQsGOV7xw62q46V9Nx7pvoU2DPig\nTtveWuV9/CW5DjIx28bBKvbbCKyp4vF+7RHcRwf+Gagd5thzgLsK+728ijY9o/DYbAYWjeFz7PJC\nmy6ucr+mCo/Dkgr18s+bq4jBrF8f4rGs2Dkmvrj8I/GlpNr/y61U+cUoneM9VT4Pu4m863WF7ZcO\nceyq6xb2+11gzwifj78e5n9c1aWK949hnyvEzDw/HuG5LwNqqjj2Vbl9NqZtb2HoIEL+f/jSKs6x\nhFj4ZqSP37fH6jWqiy66HPlFaRUT4ybiw7k0jVsr8GUze6XHjBRj7V+BPyxs6yYiH48QEaUnEgs0\nlJwL/NzMznH3PePQpjGV5oz+WLrpRHTpfuKLweOBY3LVnwh8AniNmZ0PXEmWUnR3unQT80qfkttv\nLRG5HW6xk2LufidwB/GzdTsRLV0DPJZI+Sh5OxH5evdgB3b3A2b2MiIq2ZQ2f87MfuXu91Xax8yW\nA18hS3/pA17p7ruGuR8TYXXhthOduOFcRkxpWNrnFrIO9Hrg6OIOZlZL/K9fXCg6SLwmtxKvyWOA\nx5E9Xo8FrjezM9x921CNMrO3ETPR5PUR/69NRArAqUT6Rz3R4Sy+NsdUatNHODz96VHil6KdQAvx\nvziFgbPoTDozmwtcTbyO8/YAN6brFUSaRb7tf0q8p71qhOf7PeDjuU23E9HeQ8Rz4zSyx7IeuNzM\nbnH3ewc5ngH/Rfzf87YR89nvJL5MzUvHPxalOIpMLZPdO58tF+In7WKU4BFiQYRTGLufu19dOEc/\n0bGYX6hXR3xI7yvU/48Kx2wiIlily+Zc/RsKZaXL8rTv6nS7mFryZ4PsV9630IbLC/uXomLfBY6p\nUP+lRCc1/ziclR5zB64HHl9hv/OAXYVzPXeYx7w0xd7fp3NUjF4RX0rexcCf9vuBM6v4v76+0KZf\nAQ0V6tUQPzPn675vHJ7Pxf/HxVXu98eF/e4bpN7GXJ39ub+/AqyuUH9dhW0fLJxrG5GWUelxO4bD\nX6PfG+a+nMLh0cavFZ+/6X/yUmB7qrO7sM+lQ5xjXbV1U/1ncXiU/Goiz/qw9xiic/l84if9mwpl\ni8lek/njfYPBX7uV/g/njeS5AnyxUL8d+BMK6S5E5/KfOTxq/yfDHP+qXN0OsveJbwHHVqi/gfg1\nIX+OK4c4/gWFuvcSA08rvscTvw69ELgC+M+xfq3qoosuI79MegNmy4WITHUV3jTzl11ER+99xE/i\nc47gHK0c/lPqJcPscyaH52EOmffGIPmgw+wzog/ICvtfXuEx+ypD/IxKLLldqUP9Y6BxiP2eV+0H\nYaq/fKjjVah/VuG5MOTxc/tdWWjXxyrUeW+hzk+HeoxG8Xwu/j+G/X8SX7KKKSIVc6ipnI7zoRG0\n70wGdhJ/S4UvXYV9ajg8x/s5Q9T/WaHup4Y5/mM4vGM8Zp1jIhq8rVD/k9X+/4FlQ5Tlj3n5CJ8r\nVb/2icGx+boHgbOHOf6bC/t0MEiKWKp/VYX/wScZetzFMga+tx4a7BzE2INSvR7g6BE8Vk0jeWx1\n0UWX8bloKrcJ4rFQxu8TnaJKFgLPJQbQ/BDYY2bXmNmfpNkmqvFqstkRAH7g7sWps4rt+j/grwqb\n/7TK802mR4gI0VCj7P+NiIyXlEbp/74PsWyxu3+H6EyVnDdUQ9z90aGOV6H+L4BP5TZdmGZRGM7r\niNSRkrea2QtLN8zsd4hlvEt2AL83zGM0IcysiYj6nlgo+pcqD/FrouNfrXeTpbv0Ahe6+5AL6KTH\n6U8YOJvM2yrVNbOTGPi8uAe4ZJjj3wH8+ZCtHp3XMXAO8p8Bb6n2/+/DpJBMkOJ7z/vd/bqhdnD3\nTxJR/5I5jCx15XYiiOBDnGMb0ektaSDSOirJrwT5a3d/sNqGuPtgnw8iMoHUOZ5A7v6fxM+b11ZR\nvZ6IonwWeMDM3phy2Ybye4Xbf11l0z5OdKRKnmtmC6vcd7J8zofJ13b3bqD4wXqFu2+t4vg/zf29\nNOXxjqX/zv3dwOH5lYdx93YiPaU7t/mLZrYm/b/+gyyv3YE/qPK+joXFZraucDnWzJ5sZn8O3Am8\npLDPV939piqP/1Gvcrq3NJVeftGdr7n7XdXsmzonn8ttOt/MWipULea1fjg934bzBSItaTy8rnB7\nyA7fVGNmc4ALc5v2EClh1fjLwu2R5B1/1N2rma/9e4Xbj6tinyUjaIeITBHqHE8wd7/F3Z8CnENE\nNoechzdZREQarzCzhkoVUuTxCblND7j7jVW2qYeY5qp8OAaPikwVP6yy3v2F2z+qcr/iYLcRf8hZ\nmGtmK4sdRw4fLFWMqFbk7r8i8pZLFhCd4i8xcLDbP7r7D0ba5lH4R+DBwuVe4svJP3D4gLnrOLwz\nN5TvDF+l7DwGvrd9cwT7Avw893c9cHqFOmfl/i5N/TesFMX9xgjbMywzW0KkbZT80qffsu6nM3Bg\n2req/UUm3dc7c5tOSQP7qlHt6+Tuwu3B3hPyvzqtNbM3VXl8EZkiNEJ2krj7NcA1UP6J9snErAqn\nE1HESl9cXkqMdK70ZnsyA0du/98Im3QD8Mbc7dM4PFIylRQ/qAbTXrj924q1ht9v2NSWNDvC04lZ\nFU4nOrwVv8xUsKDKerj7ZWZ2HjGIB+K5k3cDI0tBmEidxCwjf1VltA7gYXffPYJznF24vSd9IalW\nbeH2emJQW17+i+i9PrKFKH45grrVOrNw+5pxOMd4O61w+0jew05Kf9cQ76PDPQ7tXv1qpcXFewZ7\nT7iCgSk2nzSzC4mBht/3aTAbkMhsp87xFODudxJRj88DmNl84ufFS4hppfLeaGZfqPBzdDGKUXGa\noSEUO41T/efAaleZ6x2j/eqHqmxmZxH5s6cMVW8I1eaVl7yGyMNdU9i+F3iFuxfbPxn6iMd7FzH1\n2jVEisNIOrowMOWnGsXp4n5esVb1BqQYpV9p8v+v4q8Tw6k4Bd8oFdN+qkojmWIm4z2s6tUq3b2n\nkNlW8T3B3W80s08zMNjw9HTpN7PfEKl1PycGNFfz66GITCClVUxB7r7X3S8nIh9/U6HKWypsm1+4\nXYx8Dqf4IVF1JHMyjGKQ2ZgPTjOzZxODn460YwwjfC2m6NPfVSh6h7tvHEU7jtRr3N0Klzp3X+Tu\nx7v7y9z9k0fQMYaYfWAkxjpfvrVwu/jaGO1rbSwsKtwe0yWVJ8hkvIeN12DVNxO/3hwsbK8hcpXf\nRMw+s9XMfmZmL6liTImITBB1jqcwD39NvInmPb2a3Ud4Or0xH4E0EO7fGZjSshH4APAc4ATiQ78p\n33GkwqIVIzzvImLav6JXmdlsf10PGeU/AsO9Nqbia23aDMQbwlR8XKuS3rv/jkjJeRfwCw7/NQri\nM/g8YszH1Wa2YsIaKSKDUlrF9PAJ4GW526vMrNndO3PbipGieSM8R/FnfeXFVeeNDIzaXQG8uoqZ\nC6odLHSYFGH6ErCqQvH5xMj9Sr84zBb56HQv0DzGaSbF18ZoX2tjoRiRL0Zhp4MZ9x6WpoD7MPBh\nM2sFzgCeQrxOz2bgZ/BTgB+klRmrnhpSRMbebI8wTReVRp0XfzIs5mUeO8JzHD/M8aSyC3J/7wP+\nqMopvUYzNdwlhfPeyMBZT/7KzJ4yiuNPd/n5eusYZZS+KHVc8j/5HzNY3UGM9LVZjeIczhvG4Rzj\nbUa/h7l7h7v/1N3f7+7nEUtg/yUxSLXkscBrJ6N9IpJR53h6qJQXV8zHu52B898WR68Ppzh1W7Xz\nz1ZrJvzMW0n+A/xadz9Q5X5HNFWemT0R+FBu0x5idow/IHuMa4GvpdSL2eiGwu2njcM5bs79fVwa\nRFutSlPDjdYNDHyNTccvR8X3nNG8h/UTA1anLHff6e4f5PApDZ8/Ge0RkYw6x9PDCYXbHcUFMFI0\nK//hcoyZFadGqsjM6ogOVvlwjHwapeEUfyasdoqzqS7/029VA4hSWsQrRnqitFLilQzMqX2tuz/s\n7v9LzDVcspqYOmo2+nHh9sXjcI5f5P6uAV5czU4pH/yiYSuOkLvvAO7IbTrDzEYzQLQo//odr9fu\nLxmYl/u7g83rXpTua36e59vdff9YNm4cXcnAlVPXTVI7RCRR53gCmNkyM1s2ikMUf2a7apB6Xyvc\nLi4LPZg3M3DZ2e+7+64q961WcST5WK84N1nyeZLFn3UH8/sc2c/enyMG+JR8wt2/nbv9XgZGTZ9v\nZtNhKfAx5e73AT/JbTrTzIqrR47WVwu3/9zMqhkI+Foq54qPhc8Vbn9kDGdAyL9+x+W1m351ya8c\nuZDKc7pX8oHC7X8fk0ZNgJQPn5/Vopq0LBEZR+ocT4wNxBLQHzKzpcPWzjGzFwNvKGwuzl5R8iUG\nfoi9wMzeOEjd0vFP5/APlo+PpI1VegDIL/rw1HE4x2T4Te7v08zs3KEqm9kZxADLETGzP2bgoMxb\ngHfm66QP2VcwsMP+YTPLL1gxW1xauP2vZvaMkRzAzFaY2XMrlbn7HQxcGOR44KPDHO8kYnDWePk3\nBuZbPx24rNoO8jBf4PNzCJ+eBpeNh+J7zwfSe9SgzOwNZAviABwgHotJYWZvSCsWVlv/OQycfrDa\nhYpEZJyoczxxWogpfTab2bfM7MVDvYGa2QYz+xzwdQau2HUzh0eIAUg/I769sPkTZvaPZjZg5LeZ\n1ZnZa4jllPMfdF9PP9GPqZT2kV/O+lwz+7yZPc3MjissrzydosrFpYC/aWYvKFYys2Yzu4SIaLYR\nKx1WxcxOBi7LbeoAXlZpRHua4zifw9gAXDmCpXRnBHe/loHzQDcTMwF82syOG2w/M5tvZi81syuJ\nKfn+YIjTvIWBX/jeZGZfLT5/zazGzC4ifvFZwDjNQezuB4n25scovBX4SVqk5jBm1mhmzzOzbzD0\nipj5hVRage+a2e+m96ni0uijuQ8/B76S2zQH+JGZ/WExMm9mbWb2YeCThcO88wjn0x4r7wIeTs+F\nCwd77aX34D8gln/PmzZRb5GZSlO5Tbx6YvW7CwHM7D7gYaKz1E98eJ4EHFVh383ARUMtgOHuXzCz\nc4BXp001wJ8BbzGzXwBbiWmeTgcWF3a/i8Oj1GPpEwxc2vcP06XoamLuz+ngC8TsEaUO1yLgv83s\nIeKLTBfxM/SZxBckiNHpbyDmNh2SmbUQvxQ05za/3t0HXT3M3b9hZp8FXp82HQt8BnhVlfdppngf\nsYJg6X7XEI/7G9L/505iQGM98Zo4jhHke7r7b8zsXcBHcptfCbzMzG4ANhEdydOImQkgcmovYZzy\nwd39h2b2Z8A/k837ez5wvZltBW4jVixsJvLSH0s2R3elWXFKPg+8A2hKt89Jl0pGm8rxZmKhjNLq\noPPS+f/BzG4kvlwsB87KtafkCnf/zCjPPxaaiOfCKwE3s3uAB8mml1sBnMrh09V9293/Z8JaKSIV\nqXM8MXYTnd9iZxSi41LNlEU/Bl5X5epnr0nnfBvZB1UjQ3c4rwVeOJ4RF3e/0szOJDoHM4K7H0qR\n4p+SdYAA1qZLUQcxIOvuKk/xCeLLUskX3b2Y71rJJcQXkdKgrN8zs5+4+6wZpJe+RP6+md0K/C0D\nF2oZ7P9TNORcue7+0fQF5gNkr7VaBn4JLOklvgyOdjnrIaU2bSE6lPmo5QoGPkdHcsyNZnYx0alv\nHqb6qLh7e0pP+i+iY1+yiFhYZzCfIiLlU40Rg6qLA6uLriQLaojIJFJaxQRw99uISMdTiSjTr4C+\nKnbtIj4gnu/uz6h2WeC0OtPbiamNfkjllZlK7iDekM+ZiJ8iU7vOJD7IfklEsab1ABR3vxt4AvFz\n6GCPdQfwZeCx7v6Dao5rZq9g4GDMu6m8dHilNnUROcr5gT6fMLMTq9l/JnH3fyIGMl7G4fMBV/Jb\n4kvJWe4+7C8paTqucxiYNpTXT7wOz3b3L1fV6FFy968T8zv/EwPzkCvZRgzmG7Jj5u5XEuMn3k+k\niGxl4By9Y8bd9xJT8L2SiHYPpo9IVTrb3d88imXlx9ILicfoBoZ/b+sn2n+Bu79ci3+ITA3mPlOn\nn53aUrTp+HRZShbhaSeivncAd47Fyl4p3/gcYpT8QqKjtg34v2o73FKdNLfwOcTP803E47wFuCbl\nhMokSwPjHkv8kjOf+BK6F7gfuMPdtw+x+3DHPo74UroiHXcLcKO7bxptu0fRJiPSFB4DLCFSPTpS\n2+4A7vIp/kFgZmuIx3UZ8V65G3iEeF1N+kp4gzGzJuBk4tfB5cRj30MMnL4PuHmS86NFpAJ1jkVE\nREREEqVViIiIiIgk6hyLiIiIiCTqHIuIiIiIJOoci4iIiIgk6hyLiIiIiCTqHIuIiIiIJOoci4iI\niIgk6hyLiIiIiCTqHIuIiIiIJOoci4iIiIgk6hyLiIiIiCTqHIuIiIiIJOoci4iIiIgk6hyLiIiI\niCTqHIuIiIiIJOoci4iIiIgk6hyLiIiIiCTqHIuIiIiIJOoci4iIiIgk6hyLiIiIiCTqHIuIiIiI\nJOoci4iIiIgk6hyLiIiIiCTqHIuIiIiIJHWT3QCpzMwuBtYB33b3X09ua0RERERmB3WOp66LgXOB\njYA6xyIiIiITQGkVIiIiIiKJOsciIiIiIok6x0fAzDaY2WfN7B4zO2Bme83sN2b2cTM7LVevwcwu\nMLN/NbNbzWynmXWZ2UNm9tV83dw+F5uZEykVAF80M89dNk7Q3RQRERGZdczdJ7sN04qZvQX4KFCb\nNh0gvmQ0p9tXu/t5qe7zgP/J7X4w1W1Kt3uB17r7V3LHfxnwMWAhUA+0A525Y2xy99PH8C6JiIiI\nSKLI8QiY2UXAx4mO8TeAk9y9FZgDrAReBdyU26UD+CLwNGCxu89x92ZgLXAZMSDyc2a2prSDu1/p\n7suB69OmP3X35bmLOsYiIiIi40SR4yqZWT3wALAa+A93f+UYHPPfgNcCl7r7+wtlVxGpFa9x98tH\ney4RERERGZ4ix9V7GtEx7gPeOUbHLKVcnD1GxxMRERGRUdA8x9V7Urq+1d23VLuTmS0E3gQ8BzgB\nmEeWr1yyckxaKCIiIiKjos5x9Zal64er3cHMTgJ+mtsXYD8xwM6BBmABkbMsIiIiIpNMaRXVsyPY\n54tEx/hm4NnAXHdvc/dladDdRaM4toiIiIiMMUWOq/doul5bTeU0A8UZRI7yCwZJxVhWYZuIiIiI\nTBJFjqt3Q7p+rJmtqqL+6nS9Y4gc5acPsX9/ulZUWURERGSCqHNcvZ8AW4jBdP9YRf196XqZmS0t\nFprZKcBQ08G1p+v5I2mkiIiIiBw5dY6r5O49wDvSzVeY2dfN7MRSuZmtMLPXmdnH06a7gM1E5PdK\nMzs21as3sxcBPyIWCRnMHen6RWY2byzvi4iIiIhUpkVARsjM3k5EjktfLDqIaHKl5aN/l1hJr1R3\nP9BIzFLxMPBe4CvAQ+6+rnCeE4FbU91eYDvQA2x2998Zh7smIiIiMuspcjxC7v4R4FRiJoqNQD3Q\nBdwGfAy4JFf3W8BTiSjx/lT3IeCf0jE2D3Geu4FnAD8gUjSWE4MBVw+2j4iIiIiMjiLHIiIiIiKJ\nIsciIiIiIok6xyIiIiIiiTrHIiIiIiKJOsciIiIiIok6xyIiIiIiiTrHIiIiIiKJOsciIiIiIok6\nxyIiIiIiiTrHIiIiIiJJ3WQ3QERkJjKzB4E2Ypl5EREZuXVAu7sfPZEnnbGd4/Of+jQHWLNmVXnb\ngc4+AFrnLQSgt2t/uWzdirhetjAF0+uay2Vb9x8CYOGcbgDsQGe5rOaeawHotFYAulc9plw2v8Hi\nfM3xMN+/NTvf7rt+A0Bf34Hytl8+Eudu71kAwOL5reWyHXv3xR/LF8d5FzWWy3o6DwLQ39sPwJye\nvnLZUYd2AHBsQ5xnQVO5iIOp3sev7TREZKy1NTc3L9ywYcPCyW6IiMh0dNddd9HZ2Tl8xTE2YzvH\nu3bvAWBO24LytlUrogfc3x0dxYb6rnLZwrb5AJjXArB5V3u57OcPbgbgGaesA2BxY9aXPORx3Xdg\nLwC//O3D5bLu/noAVi5oAeD41LkGWLc0zn3z9uxYB/t7AGipi7JlbXOyssboDHcvjW1dHVlH2+vi\n3+gex++0hnLZQ3Ur45hEJ7m1f0+5rBYRGUcbN2zYsPCmm26a7HaIiExLp512GjfffPPGiT6vco5F\nZNYzs6vMzCe7HSIiMvlmbORYRGSy3b5lH+ve/d3JboaMgY0fumCymyAiE2TGdo4XLYx0ipb6/vK2\nBS2RY/vIvu0AdHtvuWzuvDUAlIJHV12X/RT6wI5IsVg4dy4Axy3O8n27+pcAMP/QFgBOaN5RLtve\nHYkLHZtKDThULrO6OPecuiy5obsmjttUG6kWLQvbsju0L+UV10Swv683C3K11Md+fQ2RxlGTC4D1\nHIr7v783ko37PCtrrFWqsYiIiEie0ipEZFoxszPM7Eoz22Jmh8xsq5n90MxemqtzsZl908weMLNO\nM2s3s+vM7FWFY61L6RTnptueu1w1sfdMRESmghkbOV6+OKKuC+dld3HvzojubtnyAADdKdIKQO1Z\nAHR2x6jIo1efWC46elUMqFuYZojYdWB7uayrMSLHNX3bAKjfkw1427AsBgDOXRuD/Zpqesplnfti\nQF2rZQP/Tl8cUd1tvTFTxr792aC7eo8o7yGL6/qWLHrdkKLJTTURhV65dG52ns6IUHc8EpHnzt5s\nv5babICgyHRgZq8DPgP0Af8PuBdYCjwReCPw9VT1M8CdwM+BrcAi4LnAV8zsBHd/X6q3F3g/cDGw\nNv1dsnEc74qIiExRM7ZzLCIzi5mdBHwaaAee4u53FMpX526e7O73F8obgO8D7zazz7r7FnffC1xq\nZucBa9390iNo12DTUZw4yHYREZnCZmznuCvN/Vs7N5vKrWNTfJYey6MA1C/IPrt27ImIbGN9RInP\nOvGkcllTW8yV3FgTcw3vaV+SHXN+/F3XFPnMB/uzstWPPQaApfNThLY/y/c91BlR4Xnb95a3td3/\nWwA27Yyp5jbv2FQuO1Af0eeDxJSpK+ZmcyCvb4s2n7Ai6qxY3FIu6005xj+7Od3Pe7Ood0NflgMt\nMg28gXjP+kCxYwzg7ptzf99fobzbzD4FPBV4GvDlcWyriIhMUzO2cywiM86T0vX3h6toZmuAdxGd\n4DVAc6F3KqL1AAAgAElEQVTKqsN2OkLuftogbbgJeMJYnUdERCaGOsciMl3MT9dbhqpkZuuBG4EF\nwDXAD4F9RJ7yOuDVQONg+4uIyOw2YzvHpaFvja3Ly9vmLFkLwNKmSH1YePTJ5bLOtF5cbW0M0rND\nWfrBgY5IYeipi9SLtuZl5bJDRApEzfIzADjpxCxVY/XSSJ1oJFIu+nNTx1lfDBhcsGpFedu8lUdF\nOx98EIDWe+/J2re9K7Uvbp+xenG57DHHxH2c1xKF3V3ZIL+5zREwe+bpGwD4RfvWctkju3IDEkWm\nvlIO0irg7iHqvZ0YgPcad788X2BmryA6xyIiIhXN2M6xiMw4NxCzUjyHoTvHx6brb1YoO3eQffoA\nzKzW3fuOuIUFJ6+ax01aPEJEZFqZsZ3jtrZ58UdD9utp8+rHAtDTnAazLcwG69X3xudhnzfEbs1Z\nWUPdTgCa5hwPwP69WWTW0yIbB/fGYL3eRXeWy5qWLwWguyct3NGTTeXm/emh7z9Y3jZnYbR1/aJT\nAFh+QhaFXrgxxhrt3hdTza1bM79cNn9+tLV97664n43ZgLyOrjj+umURaa4950nlshvueAiRaeQz\nwOuB95nZ/7r7nflCM1udBuVtTJvOA/4nV/4s4I8GOfaudL0GeHAM2ywiItPMjO0ci8jM4u53mtkb\ngc8Ct5jZfxPzHC8iIsr7gfOJ6d5eA/ynmX2TyFE+GXg2MQ/yyyoc/ifARcB/mdn3gE7gIXf/yvje\nKxERmWrUORaRacPd/9XMbgf+jIgMXwjsBG4DPp/q3GZm5wN/Syz8UQfcCryIyFuu1Dn+PLEIyMuB\nP0/7XA2ocywiMsvM2M6xEYPN+vuz9MGWphicVmOROrF/97ZyWU9vDHirb430g56e2uxYfTEQb9u9\nVwPQ2JMNZDv6qBMA2H0g9n/orixV4ZjjVwLQ1BYPc39v9nB3H4wUi96+7Fj9nTuiXQfjF96e/myl\nu6VL1gMwpyXqtM3N9tvfEQP/WubFvMpzG7L5lHe1b0v3J9Ix1h+1MLtf3dkAQZHpwt1/Abx4mDrX\nE/MZV2IV6vcB70kXERGZxWomuwEiIiIiIlPFjI0c16aV7jq7O8vbmhriu0Dv/hhvU790XbmsOUVb\n+2tiP+vLBsptfjRW1Lvz5lhm7nm/88xyWWtLDO7rWhCD7+7/bbaq3Tf++1oA5i6YA8AJx2brDsxt\njbYc2J+1b1d7rFi3b1sHAN3dDdl50kp/cxfE+dr3ZSvr1bfE1HQNTRFp7uhpKpf1Wvx9oDfO05em\nngNoXbQUEREREckociwiIiIikszYyPGSpbEwRn9fFpmtrYsoan/XIwAc2p6VNS6JXOPmORFBbmpd\nWy7btDXydufWxsO1dF6WC7x8Qfx96EBjOl9/uezGG+8F4NZ7Ig95xfJ55bIVSyL3t742y22eNyeO\ntW7dSQAcve6ErO2kqPL+yEeeM7e1XNbcGMc61Jtyqslyjuvrot7B3shR7u7J8owP9StyLCIiIpKn\nyLGIiIiISKLOsYiIiIhIMmPTKo5ZezQANTW5WZs8UhP21MZKeR3bf5uVHdwDQF9NSr1oW1Mu2rc3\npkprqovUiYa67DtFXV2kMNTWxLXVZA9pQ138vWJZDJjr78umldu4eTcASxdkqRbrVi4CYM3aWIlv\nxeIl5bJDvdsBaK4/Ls6T7gtAbW2kTPSlGarqa7OV+BpS+7r64n71dGdpH4d69iAiIiIiGUWORURE\nRESSGRs5Xr40Bpv1exYptdr4LrB4SUSFdy1dWS677+5fA3Dvrb8EoKf2nnLZ/p0xldvRKZLb1ZVN\nh7Z3XwzW6+6JRUB6c+errY+I7uqVMTjQsnFydPfGwLgasmhyU3NEppfMj0F0vYey89jcNgDmNcdB\nDnY2ZwdL0fFa4tyl6dsArCYGHdZbnO+gZwP5+jofRkREREQyihyLiIiIiCQzNnLc2hKR1dqG7C52\ndUWebk3KzW1bt6Fc9otf3QLAT66JhT7y68ueeFxM67Z9dyzdfOeD95bLTmiKaO3WbZET3N61v1xW\nl87d2BAR4ab6LKLb0xN5wV2dWXS4pjbO2pGWfK6tbSmXNS2KNvQRC4TUZqtH09ebWuu96SoLUfcf\nirxi643vQT3dWT5yd1+2yIiIiIiIKHIsIiIiIlKmzrGIiIiISDJj0yrq0nRrdXXZCnQt8+cD0NER\nqQxzWrJBbc979oUA3HrLnQBs3byxXDa3JQax7TkYKRMPbt9WLvOUmbB5a6RV7OvIVt1zjzbUpiSN\nfnKDA9Mqdg312b+gNw3Se2R7rOC3at0x5bKmpjlRpzsN5PNHs/364350H4qUidr6bHo4r4/73NMT\nA/+6e3aXy/osS9sQEREREUWORWSaMbONZrZxstshIiIz04yNHDc1xSC4nhSNjW0xIK6xMa73d2SD\n544/LhbeePUf/AEAX/3yv5XLaiy+Q6xcnqZky0WjH3g4orydXd0A1DVkI+X6U7S2oyMG0dXnyloa\no31duYVB2jsOAlBb0w7A3MVZdHixrYv6ByMy3d2ZLQJSGshXVx/TvRnZfe7pidD2jj1bo6yvu1zW\n15NFuUVERERkBneORUQm2+1b9rHu3d+d7GZMeRs/dMFkN0FEpExpFSIiIiIiyYyNHDc0ROpEXV2W\nYtDfF39bbdzt0gp2ALv37gPgKWf/DgA7tm4ul/3m1psAmDsnBuY1NWTzA7fvj/1a0+p789sWlMs2\nPhKpDHgMvqvNLZHX1ZXSI3qy9rW0zI16aV7kTQ9lK9i1tsXqfAe6Y/Bdb2eWElLfHAP96hqirK4r\nS5fYtzPmOT7YGQPx5s1fUS5zNM+xTE1mZsCbgDcAxwC7gG8B7x2kfiNwCfBK4FigF7gV+IS7f32Q\n478V+BNgfeH4twK4+7qxvE8iIjI9zNjOsYhMa5cRndetwOeAHuCFwJlAA1BOnjezBuB/gXOBu4FP\nAS3AS4Arzezx7v6ewvE/RXS8H0nH7wZeAJwB1KfzVcXMbhqk6MRqjyEiIlPHjO0c19TEoLmmxiw6\n2tkVA95KU6aVBuYB1NbEZ2Fjijg/81nPK5c9/NBDAPR1R0S2rjWbAq6vLwbPzZsTU60dc8LR5bL6\nuVFv26aIQtdVmLatrTbbtmJFDPjzNM3bnXc9UC5rnX87APMXxPE7Du4tl7Wk+ls2R6S6pj/7XO+v\njSh0S10aMOjZinxel91/kanCzJ5MdIzvB85w991p+3uBnwErgIdyu7yD6Bh/H3iBeywVaWbvB24E\n/sLMvuPu16ftTyE6xvcAZ7r73rT9PcCPgZWF44uIyCyinGMRmWpek64/WOoYA7h7F/AXFeq/FnDg\n7aWOcaq/HfhAuvlHufqvzh1/b65+9yDHH5K7n1bpQkSxRURkmpmxkeODnRElrqubW95Wl3KMD3XF\nNGjNuUhufV383ZnydZcvX1YuO/f8pwPwsx/GqPOND20ql3V3dwFwwjEbAOhpyKZYm7cyplhbu/QU\nANyzKeAe3b4LgPmL5pe3zW2LKG9Hexxzz57sWHfefh8ATzwtotG1ZPnSHQe6032OiHFTQ7bYSGNd\nWmykeXHs15/lI1uNvhvJlPSEdH11hbJrIJur0MzmEjnGW9y9Umf0p+n61Ny20t/XVqh/Q/74IiIy\n+6h3JCJTTWmJx23FAnfvIwbPFetuHeRYpe3zc9tGcnwREZll1DkWkalmX7peViwws1pgUYW6ywc5\n1opCPYD2ERxfRERmmRmbVlGbBuR15aY16+vvS9siXaG+Plstrsaifm9asO5gV1Z26hOeCEBLU6Qy\n3Hnbr8tlnV0dqX6kQjS2Z/t1d8Rn8KG5CwFYsXhhuWzvnvisrqvNUif27I1p15ob4jO7pbWlXLZl\n204AlmzaAsCJx68vlx3ySMdonhOpE401WT+g0Q+mOxjpFfMac4P1Ou9BZAq6mUitOBd4oFD2FHLv\nW+6+38zuB9ab2XHufm+h/vm5Y5bcQqRW/E6F4z+JMXxfPHnVPG7SAhciItOKIsciMtVcnq7fa2bl\nb5Rm1gT8fYX6XwAM+McU+S3VXwy8L1en5Mu548/L1W8A/m7UrRcRkWltxkaOS1HiurrG8rZDPRGl\n7dgf0d6mxmxKtrrahlQnIqvdfVmEtbU56h13Ygys6ymFl4HtmzYCsHVrLNjx8INZGmNLGgz34OY7\nAJj32MeUy+o92rLt4Z3lbQsWxi/DO/fuTm2yctm8ubG4yIMPxeIf81qztMijlkWkuCEtTtLYvSNr\nQ326P71xrIPbyoP/6ezWVG4y9bj7dWb2CeAtwO1m9g2yeY73cHh+8T8Bz0nlt5rZ94h5ji8ClgIf\ndvdrc8e/2sw+B/wxcIeZfTMd//lE+sUjQD8iIjIrKXIsIlPRnxKd433EKnavIBb6eDq5BUCgPAXb\nM8hWz3sLMV3bvcAr3f1dFY7/BuDtQAfwemJlvR+n47SR5SWLiMgsM2Mjx6Vev+e2NTfFQh2dDZEf\nnAvMMmdOWm66Ox6SA2nBEID+/jhKY9p/3bEbymWWpmc71B+BpkPdWQpjXW2aEWp/RHu3PJjl+LY2\nRZT3YEe2KEdjS0R5N6Vlp7u7sxmlFi5oA2DHjoj83nNPtrz1iuZo3/y26DNs3NlVLuskIufde+MR\n6difTQG3eNVxiExF7u7AJ9OlaF2F+l1ESkRVaRHu3g98NF3KzOw4oBW4a2QtFhGRmUKRYxGZdcxs\nuZnVFLa1EMtWA3xr4lslIiJTwYyNHIuIDOFtwCvM7Coih3k58DRgNbEM9X9OXtNERGQyzdjOcUNj\npEk0NGQD8jytCLd0yRIA+nqztIXauiib1xwr6tU3ZOkH3d0xeC7NDseiRYuz89RHesTCpUsBOHpd\nNsXaow/Fgl0rl8S0qRvvzRbw8u5IgZiT0isAatOxLA0OrKnLTzUX7WueE4MD73vw/qwNRP2THhcD\n/h7qzwJifekQq1pjUP6i1uxf3rpoJSKz1I+AxwHPBBYSq+LdA3wcuCyldYiIyCw0YzvHIiKDcfef\nAD+Z7HaIiMjUM2M7x2Yx2q5xQGQ2osHWEtHkfe3ZgPTu7hgMN2dODLqrq59TLtuzp7d0UADq67KH\nbeGimGJt7rwYMNe7Joscz58fZXQ8AsD+XXvLZdu2PArAKWc+Ntu2JyLUtWn6uaaGLHLc3xt/t7RE\nu1auPrpctml3DNLbe0tMC3fMsceUy9asjAXCmiweh97ubFGU7o4tiIiIiEhGA/JERERERBJ1jkVE\nREREkhmbVtHfF6vY1Vg2mXF9XYyoq6uPwXr9uVmQ96e5iGvSqLu6mvIqtDS3xCC4zoNdqU72naK5\nKaVApGv3rKz+hJMB6DqwBoATe9vKZUftjRXuVq0/qryt675YZa+pPdIr9j28vVzWluZhbkwDDB85\nkK3E17435mQ+eCBSJhpqs1SSNcvWRZ00qHBfR5ZW8ejWbHU+EREREVHkWERERESkbMZGjnv6YoCd\nW25GphRELg3WW7hwfrmopibquafBd7kIcFNjWs3uQIrQHsyir0vSNG21KZp84EBW1tYWkeLG5hYA\n1pyURaMPtHcAsOvhTeVtrbURoZ7XHIPuDs7NBgW2NMS/qiENJmyaMy+7Wx3Rrp6eiGxv2pQNtLtn\nUUwft+7o4wF4aPPWctn2TY8iIiIiIhlFjkVEREREkhkbOe7YH9HU+W195W2trWkqtxQ57s8FlUsL\ne7S3R+5xbW32vaGlPiK6NYtjv50795TLtm3bAcCclpYBxwaoT1PHzWmOfOH6Fcuy9qWp39rqsvzg\n/d/9EQCP3HMfANvnZg2sT/nRdaldR69dVS5raIhzbtsaEeOuFEnOt+/4408FYG1NFo1uzk1XJyIi\nIiKKHIuIiIiIlKlzLCJThpmtMzM3s8urrH9xqn/xGLbhvHTMS8fqmCIiMn3M2LSKHTtiqrMlyxaX\nt/X2xWC7htoY1NbXm6VcNDZEekNDuu5LdQFamiJlojFN11ZTkz1su3bF6nRdh7rTcerLZTUpBaKx\nNJiuIduvuSVSLUrpGADzX3UhAOt/ENseuf0X5bJaj1SJWuuPDbmckG2PxsC6/fsPpLL+ctnuPZEC\n8vCmBwFYtn5tuWzNqaciIiIiIpkZ2zkWkVnhW8ANwNbhKk6G27fsY927vzvZzRi1jR+6YLKbICIy\nYWZs5/imG38DQFt9NuCttFBH89KIzNblBt31p2hrU1PU79jfUS4rRZybGyLaO3/B3HJZXVpYpKsr\nFtlobGzKztdYGgAYt/v6sohuY2pXQ1vWvoMpav3ki14UbVrUWi6759aIIu/rjOnaOg5kkW2zaMO+\n9n1xnlxEfFuKHPfXxX2+8MKLymULV65EZDpz933Avsluh4iIzBzKORaRKcnMTjSzb5vZbjM7YGbX\nmtkzC3Uq5hyb2cZ0aTOzj6S/e/J5xGa2zMz+zcy2mVmnmf3azF49MfdORESmqhkbOfYHYnGNjfs2\nl7ftSUtEP+X8cwBYkBbwAKitjeirpWWjG5uyCHBPyicuLRVdV5s9bAsWxJRsB1NEt7c3iw7X1UW9\n0nLT7j3lst60vHVjLg+5qT7OXdMSEeOzz3tauWzJ4pgG7rrrrwXg4W23l8v6e6N9XYciet1x4EC5\nbNnqowF40llxnxcvzqaTq6nNFiURmWKOBn4B3A78C7ACeBnwfTN7pbtfWcUxGoCfAguBHwLtwIMA\nZrYIuB5YD1ybLiuAz6a6IiIyS83YzrGITGvnAP/k7u8sbTCzTxId5s+a2ffdvX2YY6wA7gTOdfcD\nhbK/JzrGl7n7JRXOUTUzu2mQohNHchwREZkalFYhIlPRPuBv8hvc/VfAV4H5wO9WeZx3FDvGZlYP\n/B6wH7h0kHOIiMgsNWMjx3W9kTKwY/OO8rYt//P/ADiU0g7OefYzymUrj4oV5+rq4yFpbcsGw3Ue\n7Iw/PKZPq63JvlN4Gmw3J03N1tl5qFzWnwbg1aX0hfr67OHu7YkBdX35NIxU7mk1vLb588tlj3vi\nmXGMlhgMeCClcQDcf++9ANSk7zp9uancHnPS42L/Ux6f2petnldadU9kCrrZ3fdX2H4V8GrgVOBL\nwxyjC7itwvYTgRbgmjSgb7BzVMXdT6u0PUWUn1DtcUREZGpQ5FhEpqJtg2x/NF3Pq+IY29290jfA\n0r7DnUNERGahGRs5buyPyOr6OY3lbXcfbAbgtvtjmrbWX91SLutKEd8TTjoWgOaW5uxYaXq2/t4Y\nUJf/uK2rS4t+pAhybW6QW1c5iuzpONm0bXVp4F93dzZIr7RvaXq4/MC6hqbYdtJjToqy/VlQ7de3\n3AzAvo4Igh3smVMuW792PQAHOjrSOaxctnDRQkSmqGWDbF+erquZvm2wn0ZK+w53DhERmYVmbOdY\nRKa1J5jZ3AqpFeel61s4cncDB4HHm9m8CqkV5x2+y5E5edU8btICGiIi04rSKkRkKpoH/FV+g5k9\nkRhIt49YGe+IeMyp+FVgLoUBeblziIjILDVjI8d9tZHCcO2+bGDdqtZIKVizdh0ARx23vlx24//d\nGPv1x/zDj3ncSeWy0kp6pd9oe7q7sxNZbLXSID3LFaVNfR4D5Gr7+w7bry43SK80gK80IK8xt7rf\ngYMxkK5tbqx0t/7orO3UxDEOHYxUkg0nnFwuOvaY42K/trZ0zGy3pobcDZGp5efAH5nZmcB1ZPMc\n1wB/UsU0bsN5D/A04G2pQ1ya5/hlwPeAF4zy+CIiMk3N2M6xiExrDwKvBz6UrhuBm4G/cff/He3B\n3X2nmZ0N/B3wfOCJwG+BNwAbGZvO8bq77rqL006rOJmFiIgM46677gJYN9HntcqDuUVEZDTM7BBQ\nC9w62W0RGURpoZq7J7UVIoN7HNDn7o3D1hxDihyLiIyP22HweZBFJltpdUc9R2WqGmIF0nGlAXki\nIiIiIok6xyIiIiIiiTrHIiIiIiKJOsciIiIiIok6xyIiIiIiiaZyExERERFJFDkWEREREUnUORYR\nERERSdQ5FhERERFJ1DkWEREREUnUORYRERERSdQ5FhERERFJ1DkWEREREUnUORYRERERSdQ5FhGp\ngpmtNrMvmNkjZnbIzDaa2WVmtmCEx1mY9tuYjvNIOu7q8Wq7zA5j8Rw1s6vMzIe4NI3nfZCZy8xe\nYmafMLNrzKw9PZ/+/QiPNSbvx4OpG4uDiIjMZGZ2DHA9sBT4b+Bu4AzgT4Fnm9nZ7r6riuMsSsc5\nHvgpcAVwIvAa4AIzO8vdHxifeyEz2Vg9R3PeP8j23lE1VGazvwQeB3QAm4n3vhEbh+f6YdQ5FhEZ\n3qeJN+K3uvsnShvN7CPAJcAHgddXcZy/IzrGH3X3t+eO81bgY+k8zx7DdsvsMVbPUQDc/dKxbqDM\nepcQneL7gHOBnx3hccb0uV6Jufto9hcRmdHMbD1wP7AROMbd+3Nlc4GtgAFL3f3AEMeZA+wA+oEV\n7r4/V1aTzrEunUPRY6naWD1HU/2rgHPd3catwTLrmdl5ROf4q+7+qhHsN2bP9aEo51hEZGhPTdc/\nzL8RA6QO7nVAC/CkYY5zFtAMXJfvGKfj9AM/TDfPH3WLZbYZq+domZm9zMzebWZvN7PnmFnj2DVX\n5IiN+XO9EnWORUSGdkK6vmeQ8nvT9fETdByRovF4bl0B/D3wz8D3gIfN7CVH1jyRMTMh76PqHIuI\nDG1eut43SHlp+/wJOo5I0Vg+t/4beD6wmvil40SikzwfuNLMnjOKdoqM1oS8j2pAnojI6JRyM0c7\ngGOsjiNSVPVzy90/Wtj0W+A9ZvYI8AliUOn3x7Z5ImNmTN5HFTkWERlaKRIxb5DytkK98T6OSNFE\nPLc+T0zj9vg08ElkMkzI+6g6xyIiQ/ttuh4sh+24dD1YDtxYH0ekaNyfW+7eBZQGks450uOIjNKE\nvI+qcywiMrTSXJzPTFOulaUI2tlAJ3DDMMe5IdU7uxh5S8d9ZuF8ItUaq+fooMzsBGAB0UHeeaTH\nERmlcX+ugzrHIiJDcvf7iWnW1gFvKhS/n4iifTk/p6aZnWhmA1Z/cvcO4Cup/qWF47w5Hf9/Ncex\njNRYPUfNbL2ZrSoe38wWA19MN69wd62SJ+PKzOrTc/SY/PYjea4f0fm1CIiIyNAqLFd6F3AmMSfx\nPcCT88uVmpkDFBdSqLB89I3ABuCFwPZ0nPvH+/7IzDMWz1Ezu5jILb6aWGhhN7AGeC6R4/kr4Bnu\nvnf875HMNGZ2IXBhurkceBbwAHBN2rbT3f8s1V0HPAg85O7rCscZ0XP9iNqqzrGIyPDM7Cjgb4jl\nnRcRKzF9G3i/u+8u1K3YOU5lC4G/Jj4kVgC7iNH/f+Xum8fzPsjMNtrnqJmdArwDOA1YSQxu2g/c\nAXwd+Bd37x7/eyIzkZldSrz3DabcER6qc5zKq36uH1Fb1TkWEREREQnKORYRERERSdQ5FhERERFJ\n1DkegpnNNbOPmNn9ZtZtZm5mGye7XSIiIiIyPrR89ND+C3h6+rudGLm7Y/KaIyIiIiLjSQPyBmFm\njwFuB3qAc9x9VBNKi4iIiMjUp7SKwT0mXd+mjrGIiIjI7KDO8eCa03XHpLZCRERERCaMOscFZnZp\nmhz98rTp3DQQr3Q5r1THzC43sxoze7OZ3Whme9P2xxeOeaqZ/buZbTKzQ2a208z+18xePExbas3s\nbWZ2m5l1mtkOM/uOmZ2dykttWjcOD4WIiIjIrKMBeYfrALYRkeM2Iuc4v9pKfnUgIwbtvRDoI1YS\nGsDM/hj4DNkXkb3AfOCZwDPN7N+Bi929r7BfPbEs4nPSpl7i/3UB8Cwze/mR30URERERqUSR4wJ3\n/yd3Xw78adp0vbsvz12uz1V/EbF04RuBNndfACwj1grHzJ5M1jH+BnBUqjMfeC/gwKuAv6jQlL8k\nOsZ9wNtyx18H/AD4/NjdaxEREREBdY5HqxV4q7t/xt0PArj7dndvT+UfIB7j64CXu/vmVKfD3f8O\n+FCq9y4zaysd1MxaifXtAf7K3T/m7p1p34eITvlD43zfRERERGYddY5HZxfwhUoFZrYQOD/d/Pti\n2kTyD0AX0cl+bm77s4A5qezjxZ3cvQf4yJE3W0REREQqUed4dH7l7r2DlJ1K5CQ7cHWlCu6+D7gp\n3XxCYV+AX7v7YLNlXDPCtoqIiIjIMNQ5Hp2hVstbkq73DdHBBdhcqA+wOF1vHWK/R4Zpm4iIiIiM\nkDrHo1MpVaKo8QiOa1XU0dKGIiIiImNMnePxU4oqN5vZkiHqrS7Uz/+9Yoj9Vh5pw0RERESkMnWO\nx88tZNHd8ytVMLN5wGnp5s2FfQEen2auqOQpo26hiIiIiAygzvE4cffdwM/SzXeZWaXH+l1AE7Hw\nyPdy238IHEhlbyruZGZ1wCVj2mARERERUed4nL0P6CdmorjCzFZDzGNsZu8B3p3qfSg3NzLuvh/4\naLr5t2b2FjNrTvuuIRYUOXqC7oOIiIjIrKHO8ThKq+m9keggXwQ8bGa7iSWkP0gMvPsq2WIgeR8g\nIsh1xFzH+9K+DxFzIr82V/fQeN0HERERkdlEneNx5u7/ApwOfI2Ymq0V2Af8CLjI3V9VaYEQd+8G\nLiBWyrud6GD3Af8DnEOWsgHR2RYRERGRUTJ3zQg2HZnZ04AfAw+5+7pJbo6IiIjIjKDI8fT1znT9\no0lthYiIiMgMos7xFGVmtWb2DTN7dpryrbT9MWb2DeBZQA+RjywiIiIiY0BpFVNUmq6tJ7epnRic\n15Ju9wNvcPfPTXTbRERERGYqdY6nKDMz4PVEhPgUYClQDzwK/By4zN1vHvwIIiIiIjJS6hyLiIiI\niCTKORYRERERSdQ5FhERERFJ1DkWEREREUnUORYRERERSeomuwEiIjORmT0ItAEbJ7kpIiLT1Tqg\n3d2PnsiTztjO8fLlSxzgYHd3edv+9nYAatIEHQvmtJXLTlq3FIAVi1oBaJ27sFxWUxMP0772/QC0\n94wW3SAAACAASURBVBwsl+3YsROA/p7a2H/NmnLZvv17AahN8fn1a44pl935q+sBqJs7t7ytsTH+\nPnp11KtvyaY5vvfBuwHYeSDa8thTn1Iua6nriP3r4o517dlWLuts3x73pzHaZ33Z49HWuhiAf7ji\n54aIjLW25ubmhRs2bFg4fFURESm666676OzsnPDzztjO8alPPBeAzVs3lrfdf/+9AHTuj05yV66j\neMijf1jXHIvR9VFbLuvYF/V7+qKzun/vrnJZTepWzp+/IOp0dpTLeg9E5/hxpz8ZgLkN2cNtJxwL\nwO5UB6B9T/y9a+t9AJz4+BPLZXVp1+6u6Jj39WYd5wPdse3BzQ8CsHrVynKZ18WaIXVz0v3Kdex7\ne3sRkXGzccOGDQtvuummyW6HiMi0dNppp3HzzTdvnOjzKudYRKYUM3urmd1pZp1m5mb2tsluk4iI\nzB4zNnIsItOPmb0c+BhwC3AZcAi4YVIbJSIis8qM7Rw3NjYCsPqoLM/XaiNVYveuHQD0HcrSKrr7\n47q/pgGAvbt3l8t6eyKFoa4+ciga6+vLZT2HDqY6h+KYnYfKZU0pLr9oTnPU7ciOuag1zjOndVF5\n23YiZ7gppV/Ma51TLlu8KOpt3PoQAFs2PVAuW7lsftyHlCXxi2uvK5cdffRR8UddtGHhvCz9sbY7\nS+kQmSKeV7p290cmtSVj4PYt+1j37u9OdjNEZIxt/NAFk90EGUdKqxCRqWQlwEzoGIuIyPQ0YyPH\nv/5lRE89Nw/D/v17ALD6uNtLl2cD19YcFbNMtM6LgWsde7Io7/wFEW3dtzdmgchHdNtTvdoY90Zz\nc0u5rDmCw/Qf3AdAXV8WVa5PUegayxq4fEnMHtGTos+tzVmEuhRNNiLE3d2dHWvLpk0AtM2L2Tf2\nNDWXy2699S4ADnXFfuvXZpHj45ZlbRWZTGZ2KfDXudte+tvdLd2+Gng58LfAc4DlwB+6++VpnxXA\nXwIXEJ3sfcA1wAfd/bBRcWY2D3g/8BJgMTHl2ueAbwP3A19y94vH9I6KiMiUN2M7xyIyrVyVri8G\n1hKd1qKFRP5xB/BfQD+wDcDMjgauJTrFPwX+AzgKuAi4wMxe7O7fKR3IzJpSvScQ+c1fBeYB7wWy\neRKrYGaDTUdx4iDbRURkCpuxneM0rS97U34xQOf+NA1aut1aVw5OMXdDTK1WVxMR1sWLs1zgfTvj\nF97ly5YBcPDQgXJZn0ei74IU9e1q31MuW5K2taRostVnWSw1FnnMfV3t5W293TGP8qKFMedyfV02\nnVxLc+RQL1kcx6ytz/51tekePe7xZwFw6uPPKJf94uofAbBrRzwOjfXZfe7tzqaDE5lM7n4VcJWZ\nnQesdfdLK1Q7BfgK8Fp3L85D+FmiY/yX7v7B0kYz+zTwc+BLZrbW3UtzLb6T6BhfAbzS3T3V/yBw\n81jdLxERmX6Ucywi00U38GfFjrGZrQaeCTwMfDhf5u7XE1HkhcCLckWvJiLPf1HqGKf6m4hZMqrm\n7qdVugB3j+Q4IiIyNahzLCLTxUZ3315h+6np+hp3r/RzyE/z9cysDTgG2OLuGyvUv3a0DRURkelr\nxqZVLF0YSzHXHMrSHFbMj8ForXMjzWHR8qXlstpDkd6wf28MdOvcu69cNq8t6h911FoA9uzPBuvt\n2BOf1c2tUWf7pvvLZcetj/q1dWlgXW1fuYy0JHVdTRYEa+iM8oaWJgC6cqvtNTZEisVRa2Jqtrvu\nyKZyW5DuT20arNeWW5J63VHLAVi/avH/Z+++4+S66vv/vz4zs31Xu+rVttybwDY2LjTLEDA1EEJC\nCQSbFOqXml8wLbYhlG8SSr5ODCQE/MVAKHGA0L44ATcMNriCbbnJkmX1vqutszNzfn98ztx7PZpd\nSast0uj9fDz8mNl77j33zGq8e+azn/M5TzoHYH5HuuBP5DCweYzj3fFx0xjt1eM98bG6b/yWOueO\nd1xERI4AihyLyOEijHG8+kl20Rjti2vOqyb6Lxzj/LGOi4jIEaBhI8eF4Ivmjl00Lzl22imnAdAa\na6wNDe9J2kaLviHIhp2++G7nlnQh3/HLnwVAW9xYhKZ0sV5Pz2wAKjENcmh4MGmrxM8eo9VqbbnW\nTJv/9TffNis51jXXI8C7du4AYLiUbtKRa/ExW8WvGxlMFwVu6/ff+Xfeej0Ac7rTyHGhPAxAadQf\nly5dmrQ1NzUj0gDujo/PMrNCncV6F8XHuwBCCH1m9hiw3MyW10mteNZkDWzF0m7u1GYBIiKHFUWO\nReSwFkJYD/w3sBx4d7bNzM4DXgfsAr6bafoq/vPvk2ZpsXEzO6q2DxERObI0bORYRI4obwFuBf7e\nzF4A3EFa57gCXBpC2JM5/++AV+CbipxsZtfjuct/jJd+e0W8TkREjjANOzk+Ki5Em9OS7hY3K2YR\nlIZ8QV1rZmF7cch/b3Y0+aK44845M2mbN9vX+8yb6+kUm3ZkFszHmFOxNOT3Pe7YpKl9lqdMNLX7\njnrlzG59oxVP4wjldAx9e7zfrZt93dGKs05Lb9PpKRnb+rYDcNrppydtg72ehnHUUk+tbGvK/E4f\nimMp+Itvs2LSlLMWRBpBCOExMzsH3yHvxcBKPLf4/+E75P2m5vwhM7sI+Ci+Q957gDXAJ/Bd9V5B\nmpssIiJHkIadHIvI4SeEsHKM41bveM05G4C3HsC9dgPvjP8lzOwv4tNV+9uXiIg0joadHM+Z6wvO\ny3vSv6Tuis/bY0m1UE4XtbXiEdWjTvCd8pqb08Vq5Yqf197m367+vjSg1NTiUeEQd7+bt/jEpC1U\nd7jL+3VWyUR0y77wfuuG9cmhrWsfB+DEE33X2WXHpH0Vyz6+njOWA5BrThfybXrC+xjs9wjyUP/2\npC1nPq6mDo9+h5COId/WiciRysyWhBA21hw7CvgIUAJ+WPdCERFpaA07ORYR2YfrzKwJuBPYjS/o\neynQju+ct2EGxyYiIjOkYSfHg0MeaR0YSo8VqlHTLo/ojmYiwCcuORqA1li/w/JpVHmo5Nf1bl8H\nwOlnPD1pa9/o+wusXnMvAN1zZ6dj6PVI9caNHhFuy6V/Gd6z3fcZqAynAzz/6d7v8uNOBmDO3LQM\nXah4daq+3V7erXdPGh1uD/5am2Z5CbfdMYIMkG/1iHFf2fOLm/L5pG1hT1qSTuQIdC3wBuAP8cV4\n/cDtwD+FEP5zJgcmIiIzp2EnxyIi4wkhXA1cPdPjEBGRQ4vqHIuIiIiIRA0bObZYIq05zSKgd6un\nMuSHfTHcsUsXJG3NbV4qrRJ3yssuXCMuttu+069vmntC0nTiySsAyDV72kPI1GvLzfLn6zZ7Okb7\n7DTlYvnRnsYxf166U+38Rf68q6sHgJHh4aRt+xZfdNccd8ob7d+VtBXMx1qOC/4629PydeVYa24w\n7qg3mknjKD+p7KuIiIiIKHIsIiIiIhI1bOR4blc7ABt3PZ4cG9rhVZtOPcqjvV1t6SYYI2WPqObb\n/LpSuZx2lvfzcvjj7l1bkqb2nvkAHHesl19bdd/dSVsh52Hrnm6PGLc0p1Hlk071RXfHHnN8cmzb\nZo8Ob13/CACtbR1JWw6PTOfN+2xNK82RL/hnnM07fXOTtqa0rW9w0M+P0WTLfBxaclS6YYmIiIiI\nKHIsIiIiIpJo2MhxecDLmbVURpJjT3vqUwBYONfLm1kpzbktxK2eZ8dIrpFGeUslz1Eulj0ku7u/\nlLStX+dR3tmxhNu9t/8qaWtt9fPPefp5ACyak9nKepZ/LunflW4CkovbWXe0e1jYLM17njPH+9/T\n56XcqKTbQOeDR7lb4uYmxZH0uu5Z/lpzzR4R7zp6ftLWkykVJyIiIiKKHIuIiIiIJDQ5FhERERGJ\nGjatojLoaRXLFsxJjs2bG5+XvKxZIF10N2fJIgCWHO8L60aG0rSFjZs2A1As+fk9c9Od5bZu9h3y\nenf4gr7Fi9LSbH29vkBufjx/0fz2pG3T476j3tyeRekYuv15c97Ps3z6zzMQF9ZV4k55HR1pX7t2\n+W557S2ettHePStp27anH4DhWKJu/cYnkrZVj64B4M/egYiIiIigyLGIHGLMbK2ZrZ3pcYiIyJGp\nYSPHszs7AVg4L40cV8q+OC9f8MV25Xxa82zxkuUAdM/1jUEeuO++pG0kbuzRMcv7amtPS6zt3OaR\n2NGhPgCWLUsjx5vxBXZN5gv6SiMDSVtzzhfN9XSmEeB8zs8bGuwFoLWzO2krjvjYS2WPHBcKaS23\nGNAmV/DXc+xJT03a1t9yAwBbdnkkfdeedGOR+x5Oo8giIiIi0sCTYxGRmXbfhl6WX/ajmR4Gaz/1\nkpkegojIYUNpFSIiIiIiUcNGjhfO8xq+uXJa57ipuj1cLGFcLqefDXp3+KK2HSO/A2B4KK1lvPQo\n383O8r473ab1jyZt+WbPaTjhhOMAeOi+h5O2nVs3ANASF9YVSPvs7PK0j5bmdJe+4qiPtRz8vOHh\ndOy5Jk+jqHjmBdt27k7a9sS1g80hpomENOWi7JkdbN3ou/o9tGZN0lYiTekQmU5mZsDbgbcCxwM7\ngO8CHxrnmtcCfwmcCbQBa4CvA38fQhipc/4pwGXA84AFwG7gZ8CVIYSHas69BnhjHMtLgL8ATgRu\nDyGsnPgrFRGRw03DTo5F5JD2OeCdwCbgX4BR4OXAeUAzUMyebGb/BrwJWA/8Jz7RPR/4GPA8M3t+\nCKGUOf+F8bwm4AfAo8Ay4JXAS8zsohDCXXXG9Y/As4EfAT+GTEkbERE5IjTs5LhSXbhWSXeLa27x\nl1sxDx1X8mnUtjzq55cGvCTb6SuenbRt3u5R2ptv+KFfN5rurLfiKScCsHix7zxXHk7vt+qeewCw\n+Ps1lEaTts62WK4tl0avd+3aCkC+yUuy9fWl0eGBEb92d58v/Bspp/90C472BXgWvK8tm3ckbd3d\niwHYtu1WAObGBYcAPT2zEZluZvYMfGK8Gjg3hLAzHv8QcAOwGHg8c/4l+MT4u8CfhBCGMm1XAJfj\nUeh/jMdmA/8ODALPCSE8kDn/dOB24EvA0+oM72nAWSGENXXaxno9d47RdMr+9iEiIocO5RyLyHS7\nND5+vDoxBgghDAMfqHP+u4AS8KbsxDj6GJ6S8SeZY38K9ACXZyfG8R73A/8KnGVmp9W5198dyMRY\nREQaT8NGjluaPP+2iZAcCzGabE3+sltbOpO2jlierWPhcgBOPvmMpO13938bgO99+zsAXPLmS5K2\nM59+PgBbHrsbgDWr09/Fe3o98rtlwzof0+KepK29y/OXBwf7k2Ojo/6X5MGij3Pn7t6kbXDYj3XN\n9lJxZ6x4ZtpXz/EAfOlzH/f77kojzitf+JL4Wr0s3PxMqbnmUtq/yDSqRmxvqtN2C5BNj2gHzgC2\nA++2+FefGiPAqZmvL4iPZ8TIcq2T4uOpwAM1bb8eb+D1hBDOrnc8RpTrRadFROQQ1rCTYxE5ZFUL\neG+pbQghlM1sR+bQbHwJ7Xw8fWJ/VLew/It9nNdZ59jm/byHiIg0KKVViMh0q/7JYmFtg5nlSSe3\n2XPvDiHYeP/VueaMfVzzf+uMLdQ5JiIiR5CGjRy3xd3vKpV0sXnc6I5c3kud5TML8tpbuwAojPr5\nOzLpEcva/fw3/uHLALj4uRclbYvm+AK3R2/3nefu+NUvkrZ8/BPwyICnVQ4Vu5K2obimb2Rke3Ks\nGNfrjeIL8jqWpDvdHb90hT+e4H+l7d22Lmn74Tc+DcC2xz21Y96iZUlbS6u/nqULPVhXHEoXE/bM\nrxc4E5lyd+HpBhcCj9W0PZvMz6UQQr+Z3Q+cbmZzsjnK47gN+MPY128nZ8gTs2JpN3dqAw4RkcOK\nIsciMt2uiY8fMrNkf3czawU+Wef8z+Dl3b5sZj21jWY228yyub1fwUu9XW5m59Y5P2dmKyc+fBER\naWQNGzm24BHTXC6fHBsNXmatKUaOy+W07FqT+TEr+qK4LQ+mAadO8yj0actP8Ov6BpO2B+9dBcDG\n9QMA7E4DsyxffpQ/nvAUAJo7ZyVtw83eZ3ch/SfomeN/ZR7BI9pbt6adPfz4JgDuvuc6AJ6496dJ\n2+yCt53/dF+TlOvKzh98rAsW+F+qd+7MpHPWX9wkMqVCCLea2VXA/wLuM7P/IK1zvAuvfZw9/8tm\ndjbwNmC1mf0UWAfMAY4FnoNPiN8Sz99hZq/CS7/dZmY/A+4HKsDR+IK9uUDrVL9WERE5/DTs5FhE\nDmnvAh7G6xO/mXSHvA8C99aeHEJ4u5n9BJ8A/x5eqm0nPkn+e+BrNef/zMyeCvwVcDGeYlEENgI/\nB66bklclIiKHvcadHBdixDizTqcyGiPFcQ/mpkxUuT9uHz0y4hHnSjnNR94RI8U7Ytm1WSekEeBf\n3e7VqO69wyPN1plusnHOc18JwFOf8XwAemanbXOO92pSu7emkdw1D3ue8w0//CYAt992Q9K2ffM2\nv3csOXfh2Wn1qCWdvnX1SMGj0cXmdFvoSs6PzZrvUend/QNpmyLHMkNCCAH4p/hfreVjXPND4IcH\ncI+1wDv289xLgEv2t28REWlcyjkWEREREYk0ORYRERERiRo2raJsnkJRKadlSzvjgrhQ8mP5zPld\nzb4g74RjfdHdYLEpaRt69Al/Ms9Ln/3PTbcnbd/73vcB2L3N0x6ecl6a7nDOha/yvgZ9Yd3AxnR/\ngZ/fdjMAq+79XXJsxxZv39XneyN0dbYlbSc+8xwAejp8DVFXZvDVXcPmL1jir68zLRO7ev1GAHoH\nPDWkbXZSHID5s7sRERERkZQixyIiIiIiUcNGjgsxEpx9hdVPApVY5q2lKa3ktG3jagBGRnzBWu9w\nc9K2dfsIAP99+28AuOE3t6Wd5v0G3W2+wUff7t1J0xOPPQzAUT0ere3dke6We29cyDdKuiiuo9sj\nxS2dvniuKZ9uILZ0iT9f8/D9fp+dW5O2k48+FoANGzcAsIdt6dj7fBFhb79HjgtN6eva+MTjiIiI\niEhKkWMRERERkUiTYxERERGRqGHTKgK+6C6EdBe8ymgJgNGhIQCam9JFd7mct+3oXQ/AzsH0W7Nm\nvacwPPLoPQDk08s44QRfwFfatcsfe9PUia9ddTkAr/79Pwbg2OXHJ22nnXgMALtLw8mxBx+4G4DT\nT386ALO70oV1G9Z62scD93gt5PPPPidpK8Vaxs1xIV4rmcWEW3YCMKt7NgBPPP5I+pqH053+RERE\nRESRYxERERGRRMNGjgtxh7x8JsxbyXs0uboELpfPZ9o8cpxr8wVrrZmFayHnC/KWLPXSZ+ec8Iyk\nbenCowDY/IgvvmvJ1FhbttCjtZ15v740kEaVH733F35sdnqfltZRAObO9sV9I3vSyO4Tj60D4Njj\nz/JzZ6WL9doX+PnW4pHjPbv707YuXwxoOY+gL16UXtfVnJaKExERERFFjkVEREREEg0bOW4u+Lw/\nZ2nkOJ/zlzva3OKPI8WkLTR5FLWpYx4Aj9x9R9I2MOSbeJxz1hkAzJm3IGnriH0dd57nABctjfZ2\nt/v9Fiz0iO7G9WnptK5ZHu09/ulPS46VKz7m4d3ex5qHH03a5sxdBEAl5+M8/ox0s5Gyefm5TZs8\nv3h330DStmC+R7Zv/Z/rAGgpjCRthcVHIyIiIiIpRY5FRERERCJNjkVEREREooZNqygEn/eXK6PJ\nsXKIS/HMF+a1xdQGgNFWX7h2+933AbB5c1/SNn/+MgBOPPZEAFpCmpowMuBl4XriDnY7+9NUjfkL\nFwNQLPuOfDSnO/Kd9ewX+Jia07SPh+7xUm67tmwHoKNtdjp28/SNfEcnAO0L5yRtu/r9n/GJ7b4o\ncOumjUnbvM4OAE457jgAZrWnn4da2rsRERERkZQixyJyxDOzG83ip2YRETmiNWzkONfkUeJq0Bag\nkvPffa3tswDo7U2jww+v/p2fb+0ArHjac5O2bZvWArCnd0fsO+2zo9XPP+PpzwJgmDSqvGeHR3Bt\nxMvEzV92XNI2EIPY6x5akxzbtNbPnzffF98VcmmZN5r8n+rEs1YA0D4vEzke9Rd59x2/BmD7urTP\nC87xhYKnLffFd33b1iZtj6y6ExGZOvdt6GX5ZT+a0nus/dRLprR/EZEjjSLHIiIiIiJRw0aOh1vi\nk3ya57twsW/Z3N/vpdLu+eUNSduyEzwiuzBurvH4E2ne7sYdGwDoWXwsALkFaSm3UvDI8fZeL/cW\nSLer3rnL71Ma9Ah1e3t70vbww7/14WX+kLtg/nwAhsr+maWtI92kY/Ysvzb0e1+bHlmdtN19p5ed\ny/V5rvKZx6YbfZy/wre33vy4l4XbvH5d0tbT2YPI4cbMzgXeBzwLmAfsBH4HfCmE8O14ziXAy4Cz\ngMXAaDzn8yGEr2X6Wg6syXydTa24KYSwcupeiYiIHIoadnIsIo3HzP4C+DxQBv4LeARYAJwDvA34\ndjz188ADwM3AJmAu8GLgWjM7OYTwkXjebuBK4BLgmPi8au0UvhQRETlEaXIsIocFMzsNuBroA54d\nQri/pn1Z5ssVIYTVNe3NwE+Ay8zsCyGEDSGE3cAVZrYSOCaEcMUExjVW8v4pB9qXiIjMvIadHHfM\n8QVrS446ITlm5qvgHlrl5dpWnHlO0tY+18u6rVm/HoAtWzYkbS/6gz8E4KnnngnArTekC2wee8TL\npxWH8wC0tqZpHKE8DEAeL+9WHOpN2no6/LzmfD45tnvEn2/a4OkRG9Y9lLSV+rYAkGv20my57mOS\ntiL+us483X8Xn3JUJu2jbzMA/bv8+uXHnZy0kW9B5DDyVvxn1sdqJ8YAIYT1meer67QXzeyfgecC\nzwO+OoVjFRGRw1TDTo5FpOGcHx9/sq8Tzexo4P34JPhooK3mlKWTNagQwtn1jseI8tPqtYmIyKGr\nYSfHTXhUdPOatcmxkRHfsGPObN9co7Ul/X1ZbPFyaE899+kADJNuEFL1Pz/4PgAP/faO5NgF5/lm\nHh1NvmBuZHAgaVuwwBfY3f4L/13eM3tW0rZowRIARovppiGzuucCsDD4uPpb0qhy+2KPBrd3e599\npbTM2xNP+Gs8/fSn+Gsv7kratm3xtpNPPg2AXHP6urbt2LnXaxQ5hFVXkG4Y7yQzOw74NTAbuAW4\nHujF85SXA28E9GcTERGpq2EnxyLScHbHx6XAg+Oc9158Ad6lIYRrsg1m9lp8ciwiIlKXJscicri4\nDa9K8SLGnxxXFxpcV6ftwjGuKQOYWT6EUB7jnAO2Ymk3d2qTDhGRw0rDTo77tnvKwNBAf3JszjxP\nSWhr85SE4mjaNn+ZpzksPsnTDzrmpQvXrv3iVQCUhv38c856TtLWYr5d3uBuX/DW0d6RtDXHIsbd\nnb74rjycplwMD3gQrK29OzlWmOVpF8XNnhYRKmnN5J5587wPPNVia0yl8H699nGzlePXmdc111M1\nmpv9n7p/JG3LFzJb/Ykc+j4PvAX4iJn9NITwQLbRzJbFRXlr46GVwA8y7RcDfz5G3zvi49Fk6h6L\niMiRp2EnxyLSWEIID5jZ24AvAHeb2ffxOsdz8YjyHuAivNzbpcB3zOw6PEd5BfBCvA7yq+t0/zPg\nj4D/NLMfA0PA4yGEaw9iyMtXrVrF2WfXXa8nIiL7sGrVKvC1ItPKQgj7PktE5BBhZhcAfwU8G1+k\ntx34Lb5D3n/Ec54B/C2+Q14BuBf4Bzxv+QbgymxNYzPLAx8DXgMcFa85qB3yzGwEyMd7ixyKqrW4\nx0tTEplJZwDlEMK0LqLW5FhEZApUNwcZq9SbyEzTe1QOdTP1Hs1N581ERERERA5lmhyLiIiIiESa\nHIuIiIiIRJoci4iIiIhEmhyLiIiIiESqViEiIiIiEilyLCIiIiISaXIsIiIiIhJpciwiIiIiEmly\nLCIiIiISaXIsIiIiIhJpciwiIiIiEmlyLCIiIiISaXIsIiIiIhJpciwish/MbJmZfdnMNprZiJmt\nNbPPmdnsA+xnTrxubexnY+x32VSNXY4Mk/EeNbMbzSyM81/rVL4GaVxm9iozu8rMbjGzvvh++toE\n+5qUn8djKUxGJyIijczMjgd+CSwAvg88CJwLvAt4oZk9M4SwYz/6mRv7OQn4OfBN4BTgUuAlZnZB\nCOGxqXkV0sgm6z2aceUYx0sHNVA5kn0YOAPoB9bjP/sO2BS81/eiybGIyL5djf8gfmcI4arqQTP7\nDPAe4OPAW/ajn0/gE+PPhhDem+nnncA/xvu8cBLHLUeOyXqPAhBCuGKyByhHvPfgk+JHgQuBGybY\nz6S+1+uxEMLBXC8i0tDM7DhgNbAWOD6EUMm0dQGbAAMWhBAGxumnA9gGVIDFIYQ9mbZcvMfyeA9F\nj2W/TdZ7NJ5/I3BhCMGmbMByxDOzlfjk+OshhNcfwHWT9l4fj3KORUTG99z4eH32BzFAnODeCrQD\n5++jnwuANuDW7MQ49lMBro9fXnTQI5YjzWS9RxNm9mozu8zM3mtmLzKzlskbrsiETfp7vR5NjkVE\nxndyfHx4jPZH4uNJ09SPSK2peG99E/gk8Gngx8A6M3vVxIYnMmmm5eeoJsciIuPrjo+9Y7RXj/dM\nUz8itSbzvfV94GXAMvwvHafgk+Qe4Ftm9qKDGKfIwZqWn6NakCcicnCquZkHu4BjsvoRqbXf760Q\nwmdrDj0EfNDMNgJX4YtKfzK5wxOZNJPyc1SRYxGR8VUjEd1jtM+qOW+q+xGpNR3vrS/hZdzOjAuf\nRGbCtPwc1eRYRGR8D8XHsXLYToyPY+XATXY/IrWm/L0VQhgGqgtJOybaj8hBmpafo5oci4iMr1qL\n8wWx5FoiRtCeCQwBt+2jn9viec+sjbzFfl9Qcz+R/TVZ79ExmdnJwGx8grx9ov2IHKQpf6+DJsci\nIuMKIazGy6wtB95e03wlHkX7aramppmdYmZP2v0phNAPXBvPv6Kmn3fE/n+qGsdyoCbrPWpmAbNG\n0gAAIABJREFUx5nZ0tr+zWwe8JX45TdDCNolT6aUmTXF9+jx2eMTea9P6P7aBEREZHx1titdBZyH\n1yR+GHhGdrtSMwsAtRsp1Nk++tfAqcDLga2xn9VT/Xqk8UzGe9TMLsFzi2/CN1rYCRwNvBjP8bwD\neH4IYffUvyJpNGb2CuAV8ctFwMXAY8At8dj2EMJfxXOXA2uAx0MIy2v6OaD3+oTGqsmxiMi+mdlR\nwEfx7Z3n4jsxfQ+4MoSws+bcupPj2DYHuBz/JbEY2IGv/v+bEML6qXwN0tgO9j1qZk8B3gecDSzB\nFzftAe4Hvg18MYRQnPpXIo3IzK7Af/aNJZkIjzc5ju37/V6f0Fg1ORYRERERcco5FhERERGJNDkW\nEREREYk0ORYRERERiTQ5PkhmdomZBTO7cQLXLo/XKvFbRERE5BCgybGIiIiISFSY6QEc4UZJt0IU\nERERkRmmyfEMCiFsAE7Z54kiIiIiMi2UViEiIiIiEmlyXIeZNZvZu8zsl2a228xGzWyLmd1rZv9s\nZheMc+3LzOyGeF2/md1mZq8d49wxF+SZ2TWx7QozazWzK83sQTMbMrOtZvbvZnbSZL5uERERkSOd\n0ipqmFkBuB64MB4KQC++PeEC4Knx+a/qXPsRfDvDCr7lZge+3/c3zGxhCOFzExhSC3ADcD5QBIaB\n+cBrgN83sxeFEG6eQL8iIiIiUkOR4729Dp8YDwJvANpDCLPxSeoxwDuAe+tcdwa+Z/hHgLkhhB5g\nEfAfsf2TZjZnAuN5Kz4hfyPQGULoBs4C7gLagW+b2ewJ9CsiIiIiNTQ53tv58fGrIYSvhRCGAUII\n5RDCuhDCP4cQPlnnuh7g8hDC34YQdsdrtuAT7G1AK/DSCYynG/jLEMJXQwijsd97gIuBHcBC4O0T\n6FdEREREamhyvLe++Lj4AK8bBvZKm4iT65/GL1dMYDyPA9+o0+924Ivxy1dNoF8RERERqaHJ8d5+\nEh9fbmb/ZWavNLO5+3HdAyGEgTHaNsTHiaQ/3BRCGGsHvZvi4woza55A3yIiIiKSoclxjRDCTcDf\nACXgZcB1wHYzW2Vm/2BmJ45x6Z5xuh2Oj00TGNKG/WjLM7GJt4iIiIhkaHJcRwjhY8BJwAfwlIg+\nfLOO9wEPmNmfzuDwsmymByAiIiLSSDQ5HkMIYU0I4VMhhBcCc4CLgJvx8ndXm9mCaRrKknHaqnnR\nZWDXNIxFREREpKFpcrwfYqWKG/FqE6N4/eJzpun2F+5H230hhOJ0DEZERESkkWlyXGMfC9uKeJQW\nvO7xdFheb4e9WDP5L+OX35mmsYiIiIg0NE2O9/ZVM/uKmV1sZl3Vg2a2HPi/eL3iIeCWaRpPL/Cv\nZvb6uHsfZvZUPBd6PrAVuHqaxiIiIiLS0LR99N5agVcDlwDBzHqBZnw3OvDI8ZtjneHp8HlgJXAt\n8CUzGwFmxbZB4I9CCMo3FhEREZkEihzv7TLgr4H/BzyGT4zzwGrgK8DTQgjXTuN4RvDFgB/FNwRp\nxnfc+2Ycy83TOBYRERGRhmZj7y8hM8nMrgHeCFwZQrhiZkcjIiIicmRQ5FhEREREJNLkWEREREQk\n0uRYRERERCTS5FhEREREJNKCPBERERGRSJFjEREREZFIk2MRERERkUiTYxERERGRSJNjEREREZGo\nMNMDEBFpRGa2BpgFrJ3hoYiIHK6WA30hhGOn86YNOznevm1wzDIc5XIFALNKciyXs3gsD8DwcDFp\nq5TjObGtWCwlbQMj3tg/6o+5fD5pm9XeDMDIQC8AzU1NSVtnZ5ePc/uO5JiZj6G9o8OvK6VjGB4Z\nAWDP0LCPoZS+vNGSv45SPL9SLmdel4+nqcXHUipm+/Tnr3npuYaITLZZbW1tc0499dQ5Mz0QEZHD\n0apVqxgaGpr2+zbs5FhEGouZ3QhcGELY7w9zZhaAm0IIK6dqXONYe+qpp8658847Z+DWIiKHv7PP\nPpu77rpr7XTft2Enxy2tHjEdLY6mB+Ov1FwS3E2jvJUYRC7FqOuTyj+bp2aXYrR2sJRGZgfj+UOj\nHoVtDWkadz54/7O62v3rpvTbXYo3zBeak2Plkg9weDjUDo/mlhbvP45zpJR+kqrEvqpzBsul92lq\n8v4LBT82OpJ+P8yUci4iIiKS1bCTYxER4FRgcKZuft+GXpZf9qOZur2IyKRY+6mXzPQQppUmxyLS\nsEIID870GERE5PDSsJPjpiZPGSiX916XV01DGBwcTo6VK9UcBr+uWExTJ8px/d1IXMzWO5xeV12I\nNxoX5g2FNG0hN7wHgAVL5vr9imkAa9fOfgAGBtL7VEZ9DLmcjyHXNJK24YMYjvcZzKRHjMY0jxDH\nbrk0XWIoLiys9lmppPcLlXRBoshMMrPfB94FnAbMAXYAjwDfCiFcXXNuAfhr4FLgaGAr8A3gIyGE\nYs25e+Ucm9kVwOXARcAxwLuBU4A9wA+BD4YQNk/6ixQRkcOCkk5FZEaZ2V8C38cnxj8APg38GGjD\nJ8C1vgH8L+AW4PPAED5Z/uIB3vo9wBeAe4HPAQ/F+/3SzOYf8AsREZGG0LCR4+qCulxmcVqIB6vR\n0+FMSbZSJbbFSLMHp2JbXHQ3OOyR3JHRdDHc8LBHcCtx8V11ARyAFXwRXTXa2zc4kLTt3OXl3YaH\n0uit0RLH7J9Z8vnMojv8vGLpyY8Ao7E0XaniC/JC9jNPXIRYyBdin7lMkyq4ySHhzUAROCOEsDXb\nYGbz6px/PHB6CGFnPOdD+AT3T83sAwcQ9X0RcF4I4e7M/T6LR5I/BfzZ/nRiZmOVozhlP8chIiKH\nEEWOReRQUAJGaw+GELbXOff91YlxPGcA+Dr+8+ycA7jntdmJcXQF0Au8zsxaDqAvERFpEA0bOR4c\n8ChvObMhRjUCPBJzhgcH05zegRgBLsWIbCGfKbEWuxiMkd+Rcl/SVt0sJOQ6/frR9H6FGLUuBo8A\nD2U24CjHSLPl0+htNbIdzCPalgnsFuLmIiFuRFImkzscn4/EgVZCJhpdLUNXHo1fp50WcmPukyIy\nnb6Op1Lcb2bfAm4Cbg0hbBvj/DvqHHsiPs4+gPveVHsghNBrZvcAF+KVLu7ZVychhLPrHY8R5acd\nwHhEROQQoMixiMyoEMJngDcC64B3At8FtpjZDWa2VyQ4hLC7TjfVHKl8nbaxbBnjeDUto/sA+hIR\nkQahybGIzLgQwldDCOcDc4GXAP8GPAf4qZktmKLbLhzj+KL42DtF9xURkUNYw6ZV7NzlpdKGM2XX\nyrHkWbWU29BomlbRH3fSKw4V4znZ3uK3Kfj15fKepGU4loOzlia/fjRd5Dc05Oe3FeICu3ppDNkU\niOpHlXheJbNNn1Wq5/hJTU1NSVuluqFeNWaW2aUvSdWo7qKXvbWyKuQQE6PCPwZ+bJ4T9Cbg2cB1\nU3C7C4GvZg+YWTdwJjAMrDrYG6xY2s2dR1jxfBGRw50ixyIyo8zshZYtD5OqRoynaoe7N5jZWTXH\nrsDTKf49hDCy9yUiItLoGjZy/Ph2X8xeGk0XwFcjsZUYQS5mFuuVY/i1Gl0mEzluiVHafFxglyul\ni/V62jxSXDa/T18xTXkcKfmxaqm1lszmHLlc9bz0RiGmTTbFSHNLIRPajX1YjCrnLPO5prrxSNyt\nxCp7p102xXsXCuk/eaW8V3EAkZnwTWDYzH4BrMULED4beDpwJ/A/U3TfnwC3mtm3gU3As+J/a4HL\npuieIiJyiFPkWERm2mXAr/DKDm/DN+JoAt4PXBRCmKpPcZ+N9zuTdJe8a4Bn1NZbFhGRI0fDRo43\n7PCc40omy7YabS3H/NtyZivlaoS5ullGW2Yzj3wsf1YJFs9Nv23trf68tcmjtUPDaam0oWpptVi+\ntVxKrzNL750ci9HdkZJHgIv59Jxc7La6gUmlkr6u4mg1Qp2MeK++c/E1lDPR4qCkYzkEhBC+gO9U\nt6/zVo7Tdg0+sa09Pu5ON2NdJyIiRy5FjkVEREREIk2ORURERESihk2r2LzbS6xlUwdCLJtW3TWO\nTGqDxVppubgobo+lC9WrO93l4tq5fCldPN/e1QpAZ49vzLVzIL0uWQwYbzeSKR1XHVXIlHLLx0V6\npVgOrqkp/edpigvpyuW4qPBJteb8ukpMnQikf0mubohX/TaEUnpdzsb9i7OIiIjIEUeRYxE5ooQQ\nrgghWAjhxpkei4iIHHoaNnI8XKzGZrMRVv8sUAnVDTGyi+I8ytva0hzPSaOqw8NDAOTj9U2VdKOP\n9p4eAEoxkrtjx/akrRwX6+XMx1IaLSZthRgVzmXKu40WPdpdLJbi+DrStupCvGpJtzpR33I1PFxn\nA9309Mx1WpAnIiIi8iSKHIuIiIiIRA0bObbyUHyWKeUWc3qtWsLNsls3+7Gutk4AWjOl3HIx2lot\n75YnLYc2d47nGo8OeT5xV2e6rXMpbuYRzCPBA4NpxDlfJ2pbGfXI8eiIP+bymdJv1Y09qrnDmddV\nzaEuV6Pkma6tWgOuekyBYxEREZExKXIsIiIiIhJpciwiIiIiEjVsWkXvlocAaG9rT451dXUBsLt3\nN5BJOQC6Z/nit9nN/tjZ3pK0dbR5ubZ8zlMmyiFNnRjo2wFAruwpE4vndCVtw7EsXC4fUzS6OpO2\nQizN1hvHArBtuNfH1eTpEZUwlLQN9PvzQlzA19LSmrTlY/rFSFw4mCuk4wuVal/ZhYkiIiIiUo8i\nxyIiIiIiUcNGjjetvgOAJYuXJsfmtC0DIAxu9cdMMDXXPBeAfLENgEWLupO2lkJ14w2PBA+X088U\nhRgdzseycPN70ujwUMkX1jW1+GK6Od1zk7ZZXbMA6O/vT46tWevR4I2bNwGwrbcvabPY11AxbiRS\nTsu8VcvBrVu3EYD2jkz0esijyeVyLAWX2Twkn9dnIxEREZEszY5ERERERKKGjRyP9G0AYEcu3ep5\n14513ha3Z+5oT6PDvbt3ATDQ59Hap5x2WtLWPcsjsSFuDDI4lNZAa2nxPppirbRiMd2BY2jE793a\n5sc6W9O21lHPLw4jaXT4+IXeV0vFo8Q2mLZVU4yH23ws1pRGh3t7PTq8a7eXmBsYSbepHoll4UZj\nxLmY2cIass9FRERERJFjEXkSM7vRzKa8CraZLTezYGbXTPW9RERE9pcmxyIiIiIiUcOmVSycNw+A\nlrZ04VrI+cvNDXmqQVt7uniOvOctDFc8YBYK6Q55lYKXdRuKpdKyW8u1FGKaQ/zaSmmqwoL5CwEo\nlgcAGNizLWnbvW0zAE+sfjQ5dsqZ5wKwZOF8AHZuXJe0HXXMCQD053zMjz6xK2lLSsXlPS2jWEn/\nWStxV8AQd/ezXFqirlRMU05EMv4UaN/nWbJP923oZfllP5r2+6791Eum/Z4iIo2iYSfHIjIxIYR1\n+z5LRESkMTXs5Pi0k84BoLU1jZRaXDS3Jy6665i7IGlrm+3R2g0b1gNw1513JG2zZnlEtjTqC946\nWtI+Fy9cBMDChR4lbm1Lv6WlskeRd8aNPsrFNKpcGvG+mrvSRYEW++2JJd/OOv/CpK0QNzDZ0lcE\n4MHHNqV9jXoEuBKG49fpwr9SyUu4hUo12p1GxHO5hv3nlxpmdgnwMuAsYDEwCvwO+HwI4Ws1594I\nXBiqK1D92ErgBuBK4MfA5cAFwGzg2BDCWjNbG08/A/g48AfAXOAx4AvAVSGEfeYym9lJwJuA3wOO\nAWYBm4GfAh8NIayvOT87tu/Fez8Tf7P/BvhACOGXde5TAP4Sj5Sfhv88fAj4N+DqELRzjojIkUiz\nI5Ejw+eBB4CbgU34pPXFwLVmdnII4SP72c8FwAeAXwBfBuYBxUx7M/A/QA/wzfj1HwL/CJwMvH0/\n7vFK4C34hPeXsf/TgT8HXmZm54QQNtS57hzgr4FfAV8Cjo73/pmZnRlCeKh6opk1AT8ALsYnxN8A\nhoGLgKuA84A37MdYMbM7x2g6ZX+uFxGRQ0vDTo5bmj1C2ppPt4jOVTxaO1LyjTdsJE2rXLfGjzU1\n+dbLjzz8cNLW3uYbgyxb6huK9O1Mc4d7d/nzlla/riuzRfSWLVv8vq0+lvmZDUm6lh4NQGdLW3Ks\nEOu1DcV41WApjQDf99BjADy6zoNmd/7mtsyr9X/GUslfazbnuJqPnItbTBPSPvcdw5MGsiKEsDp7\nwMyagZ8Al5nZF8aYcNZ6AfCWEMIXx2hfjEeKV4QQRuJ9LscjuG8zs2+FEG7exz2uBT5bvT4z3hfE\n8X4YeGud614CXBpCuCZzzZvxqPW7gLdlzv0QPjH+J+DdIYRyPD8P/AvwJjP7jxDC9/cxVhERaTCq\nViFyBKidGMdjReCf8U9Xz9vPru4ZZ2Jc9YHsxDaEsBP4WPzy0v0Y64baiXE8fj1wPz6prefW7MQ4\n+jJQAs6tHjCzHPAOPFXjPdWJcbxHGXgfEIA/2ddY4zVn1/sPeHB/rhcRkUNLw0aORSRlZkcD78cn\nwUcDbTWnLN3rovp+vY/2Ep4KUevG+HjWvm5gZoZPTC/B85dnA/nMKcU6lwHcUXsghDBqZltiH1Un\n4WkljwAf9tvtZQg4dV9jFRGRxtOwk+ONG1cB0L87LXk2d5anUfTFVIjh9emi/FKbzw1+7/nPB+Dk\nk05O2goF/zZ1dXrKRC6XrtMZGRmK53haRbGY/t4+5pjlALTNmuXXNaeL4Ur9HhjbtDMd39rH7wPg\nzrvvAmD1Y08kbdt3+HmPrfExD8Vd/gCOO+kkf9Lsi/Z6e9Od9dpavJRdPucpG9VFiQBlrTc6IpjZ\ncfikdjZwC3A90AuUgeXAG4GWsa6vsXkf7duzkdg613XXaav1GeDdeG70T4EN+GQVfMJ8zBjX7R7j\neIknT67nxscT8YWFY+kcp01ERBpUw06ORSTxXnxCeGlt2oGZvRafHO+vfWWqzzOzfJ0J8qL42Dve\nxWa2AHgncB/wjBDCnjrjPVjVMXw3hPDKSehPREQaSMNOjh968HcADA+mG11s7/IoaiHnf0YdJv39\nXRj1tt5dHqHtbE8X63V1eeS3+ufXcnk0aVu78XEAhoa9jFqlkkZjm+PivkK7/wV7+840onvn7b7A\nfc2ja5Jjfb3bAcjHTPA53XOStkWz/K/Ca4Y9dbS/N51jDA15tLq9w6PDoTKSaRuJYy/Fx0ya+dTv\nECyHhhPi43V12i6sc+xgFIBn4BHqrJXx8e59XH8cvhbi+joT42Wx/WA9iEeZzzezphDC6L4umKgV\nS7u5UxtyiIgcVrQgT6TxrY2PK7MHzexivDzaZPukmSVpGmY2B68wAfCVfVy7Nj4+K1aOqPbRCfwr\nk/CBPoRQwsu1LQb+j5nV5l9jZovN7LSDvZeIiBx+GjZyLCKJq/EqEd8xs+vwHN4VwAuBbwOvnsR7\nbcLzl+8zs/8CmoBX4RPRq/dVxi2EsNnMvgm8BrjHzK7H85Sfj9chvgc4cxLG+TF8sd9b8NrJP8e/\nLwvwXORn4uXeHpiEe4mIyGGkYSfHQ3s8vWFnb7pA7tHHYtpCky+MG62ka3TyLV6TeOf2nQCsuj/9\nndjT0wNAqeypCf39aarGQw/6vgKVuLgtl0lb6O3z9UGFFr9fuZK2Pfywp2Mctfio5NhTT/eFdVaJ\nO931pakT2/ds9XEW/S/AYSRN39i6zVNBevK+zihvrUlbcdT78mAZBNKFfHXX6EvDCSH81swuAv4W\n3/ijANyLb7axm8mdHBfxne0+gU9w5+F1jz+FR2v3x5/Fa16NbxqyDfgv4G+onxpywGIVi1cAr8cX\n+b0UX4C3DVgDfAT4+mTcS0REDi8NOzkWkVTcPvm5YzRbzbkr61x/Y+1549yrF5/UjrsbXghhbb0+\nQwiDeNT2Q3UuO+CxhRCWj3E84BuOXDveOEVE5MjSsJPjoRH/XbmrL110t27jAACWj8cyC9JGR73S\n1IMPet3+n994Q9KWi6XcKnFLuVIp7bNcigvd4u/mUE4jutXf1tWd71ra0ypWzTlPyWzObFM3sM0j\n23t2b459p1HvfMHTIju6vFzb/M6epK1tzjy/X4xaV8qZ6HDsvlrKNWTuVymplJuIiIhIlhbkiYiI\niIhEDRs5fnTdWgBGy+lLLMQNMcrBc4CbmtLPBrm8h1aLA15urX9weK+26keJ7EYahRiSbcr7fcqZ\nqC0VP8+qldUyTV3dHkVuLmVKsu2O9yz5Bfn2jqStNZZyy+c9gpwvp/nS5YKXnRseifnImTHkqmMN\nFoeUfj/M6u3VICIiInLkatjJsYhMr7Fye0VERA4nSqsQEREREYkaNnK8s8/LmzU3dyXHmlo7AbCY\nklDOLHAv5TzVohLy8Zx0MVxn3OnO4o642bSFtmZfWNfWVO2zKWlrj9fNbfYFeXMyaRK5tupudukm\nYEU8LaLQ7eNs65ybjr3dd+krjfrnmUIhvU9cE0gpPqlU0nSJ6q5+ye5+pUzKRS5NzRARERERRY5F\nRERERBINGzl++rnnAtC3J40AP7HBS6X17vFFd8VypjRqjKK2t3ukubWSfmvmt/hniKa4uC37TZsd\no7wdHb4oriUb0S36vUu9Hh0e3bMjadvd7311Ll2W9tWzFIAQFw5aU3vSVt2wpBKqEeB0UeDQkC/k\nK8eIeKZa216qEWQ/T6XcRERERLIUORYRERERiRo2cnzuec8B4Lbf3JUcG3h0PQCFJn/Zs5csTtpm\nzVkAwKJOzz1eUEjzdhfF3ZjLezyPefeutPxab69vET2407ed3jmSloAbiOXg9pS9r6F8muM7e+4i\nAObNX54ca2vxyPGoefR5ZDQdQylGikfK/jgUt5EGKA57hHq07J91Qkg/81Q3/cjl9v4cVKkociwi\nIiKSpcixiIiIiEikybGIiIiISNSwaRXXX38L8OT0g6OOPh6AWbO9RFrHrJ6kbTie11oZAqA4OpS0\nPbL2CQB6t2wCYGBgJL1uJKY0xBSFbKKCxTSKYo/fp23xUUlbR/dCv65lTnJsz6CfXy2xNpRPexss\neopGNa1iNHOjUsnTL0rxYLlOtkSuEFM6KulqvTDeyj0RERGRI5AixyJySDKzYGY3HsD5K+M1V9Qc\nv9HM9ElQRET2S8NGjlvaPCLb1dOWHBuNvx6rG2gUsp8NRvoB2DPoZdee2Lomadq1xSPHpRGPLlcq\n6cK6QtwEJNfs38pCa2vS1jnLy7zN7vZIdWtnGiVujWXaQqaa3FDwiHR51B+HR9PxDcXFeSFuXFLO\n/KqvxFBxdX3daCasnK8uAox7f2SjxVqQ11jiBPCmEMLKmR6LiIjI4aphJ8cicsT5NXAqsH2mByIi\nIoevhp0c9/R4abZsbLQQt30uVze/yGyz3B4DrNuHPYK8a09/0lbM+bcp3xEjzrk0Opwv+POWDn9s\n7Ui3q+7o8sjxrDbf+rk115K0Ve9cKqf5yyHvR0dicLdUSc+vlmerxI1IqnnGAKMxqlwsVXOOMxt9\nxKcWqtHl9DuiyLE0khDCIPDgTI9DREQOb8o5FpkmZnaJmV1nZo+Z2ZCZ9ZnZrWb2+jrnrjWztWP0\nc0XMrV2Z6beaL3NhbAtj5N/+sZndbGa9cQy/M7MPmFlLzW2SMZhZp5l91syeiNfcY2aviOcUzOyD\nZvaImQ2b2Woze8cY486Z2VvM7Ddm1m9mA/H5W81szJ9FZrbEzK41s63x/nea2evqnFc353g8Znax\nmf3YzLab2Ugc/9+bWc++rxYRkUbUsJFjkUPQ54EHgJuBTcBc4MXAtWZ2cgjhIxPs9x7gSuBy4HHg\nmkzbjdUnZvYJ4AN42sE3gH7gRcAngIvN7PkhhFGerAn4b2AO8H2gGXgtcJ2ZvQB4G3Ae8BNgBPgj\n4Coz2xZC+FZNX9cCrwOeAL4EBOAPgKuBZwF/Uue1zQZ+CewGvgL0AH8MfN3MloYQ/n6f350xmNnf\n4N+3ncAPga3AU4G/Al5sZheEEPom2r+IiByeGnZyXDHPJ8jGo/LVbIOYTlAuFZO2UPY5QTnuZpcr\nNCdtrZ3dADQ15eNj2pbP+/OWVl9g196eplW0xmNNMfWibOl1o7Gk2mg5syIvKiUL6zLzFMs/6bpi\nZve86rEQS8BZLu0zTSHZe7G+kiqm3YoQwursATNrxieWl5nZF0IIGw600xDCPcA9ZnY5sDaEcEXt\nOWZ2AT4xfgI4N4SwOR7/APBd4KXA/4dPlLOWAHcBK0PwFaNmdi0+wf8OsDq+rt2x7TN4asNlQDI5\nNrPX4hPju4HnhBD64/EPAzcBrzOzH4UQvlFz/6fG+7wmBH8zm9mngDuBj5vZdSGExw7sOwZmdhE+\nMf4V8OLq+GPbJfhE/ErgPfvR151jNJ1yoOMSEZGZp7QKkWlSOzGOx4rAP+MfVJ83hbd/U3z82+rE\nON6/BLwP/6z052Nc++7qxDhecwuwBo/qvj87sYwT1VuBp5hZPtNH9f6XVSfG8fwB4P3xy3r3L8d7\nVDLXrAH+Dx7VfsOYr3h874yPf5Edf+z/GjwaXy+SLSIiDa5hI8fV6GmlUkqOVSrVcmjVyHG6GG50\n2KPI1QBrW1taAi4XP0I0Nfvv+ubmND0zn4+L9GIEuRC/BrC4Gm40lloLlsZqS7EkWyXz+STEKHK5\nWpqtnEaHQ0wpLZfigrzMYrqRahQ5V52L7B2NrsS5RW7s1E6ZYmZ2ND4RfB5wNNBWc8rSKbz90+Lj\nz2sbQggPm9l64Fgz66mZLO6uN6kHNgLH4hHcWhuAPLAoPq/ev0ImzSPjJnwSfFadtnVxMlzrRjyN\npN41++MCYBT4IzP7ozrtzcB8M5sbQtgxXkchhLPrHY8R5afVaxMRkUNXw06ORQ4lZnYcXmpsNnAL\ncD3Qi08KlwNvBPZaFDeJuuPjpjHaN+ET9m48v7eqd4zzSwAhhHrt1U+kTZlj3cDOGCl/khBCycy2\nAwvq9LVljPtXo9/dY7Tvy1z859/l+zivExh3ciwiIo2lYSfHhZgzXMrsllGuxN/L8dBhySpwAAAg\nAElEQVRothxafF7durm5uT1py8fvUjXnuNCU/s7Pxb8c5/Mt8dz0W1qJYehq3m8u80fm6n2yWz2X\nQizJVowR7pCJKueePM7qttUAIY6hEpOVs7tCV3OoLeZg5/Pp+KxOhFmmzHvxCdml8c/2iZiP+8aa\n8yt49LKeiVRSqE5iF+F5wrUW15w32XqBOWbWVLvoz8wKwDyg3uK3hWP0tyjT70THkwshzNnnmSIi\nckTR39hFpscJ8fG6Om0X1jm2C1hoZk112s4Z4x4VPJ2hnrvj48raBjM7AVgGrKnNv51Ed+M/b55T\np+05+LjvqtN2tJktr3N8ZabfibgNmG1mp0/wehERaVCaHItMj7XxcWX2oJldTP2FaL/G/7Jzac35\nlwDPHOMeO4Cjxmj7cnz8sJnNz/SXB/4B/1nwb2MNfhJU7/9JM0v+LBOffyp+We/+eeB/Z+sgm9mx\n+IK6EvC1CY7ns/HxX81sSW2jmXWY2fkT7FtERA5jDZtWMTw8BECxOJQc84X5QFzcVsrkNBRL1d3z\n/OtKZuFaIe/BuBCPhcxnikpMTaimKJSzC+yq58TbjpZGM9fF1InMgv7qIrt0XGnaQ6UcUydimkQl\ns7tfMZZ8S1ImQnqdVUcRcy0qmb9oV1MtZFpcjU90v2Nm1+EL1VYALwS+Dby65vyr4vmfN7Pn4SXY\nzgCegdfkfWmde/wMeI2Z/QBfKFcCbg4h3BxC+KWZ/R3w18B9ZvYfwABe53gF8AtgwjWD9yWE8A0z\nezleo/h+M/se/r/IK/CFfd8OIXy9zqW/xeso32lm1+M5xq/GU0v+eozFgvsznp+Z2WXAJ4FHzOzH\neAWOTuAYPJr/C/zfR0REjiANOzkWOZSEEH4ba+v+Lb7xRwG4F3glvgDu1TXnP2Bmv4fXHX4ZPtG9\nBa+y8ErqT47fhU84nxfvkcNr9d4c+3y/md0NvAP4U3zB3Grgw8Cn6y2Wm2SvxStTvAl4czy2Cvg0\nvkFKPbvwCfzf4R8WZuEbqfxDnZrIBySE8L/N7FY8Cv0s4OV4LvIG4F/wjVIOxvJVq1Zx9tl1i1mI\niMg+rFq1CnzR+rSyEPbeHEJERA6OmY3gaSH3zvRYRMZQ3ajmwRkdhcjYzgDKIYSprOa0F0WORUSm\nxn0wdh1kkZlW3d1R71E5VI2zA+mU0oI8EREREZFIk2MRERERkUiTYxERERGRSJNjEREREZFIk2MR\nERERkUil3EREREREIkWORUREREQiTY5FRERERCJNjkVEREREIk2ORUREREQiTY5FRERERCJNjkVE\nREREIk2ORUREREQiTY5FRERERCJNjkVE9oOZLTOzL5vZRjMbMbO1ZvY5M5t9gP3Midetjf1sjP0u\nm6qxy5FhMt6jZnajmYVx/mudytcgjcvMXmVmV5nZLWbWF99PX5tgX5Py83gshcnoRESkkZnZ8cAv\ngQXA94EHgXOBdwEvNLNnhhB27Ec/c2M/JwE/B74JnAJcCrzEzC4IITw2Na9CGtlkvUczrhzjeOmg\nBipHsg8DZwD9wHr8Z98Bm4L3+l40ORYR2ber8R/E7wwhXFU9aGafAd4DfBx4y3708wl8YvzZEMJ7\nM/28E/jHeJ8XTuK45cgxWe9RAEIIV0z2AOWI9x58UvwocCFwwwT7mdT3ej0WQjiY60VEGpqZHQes\nBtYCx4cQKpm2LmATYMCCEMLAOP10ANuACrA4hLAn05aL91ge76Hosey3yXqPxvNvBC4MIdiUDViO\neGa2Ep8cfz2E8PoDuG7S3uvjUc6xiMj4nhsfr8/+IAaIE9xbgXbg/H30cwHQBtyanRjHfirA9fHL\niw56xHKkmaz3aMLMXm1ml5nZe83sRWbWMnnDFZmwSX+v16PJsYjI+E6Ojw+P0f5IfDxpmvoRqTUV\n761vAp8EPg38GFhnZq+a2PBEJs20/BzV5FhEZHzd8bF3jPbq8Z5p6kek1mS+t74PvAxYhv+l4xR8\nktwDfMvMXnQQ4xQ5WNPyc1QL8kREDk41N/NgF3BMVj8itfb7vRVC+GzNoYeAD5rZRuAqfFHpTyZ3\neCKTZlJ+jipyLCIyvmokonuM9lk15011PyK1puO99SW8jNuZceGTyEyYlp+jmhyLiIzvofg4Vg7b\nifFxrBy4ye5HpNaUv7dCCMNAdSFpx0T7ETlI0/JzVJNjEZHxVWtxviCWXEvECNozgSHgtn30c1s8\n75m1kbfY7wtq7ieyvybrPTomMzsZmI1PkLdPtB+RgzTl73XQ5FhEZFwhhNV4mbXlwNtrmq/Eo2hf\nzdbUNLNTzOxJuz+FEPqBa+P5V9T0847Y/09V41gO1GS9R83sODNbWtu/mc0DvhK//GYIQbvkyZQy\ns6b4Hj0+e3wi7/UJ3V+bgIiIjK/OdqWrgPPwmsQPA8/IbldqZgGgdiOFOttH/xo4FXg5sDX2s3qq\nX480nsl4j5rZJXhu8U34Rgs7gaOBF+M5nncAzw8h7J76VySNxsxeAbwifrkIuBh4DLglHtseQvir\neO5yYA3weAhheU0/B/Ren9BYNTkWEdk3MzsK+Ci+vfNcfCem7wFXhhB21pxbd3Ic2+YAl+O/JBYD\nO/DV/38TQlg/la9BGtvBvkfN7CnA+4CzgSX44qY9wP3At4EvhhCKU/9KpBGZ2RX4z76xJBPh8SbH\nsX2/3+sTGqsmxyIiIiIiTjnHIiIiIiKRJsciIiIiIpEmxw3IzG40sxAXVxzotZfEa2+czH5FRERE\nDgcNvX20mb0b31/7mhDC2hkejoiIiIgc4hp6cgy8GzgGuBFYO6MjOXz04jvQrJvpgYiIiIhMt0af\nHMsBCiF8F/juTI9DREREZCYo51hEREREJJq2ybGZzTGzN5rZdWb2oJntMbMBM3vAzD5jZkvqXLMy\nLgBbO06/ey0gM7MrYoHzY+KhG+I5YZzFZseb2RfN7DEzGzazXWZ2s5n9uZnlx7h3skDNzGaZ2d+Z\n2WozG4r9fNTMWjPnP8/Mfmpm2+Nrv9nMnr2P79sBj6vm+tlm9tnM9evN7F/MbPH+fj/3l5nlzOwN\nZvbfZrbNzIpmttHMvmVm5x1ofyIiIiLTbTrTKj6I77xT1Qe04Vunngq83sx+L4Tw20m41//f3r3H\n2VnV9x7//PbeM0lmQu4JIUEIYoUIohIVuSjRWhDQShVFbL311F44vqDai+DxEk6rYmvFem+rlqPi\nUalWjkVblMq1UjSACIRriIQQQu5zn9mX3/ljrecyO3tPJpM9M8nO9/16bZ+ZZz3PWuuZbIe1f/Nb\na/UBW4DFhA8AO4H8rj71OwW9FrgWSAayuwn7c788vi40s/PH2Kt7PvDfwPFAP1AEjgE+BLwQ+G0z\nuxj4HOCxf12x7p+Y2avc/fb6SlvQr4XAz4FjgUGgAiwH3g2cb2Znuvu6JvfuEzM7DPge8Op4ygk7\nKx0BvBm4wMwudffPtaI9ERERkckwlWkVm4ArgZOBw9x9LjADeDHwH4SB7DfNbI/tVveVu3/S3ZcC\nG+OpN7j70tzrDcm1cY/ubxEGoDcDx7v7POAw4I+AYcKA7+/HaPIjgAEvd/fZwGzCALQCvM7MPgR8\nOj7/wvjsK4CfAZ3AVfUVtqhfH4rXvw6YHfu2mrAl42LgWjPrGOP+ffG12J97gfOA7vic8wkfjCrA\n35vZ6S1qT0RERKTlpmxw7O5Xufvl7n63u/fFc1V3Xwu8HngAOAF4xVT1KfoAIRr7GHCuuz8U+zbs\n7v8IXBKv+30ze06TOrqB17r7bfHeEXf/MmHACGH/72+4+wfcfVe85tfARYQI60vM7KhJ6Ncc4AJ3\n/zd3r8X7bwbOIUTSTwAu3MvPZ6/M7NXA+YQVQV7p7j9098HY3i53/zhhoF4ALt/f9kREREQmywEx\nIc/dh4Efx2+nLLIYo9RvjN9e5e4DDS77MiHqbcAFTaq61t0fbXD+J7mvP15fGAfIyX0nTkK/bnX3\nWxu0+xDwL/HbZvfui3fE49XuvqPJNd+Mx1eOJ1daREREZDpM6eDYzI43s8+Z2b1m1mNmtWSSHHBp\nvGyPiXmT6NnA3Pj1TxtdECOuN8VvT25Sz6+anH8mHofIBsH1tsTj/Eno101NzkNI1Rjr3n1xWjy+\n18yebvQCfhGv6SLkQouIiIgccKZsQp6ZvYWQZpDkuNYIE8yG4/ezCWkE3VPVJ0LebWLTGNc92eD6\nvM1NzlfjcYu7+16uyef+tqpfY92blDW7d18kK1/MJRvUj6WrBW2KiIiItNyURI7NbDHwT4QB4LcJ\nk/Bmuvv8ZJIc2aS0/Z6QN0EzpqndvZmsfrXy55y8j17v7jaO14YWti0iIiLSMlOVVnEOITL8APBW\nd1/r7uW6aw5vcF8lHmc2KEuMJ1LZzNbc10c3vQqObHD9ZGpVv8ZKUUmiva14piQ15HktqEtERERk\n2kzV4DgZxN2brJqQFyegvarBfbvicYmZdTap+yVjtJu01SxKuj7XxisbXWBmBcLyZwB3jdFWK7Wq\nX2eO0UZS1opn+lk8vnHMq0REREQOcFM1ON4djyc2Wcf43YSNKuo9TMhJNsJavaPEJczGGpD1xOO8\nRoUxD/h78dtLzaxRLuwfEDbOcLIVHiZVC/t1ppmdVn/SzH6DbJWKa/ezuwBXx+OLzeztY11oZvPH\nKhcRERGZTlM1OP4JYRB3IvAZM5sHELdc/gvg88D2+pvcfQS4Ln57lZmdEbcoLpjZWYTl3wbHaPf+\neLwov41znY8RdrVbBlxvZsfFvs0ws3cDn4nXfaXJcm2TpRX96gG+Z2bnJh9K4nbVPyLkMt8PfGd/\nO+ru/042mP+qmV2R3546bmH9ejO7DvjU/rYnIiIiMlmmZHAc19X9dPz2PcBOM9tB2Mb5b4AbgS81\nuf1ywsD5WcCthC2J+wm76u0C1ozR9Ffi8U3AbjPbaGYbzOxbub49RtiMY4iQpvCgme2M7fwjYRB5\nI/Cn43/i/deifv0VYavq64F+M+sFbiFE6bcCb26Q+z1Rbwe+T9g6+8PAU2a2y8x2E/6dvw/8dova\nEhEREZkUU7lD3vuAPwTuJqRKlIB7CIO788gm39Xftx44Bfi/hAFdkbCE2UcJG4b0NLov3vufwO8Q\n1vQdJKQhHA0srbvuB8DzCStqbCAsNTYA3Bb7fLa79+/zQ++nFvRrOyEn+9OESXOdwFOxvhe6+wMt\n7Gu/u/8O8FpCFHkTMCu2+ShhE5ALgItb1aaIiIhIq1nz5XdFRERERA4tB8T20SIiIiIiBwINjkVE\nREREIg2ORUREREQiDY5FRERERCINjkVEREREIg2ORUREREQiDY5FRERERCINjkVEREREIg2ORURE\nREQiDY5FRERERKLSdHdARKQdmdnjwBxgwzR3RUTkYLUC6HH3Y6ay0bYdHPdTdoCh3X3puZIlgXIP\n/+vZ9WaWfJWcSMtqXovHcEORYlpWLCRf12KdlbTM0+OeAfqs6aydSmyn7OH6kawq+nuHARjoH4l9\nqaZlM2bMAGBoeCAcBwez+8qhkmqomoLl/8lDO6894zhDRFptzqxZsxasXLlywXR3RETkYLRu3ToG\nc2OaqdK2g+NSpQOAjuKs9JzXwgjR9hz/UiyFH0VnRzh6buScH/BCfpANxTjArKbXZvfVYv2V2JBX\nswFtNfaldzg7t31nGNz29A8BUC5ndY0Mh+sr8fJyrZy1Uw0fAKrVkVF1A1R89KC/UBhKy0xJNXIA\nMzMHbnb31eO8fjXwU+AKd1+TO38TcKa7T/WHwA0rV65csHbt2iluVkSkPaxatYq77rprw1S3q+GR\nSJswM48DQREREZmgto0ci8gh505gJbBtujuSuG/TblZcdv10d0MOYhuuPG+6uyByyGnbwfE9D28G\noFzO0giq1ZiKkKZVZH9l7SiFNIy53SENIx9St5jf29ER8otLxazUa6GsGvOEq7k85nIlpDcMV0Na\nxtDQcFo2NBK+3jkwkp7r7wvXjcQE4WI+7SPWW4lpG7VcWa0uXSTPPEmxSDqWXWQF/eFA2oe7DwAP\nTnc/RETk4KbRkcgUMbN3mtl3zWy9mQ2aWY+Z3W5mv9fg2g1mtqFJPWtiCsXqXL3Jp58zY1nyWlN3\n75vN7BYz2x378Cszu9zMZjTrg5nNNrOrzGxjvOceMzs/XlMysw+Y2SNmNmRmj5nZe5r0u2Bmf2xm\nPzezPjPrj1//iVnzDHgzW2ZmXzezZ2L7a83srQ2uW93omcdiZmeb2Q/NbJuZDcf+/62ZzRtvHSIi\n0l7aNnK8ccsuAAYGetNz6WoTcYWJQi4CXCiEiGpncUc4kV/KIirGSGv+v+PJZbUYma1Ws8lwI3Gl\niOSaSm5CXi2OZbKrwWqjxwe1UZHg0WFhq2X9K9R1dfRkwnh92vdc5FifjabaF4EHgFuAzcBC4Fzg\n62Z2nLt/aIL13gNcAXwE+DVwda7spuQLM/sYcDkh7eCbQB9wDvAx4Gwz+y13LzNaB/BjYAFwHdAJ\nXAR818zOAi4GTgF+BAwDbwI+a2Zb3f3bdXV9HXgrsBH4MuHPGb8DfAE4A/jdBs82H/gvYBfwz8A8\n4M3ANWa23N3/dq8/nSbM7MOEn9sO4N+AZ4CTgD8HzjWzU929Zxz1NJtxd/xE+yYiItOnbQfHIgeg\nE939sfwJM+skDCwvM7Mvufumfa3U3e8B7jGzjwAb8is15No5lTAw3gi81N2fjucvB/4VeC3wF4SB\nct4y4C5gtbsPx3u+ThjgXws8Fp9rVyz7FCG14TIgHRyb2UWEgfHdwCvcvS+e/yBwM/BWM7ve3b9Z\n1/5JsZ23uIccITO7ElgLfNTMvuvu6/ftJwZm9krCwPhnwLlJ/2PZOwkD8SuA9+5r3SIicnBr28Hx\njt7dAAz159Y5LoWIcWdn+AtyoZKPnIYQ60C6XnEuHBujymkUNneXMbqM3GpRSR31Syjnvx4d5a3m\nuzJKdc9T2X1JFNn3jEbvKStNlq+TqVE/MI7nRszs88CrgN8EvjZJzf9+PP51MjCO7VfM7M8IEew/\nYM/BMcCfJgPjeM+tcYOLY4D35weW7r7ezG4HXm5mRU/f1Gn7lyUD43h9v5m9H/hJbL9+cFyNbdRy\n9zxuZp8hRMrfRhjE7qtL4vHd+f7H+q82s0sJkey9Do7dfVWj8zGifPIE+iYiItNIoyORKWJmRwHv\nJwyCjwJm1V2yfBKbTwZp/1lf4O4Pm9mTwDFmNq9usLir0aAeeIowOG6UUrAJKAJL49dJ+zVyaR45\nNxMGwS9qUPaEuz/e4PxNhMFxo3vG41SgDLzJzN7UoLwTWGxmC919+wTbEBGRg5AGxyJTwMyeTVhq\nbD5wK3ADsJswKFwBvAPYY1JcC82Nx81NyjcTBuxzCfm9id1Nrq8AuHuj8mTXnI669ne4+0j9xTF6\nvQ1Y0qCuLU3aT6Lfc5uU781Cwu+/j+zlutmABsciIoeQth0cV2rJls+53e2SiXGV8N/nQu7xq3VL\npOX+ikuhGNIxsslsuc2f48S6ZFKc5VIiPNmmOs2JyPIqklXU8pkWtXSS3Z5rsllh9DnPTcir1U0e\nrNWyJIxCnHyYPM+o5/KxEzCkpd5HGJC9y92vzhfEfNx31F1fI0QvG5nISgrJIHYpIU+43hF117Xa\nbmCBmXXUT/ozsxKwCGg0+e3wJvUtzdU70f4U3F1bO4uIyChtOzgWOcA8Jx6/26DszAbndgInNRpM\nAi9u0kaNkM7QyN2E1IbV1A2Ozew5wJHA4/X5ty10NyGd5BXAjXVlryD0+64G9x1lZivcfUPd+dW5\neifiDuA8MzvB3e+fYB17deLyuazVJg4iIgeVtl3Lq4RTwilaIX3hgIPXanitRqVaTl/V+KJSgUoF\nK3v6olyGcplipUKxUqFU8fTl5TJeLkN1JL6G05dVy1i1TBiz1Kh4JXvVqlRqVbzm6cs8RJ5r7iEa\nHPuLAxWPr1p41arZy2vgNZzwqnn2Sp+vVqVaq9aVVUctLyeTakM8rs6fNLOzCRPR6t1J+PD6rrrr\n3wmc3qSN7cCzmpR9NR4/aGaLc/UVgU8Sfhd8pVnnWyBp/+Nm1pVrvwu4Mn7bqP0i8In8Oshmdgxh\nQl0F+MYE+3NVPP6TmS2rLzSzbjN72QTrFhGRg5gixyJT4wuEge61ZvZdwkS1E4HXAN8BLqy7/rPx\n+i+a2W8SlmB7AXAaYU3e1zZo40bgLWb2A8JEuQpwi7vf4u7/ZWZ/A/wlcJ+Z/QvQT1jn+ETgNmDC\nawbvjbt/08xeT1ij+H4z+z7hY9/5hIl933H3axrcei9hHeW1ZnYDIcf4QkJqyV82mSw4nv7caGaX\nAR8HHjGzHwKPE3KMjyZE828j/PuIiMghRINjkSng7vfGtXX/mrBsWgn4JfAGwgS4C+uuf8DMXk1Y\nWu11hIHurYRVFt5A48HxpYQB52/GNgqEZc5uiXW+38zuBt4DvJ0wYe4x4IPA3zWaLNdiFxFWpvh9\n4I/iuXXA3xE2SGlkJ2EA/zeEDwtzCBupfLLBmsj7xN0/EZedu4SwCcnrCbnIm4B/ZM9l5URE5BBg\n3mAnuHbw+Wt/6gAjA/3puYLFnfHi+r613MS6+t3vqiNZ2czOcK57dvhr8IyObFGBSlyB2OOMvEpu\nh7xa3CGPWkgZreTm1CU/9lJuVzyvhOsHK+V4TS7rJc4UTCf51bJ2ksl6tThhML9LnyXrGjdIoCkU\nw8/hkre8es8ZgCKyX8xs7cknn3zy2rXNNtATEZGxrFq1irvuuuuuZuvJT5a2zTkWEREREdlXbZtW\nMVIOG3qVy9lfigsWHtfiUmeFjuzxSzFK21EdDPdXsujrtv5Qx66+nQAsW3ZkWnbE8rDS1M5tYfnY\n3dueSct27woT/weGhkK7ndnKXGkUO7ea2swYye3oChFqK2XXe1zerVapxmfI7ivEqHduC7HsvuTj\nT7ZGXXbjXvbSExERETnUKHIsIiIiIhK1beS4Wo65wJ7fLCOcK4+ESHDvtr60bGhXiAqv2Boiv5v6\nBtKyXw6Hr0diLvAFb/vdtGxpIayK9dCjjwLQ1zeYlvX3h3xni5uILJw7Py2bNasbgMGeLCd6OB4X\nLArR6N7ebE+EaiWJgMfn8SxNOHnGZGOQbLOSbIOQ5Jpi/r6alnETERERyVPkWEREREQk0uBYRERE\nRCRq27SKkZGQpFAdyVIHOuOSZ71xotymrVvSsuKckObw4OIFAGzcvS0ri/PiSiNhAtuC2ekGX/Tv\nDHX1D4W6Z8xfmnWioxeA7q6w9Nv8xUvSoq6u0F55znB6buvTTwNw2Px5oZ1lh6dlO558CoCeHTvC\n81k20bBY7QAgXd0ttzCbxYl46aS9Wq5QaRUiIiIioyhyLCIiIiIStW3kuFgNG2o4lfRcNQZK+wbD\npLnueQvSsjkxSrt505MAlC230UeMDnccFqKuh83LIsd927YD0NUdyubMy36kgxZCzp1x05AZheyz\nSClOrJu3KOtDZSBM/CsPhf4tXro4LVt02BwANq0Pu+U+9vCDWV2FmaF/pWS5tuznUI1ruXmMGI9U\nsp9HB4oci4iIiOQpciwiIiIiErVt5PiwWeHRKuUsOtrTEzbjKHWEHN35ixemZZXhkPtb2R2ixJ0d\nWW5usoTbglrIBZ4xUE7LNsU6YVa8OIs4lyzZsCMci5aFdJP6Z83oSM/NmxMi0uWhEEEuetb3JGd4\ncCgs7/bounvSspNPekmsMzzzUDnLYyYu61YeCRHjof5sibqZh3UjIiIiIhlFjkVEREREIg2ORURE\nRESitk2r6IwT8cyy1IS+vpAy0dsXdqUbGtmVlg0NhaXRfCSkTJRyP5k5veH6F3WGZdSOvOnLaVnP\n8Inh+lOeD8CMGTPTssKckGrR0RHSKUozs7KOzmT5tWxJttlxibhyOfRhoGdnWlYuh+d5+JEHANi2\n85m0bMERiwDomnUYANu3Z8vQDcXd/aqVcCzlfh6W2z1Q5EBnZjcBZ7rntnnc+z0O3OzuqyerXyIi\n0l4UORYRERERido2clypheXQigND6bnClq0ADFTCuVohi5zOnhOirseseFa4b9ectKz7zrABx6pj\nQwR5yfwn0zLbdQoAfSuPCd/PzCbkdZaKoa6OcOyPS8hBFh1esDCbFEgMiFlcc27Hjiw6XIyT585c\nfQYAL3rh89KyY48JXxctRKMX5jYP6e0NE/h2bA593rppU1pWq2Q/G5E2tRIY2OtVIiIiUdsOjkVE\n3P3BvV81ee7btJsVl10/nV04ZGy48rzp7oKItAmlVYjItDOz3zazG81ss5kNm9lTZnazmV3c4NqS\nmX3AzB6J1240s0+YxV13Rl/rMVc5f25NPL/azN5hZneb2aCZPWNmXzWzpfX1iIjIoaNtI8dlj2kL\nI9maxM+rhvSGpS95EQBdixalZcmnhJFCDYDO9VnqRPEFxwEweFJIvdh85Py0bN5pp4Xru8Nku0pn\ntm4xMa1i5oz43+xqllYxnKyd3J3999wsXF+I6xuP9GZpHzO7QtlvHB1SKGq5NZC9GtIxPG6NV9qZ\nPfPMGXMBWL4w9PmWp7ekZTt7sgmJItPFzP4Q+AfgaeAHwDZgCXAS8C7gC3W3fBN4OfAjoAc4F/jL\neM+79qHp9wJnAd8G/h04I96/2sxOcfetE3wkERE5iLXt4FhEDhp/BIwAL3D3Z/IFZraowfXHAie4\n+454zf8Cfgm83cwud/enx9nuOcAp7n53rr2rgD8FrgT+x3gqMbO1TYqOH2c/RETkANK2g+OZFh7N\nu7LIbG1JmGQ3PBjm5/Q8+evshmqIGFficVZ/f1p02ElHATAyN9xf6DwiLVvw7DChblaM2mYLs8HT\nO3cDUCqGsvmzimlZV1foX21gZ+6OEL+uxB3uuotZdLgYJxgO7giTA/OLsFUr4Tsrhgjyz2/5WVp2\n99pfAXDR770ttjs7Ldv0xGZEDhAVoFx/0t23Nbj2/cnAOF7Tb2bXAB8GXgz8216mwaEAABGXSURB\nVDjb/Hp+YBytIUSP32pmF7v78J63iYhIO1POsYhMt2uALuB+M7vKzM43s8VjXP+LBuc2xuP8BmXN\n3Fx/wt13A/cAMwkrXeyVu69q9AKmdTKgiIhMTNtGjksxD3fLcBb46Y1B5NpQiA53zMiiynNi7u9w\nf4j9Pj2YRXT7rA8A64l1LZyVlq3oCYGtmV0h17hnMPu8cc8vQlDql3feBsArTn9pWnbssStCnYX8\n55MQAa7GyHG1UklLinFXEq/V4kPU0rJKJXw9UgvX33jDDWnZ2jtCH5YtCWOG41aemJatq+4RqBOZ\ncu7+KTPbBlwMXEJIa3Azuxn4C3f/Rd31jZLlk/+zFBuUNbOlyfkkLWPuPtQlIiJtQpFjEZl27v41\nd38ZsBA4D/gK8ArgP8xsySQ1e3iT88lqFbsnqV0RETmAaXAsIgcMd9/l7j9093cDVwMLCCtTTIYz\n60+Y2VzghcAQsG6S2hURkQNY26ZVbIuT4bbs2J6e641pBLPnhBSI+V3dadnAQJik19sTdpTr6cuC\nRiOV8Jda6wzpFDN2ZnVu2LQBgK6YorFrIJsqd/tPfwzAr+68E4AdmzemZW986xuB0Z9OKrXkL8Oh\nDq9ldRWK8a/F8VwxNyWvUAhlW7aGVJCu7mx3v1PPeAUA6+67F4BnLc9SOed26bORTD8zew3wE3ev\n1BUlEePJ2uHubWb2ubpJeWsI6RT/3IrJeCcun8tabU4hInJQadvBsYgcNL4FDJnZbcAGwAjR4pcA\na4GfTFK7PwJuN7PvAJsJ6xyfEftw2SS1KSIiB7j2HRx3xEhrMYuwjgwOAbBrZ1jCrL8viwBbNUzE\nKxTDj6R7bhZV7uiYEY4zwzJoM2bOTMv6e8OSb+XhsNTa+sezJVb7doRI7lmveR0Ad629LS3b9MQG\nAGZ1d6XnhgZDHUaYTFitZUu5Ec/hPupbgELcPGTn9tCX41Y+Py2bNTNsXPLw/SF6vTm3fN2i2Xts\nKCYyHS4DzgZOJmzoMQT8Gng/8EV3n6yZo1cB/0qYAHgh0EdI5fhA/XrLIiJy6GjfwbGIHBTc/UvA\nl8Zx3eoxyq4mDGzrz9seF4/jPhEROXS17eB4dtxkww7PNuyolEIecSGu9lTNb8HsIRrc2Tljj7oK\ncXONaswQHhzKNgjp64sR57gk2/qHH07LjlgUJr2/aFVYwm3z5ixqu2HDJgDmzMk25ejZHfKdk22k\n89x99Pf5/+THot6dYcm57rnL06Kly8IGY8WOkC+9Y3e2CtbMefP2aEdERETkUKYZWSIiIiIikQbH\nIiIiIiJR26ZVDAyEyW1DlWz8XyqECWjlckiF8Gq2UtPISFhFqr8npF5UcjvrDW8LqQi7hkM6Rakr\nS3F4ellYGq0jpmM8+uAjadnhi0JKx9JlIb1ixbOfm5Y9+dRjABSK2aS4crkW+xXq7yhlZaVieA5L\n5+VlfajFiXvds0OKxvz5C9Kyzs7wT7z48NDPDQ9l84wGdu33SlUiBx13X0NYsk1ERGQPihyLiIiI\niERtGznevSVMeHtyazYBbfO2MOFtuBJWhhoa6E3LenvC1329Yb+BkeGhtKzWF+4biZFmL2RR23u6\n4/JuXbP2uO/F7wgT8WbGsmIpm2hXrIQQcFcpWxauaiGiPVAJx925DUwGB/riMUSv+/v7svvi5iaF\njhBpXrD46LRsYCg8z4bHHgdgV25D3EJJS7mJiIiI5ClyLCIiIiISaXAsIiIiIhK1bVrFskVhZ7hd\nO7PUhPX3hV3iNm3ZBsDwSLbxVjLBrRbmxKUT3wBK8adUTLaly61DXKnE9IYkzSG3q926B+4H4PgT\nXxRuy21rt/bOnwMwMpSlYWzfHfpaDXMDmb9gcVqW9MfiF6WOLCUiqfWZZ54A4DknrE/Lih3h88+O\nbSGfYqTqe9wnIiIiIoEixyIiIiIiUdtGjoerIS66cMGi9NwZp58OwK/WPQjA/evWpWUjQ2Gy3azO\nMEGu1NmRli1ZsgyA7jlzAOgoZpHjjhjBrXiYRPfwg/elZTt37ASgpydMChwYHEjLtu8Kk/ye3PhE\nrq7iqPaOfe7KtCybzBeeyyz7XFOrhmh1uRJCzj+/4460bOOvN4a6a6GfM2Z1p2VUs6i1iIiIiChy\nLCIiIiKSatvI8cMbQ/7u8EBuSbbSXACOes6JAGx86um0bNtgWPqtED8uzI1RYoCVJ7wAgK5ZXcDo\nDTiqHqK2lbg83NYt2SYbjz4WNgS59+61AOzYviMtW7QkbBAyNFxJz5VKofElRywPfc+V+VCo3wo2\n6ghgxfD1kXGTEfcs77mjEP6JZ84My8kVcsvJmbKORUREREZR5FhEREREJNLgWEQOOWa2wszczK6e\n7r6IiMiBpW3TKnbsCjvelYdzy7XFzwIVQlrEgtxSaTu3hnSIalzKrfuwuWnZ0MhwrCtMusunVQyX\nw7lkibWFC7M6e3vDpLv77g0TADtK2SS/+fMXAjB37vz0XLEU6pg5I0nfyP55ijE9olBK0ipqaVnS\ndkexI5Zl6RJJuofZnikUNa/tcU6kVcxsBfA48H/c/Z3T2hkREZFxatvBsYjIdLtv025WXHb9dHej\nLWy48rzp7oKIHCLadnDc3xeivbXycHquGiPGlTgRbcHCw9Oy2rFh8psXwoS1eQuzJeBGRkJ0eEYh\nRGaLuUltHXHJtySyu3TpkWnZ8iPDdQUrxfuyH3fBQtS2lotCE8/hIcLtvme01+IzJNfEb+JDJFHi\nfFk1thOvzLWX/1pERERElHMsIpPAzNYQUioA3hHze5PXO81sdfx6jZm91MyuN7Md8dyKWIeb2U1N\n6r86f21d2UvN7NtmtsnMhs1ss5ndYGZvHke/C2b2mVj398xs5sR+AiIicrBq28hxGk0tZOP/Yvy6\nGPNvZ+eiwwsXhBzgWiwr5PJ2izGaXCokkeBcRLeQRG2TaG8+Mhuj0TGRuUCW41tOtpnO1VVMbo17\nWFdyS7LVYl3J8mtJpDr/XMS2S6VcnaUZ8f5Qp9eyPihyLJPoJmAecCnwS+D7ubJ7YhnAqcDlwG3A\nV4FFwMhEGzWzdwNfJPzJ5P8BjwBLgBcDFwPfGePemcA3gDcCnwcucVdivojIoaaNB8ciMl3c/SYz\n20AYHN/j7mvy5Wa2On55FvDH7v4P+9ummT0P+ALQA7zc3e+vKz+y4Y2hbAFwHXA6cJm7f2If2l3b\npOj48dYhIiIHDg2ORWQ63dOKgXH0J4TfaX9VPzAGcPcnG91kZkcD/w4cC7zN3a9pUX9EROQg1LaD\n4+7Dwo5wtUq2y1yhGB7XLKQT5HeI82pITagm6Qe5v6aWOorxvuTiLB2hVg31V6vhXC5rgVo1pEUk\nq65VyfpSjTkUxWKW9lGIKeANVl2D2PdiMaZ25NJFshvCuVJnVmYUY/+S58pSNfQXYzkA3NnCul4W\njz/ah3uOA34GdAPnuPuN+9qou69qdD5GlE/e1/pERGR6aUKeiEynp/d+ybglecyb9uGe5wJHAOuB\nu1rYFxEROUi1beS4Ky6xVs1FZpMJaMmxQBYBrqaT5cKxWsuivF6O0dY4Sc9ydVZjdNgsTtorZsu8\nWYz2dhSS73OR6hi9LuTqKsYl2PKT5tK6LGk71D9qA4906bY4Wa8je66OGDkuj1TifblKvYjINBtr\nVqjT/HfUvAbndsXjcuDBcbb/A+Ah4GPAjWZ2lrtvG+e9IiLShtp2cCwi0y7J4Znop7CdwLPqT1r4\nJPrCBtffQViV4hzGPzjG3T9uZoPAVcBPzezV7r5lYl0e7cTlc1mrzStERA4qSqsQkcmykxD9PWqC\n998JHGVmZ9Wd/yBwdIPrvwhUgA/FlStGGWu1Cnf/NGFC3wnAzWa2bIJ9FhGRg1zbRo77e8NfWEcv\n5RtTEWLaQqmYPX41TshLPy7kJ90laRhxxzordKZlpY44US7unufk0yTipLtkN7xR6RKV2Fw+7SP2\nJa6BXMgF3DzuiFerhF3wyrm0imIxtJ3sjJffIC9JD/E4cTBZjzk8FyKTxt37zOy/gZeb2TXAw2Tr\nD4/HJ4GzgevM7NvADuA04BjCOsqr69p7wMwuBr4E3G1m1xHWOV5IiCj3Aq8co79fMrMh4CvALWb2\nKnd/Ypx9FRGRNtG2g2MROSC8jZCu8BrgIkJi/JPAhr3d6O43mtn5wIeBtwD9wI+BC4ErmtzzT2Z2\nH/DnhMHz+cA24F7gy+No82ozGwa+RjZAXr+3+5pYsW7dOlatariYhYiI7MW6desAVkx1u6Zd0kRE\nWi8OsouEHQJFDkTJRjXjztEXmWIvAKruPmMqG1XkWERkctwHzddBFpluye6Oeo/KgWqMHUgnlSbk\niYiIiIhEGhyLiIiIiEQaHIuIiIiIRBoci4iIiIhEGhyLiIiIiERayk1EREREJFLkWEREREQk0uBY\nRERERCTS4FhEREREJNLgWEREREQk0uBYRERERCTS4FhEREREJNLgWEREREQk0uBYRGQczOxIM/uq\nmT1lZsNmtsHMPm1m8/exngXxvg2xnqdivUdOVt/l0NCK96iZ3WRmPsZr5mQ+g7QvM7vAzD5rZrea\nWU98P31jgnW15PdxM6VWVCIi0s7M7Fjgv4AlwHXAg8BLgUuB15jZ6e6+fRz1LIz1PBf4T+BbwPHA\nu4DzzOxUd18/OU8h7axV79GcK5qcr+xXR+VQ9kHgBUAf8CThd98+m4T3+h40OBYR2bsvEH4RX+Lu\nn01OmtmngPcCHwX+eBz1fIwwML7K3d+Xq+cS4O9jO69pYb/l0NGq9ygA7r6m1R2UQ957CYPiR4Ez\ngZ9OsJ6Wvtcb0fbRIiJjMLNnA48BG4Bj3b2WKzsM2AwYsMTd+8eopxvYCtSAI9y9N1dWiG2siG0o\neizj1qr3aLz+JuBMd7dJ67Ac8sxsNWFwfI27/94+3Ney9/pYlHMsIjK2V8XjDflfxABxgHs70AW8\nbC/1nArMAm7PD4xjPTXghvjtK/e7x3KoadV7NGVmF5rZZWb2PjM7x8xmtK67IhPW8vd6Ixoci4iM\n7bh4fLhJ+SPx+Nwpqkek3mS8t74FfBz4O+CHwBNmdsHEuifSMlPye1SDYxGRsc2Nx91NypPz86ao\nHpF6rXxvXQe8DjiS8JeO4wmD5HnAt83snP3op8j+mpLfo5qQJyKyf5LczP2dwNGqekTqjfu95e5X\n1Z16CPiAmT0FfJYwqfRHre2eSMu05PeoIsciImNLIhFzm5TPqbtususRqTcV760vE5Zxe2Gc+CQy\nHabk96gGxyIiY3soHpvlsP1GPDbLgWt1PSL1Jv295e5DQDKRtHui9Yjspyn5ParBsYjI2JK1OM+K\nS66lYgTtdGAQuGMv9dwRrzu9PvIW6z2rrj2R8WrVe7QpMzsOmE8YIG+baD0i+2nS3+ugwbGIyJjc\n/THCMmsrgP9ZV3wFIYr2tfyammZ2vJmN2v3J3fuAr8fr19TV855Y/39ojWPZV616j5rZs81seX39\nZrYI+Of47bfcXbvkyaQys474Hj02f34i7/UJta9NQERExtZgu9J1wCmENYkfBk7Lb1dqZg5Qv5FC\ng+2j7wRWAq8Hnon1PDbZzyPtpxXvUTN7JyG3+GbCRgs7gKOAcwk5nr8Afsvdd03+E0m7MbPzgfPj\nt0uBs4H1wK3x3DZ3//N47QrgceDX7r6irp59eq9PqK8aHIuI7J2ZPQv434TtnRcSdmL6PnCFu++o\nu7bh4DiWLQA+QviPxBHAdsLs/w+7+5OT+QzS3vb3PWpmzwf+DFgFLCNMbuoF7ge+A/yDu49M/pNI\nOzKzNYTffc2kA+GxBsexfNzv9Qn1VYNjEREREZFAOcciIiIiIpEGxyIiIiIikQbHIiIiIiKRBsci\nIiIiIpEGxyIiIiIikQbHIiIiIiKRBsciIiIiIpEGxyIiIiIikQbHIiIiIiKRBsciIiIiIpEGxyIi\nIiIikQbHIiIiIiKRBsciIiIiIpEGxyIiIiIikQbHIiIiIiKRBsciIiIiIpEGxyIiIiIi0f8H2nLU\nyVZMclgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdb3bcbb4a8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
